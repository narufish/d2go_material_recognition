
Code for root model, type=TracingAdapter:
class TracingAdapter(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  model : __torch__.d2go.modeling.meta_arch.rcnn.___torch_mangle_6661.D2RCNNInferenceWrapper
  def forward(self: __torch__.detectron2.export.flatten.___torch_mangle_6662.TracingAdapter,
    argument_1: Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]:
    model = self.model
    _0, _1, _2, _3, _4, = (model).forward(argument_1, )
    return (_0, _1, _2, _3, _4)

--------------------------------------------------------------------------------
Code for .model, type=D2RCNNInferenceWrapper:
class D2RCNNInferenceWrapper(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  model : __torch__.d2go.modeling.meta_arch.rcnn.___torch_mangle_6660.GeneralizedRCNN
  def forward(self: __torch__.d2go.modeling.meta_arch.rcnn.___torch_mangle_6661.D2RCNNInferenceWrapper,
    argument_1: Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]:
    _0 = __torch__.detectron2.layers.wrappers.move_device_like
    model = self.model
    roi_heads = model.roi_heads
    model0 = self.model
    proposal_generator = model0.proposal_generator
    model1 = self.model
    backbone = model1.backbone
    model2 = self.model
    pixel_std = model2.pixel_std
    model3 = self.model
    pixel_mean = model3.pixel_mean
    x = _0(argument_1, pixel_mean, )
    t = torch.div(torch.sub(x, pixel_mean), pixel_std)
    _1 = ops.prim.NumToTensor(torch.size(t, 1))
    _2 = ops.prim.NumToTensor(torch.size(t, 2))
    image_size = torch.stack([_1, _2])
    max_size, _3 = torch.max(torch.stack([image_size]), 0)
    _4 = torch.sub(torch.select(max_size, 0, -1), torch.select(image_size, 0, 1))
    _5 = int(_4)
    _6 = torch.sub(torch.select(max_size, 0, -2), torch.select(image_size, 0, 0))
    _7 = torch.pad(t, [0, _5, 0, int(_6)], "constant", 0.)
    batched_imgs = torch.unsqueeze_(_7, 0)
    X = torch.contiguous(batched_imgs)
    _8 = (backbone).forward(X, )
    _9 = (proposal_generator).forward(_8, image_size, )
    _10 = (roi_heads).forward(_8, _9, image_size, )
    _11, _12, _13, _14, = _10
    return (_11, _12, _13, _14, image_size)

--------------------------------------------------------------------------------
Code for .model.model, type=GeneralizedRCNN:
class GeneralizedRCNN(Module):
  __parameters__ = []
  __buffers__ = ["pixel_mean", "pixel_std", ]
  pixel_mean : Tensor
  pixel_std : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  backbone : __torch__.mobile_cv.arch.utils.quantize_utils.___torch_mangle_6287.QuantWrapSubClass
  proposal_generator : __torch__.detectron2.modeling.proposal_generator.rpn.___torch_mangle_6423.RPN
  roi_heads : __torch__.detectron2.modeling.roi_heads.roi_heads.___torch_mangle_6659.StandardROIHeads

--------------------------------------------------------------------------------
Code for .model.model.backbone, type=QuantWrapSubClass:
class QuantWrapSubClass(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  body : __torch__.d2go.modeling.backbone.fbnet_v2.___torch_mangle_6277.FBNetV2Backbone
  quant_stubs : __torch__.mobile_cv.arch.utils.quantize_utils.___torch_mangle_6280.QuantStubNested
  dequant_stubs : __torch__.mobile_cv.arch.utils.quantize_utils.___torch_mangle_6286.QuantStubNested
  def forward(self: __torch__.mobile_cv.arch.utils.quantize_utils.___torch_mangle_6287.QuantWrapSubClass,
    X: Tensor) -> Tensor:
    dequant_stubs = self.dequant_stubs
    body = self.body
    quant_stubs = self.quant_stubs
    _0 = (body).forward((quant_stubs).forward(X, ), )
    _1, _2, _3, _4, = _0
    _5 = (dequant_stubs).forward(_1, _2, _3, _4, )
    return _5

--------------------------------------------------------------------------------
Code for .model.model.backbone.body, type=FBNetV2Backbone:
class FBNetV2Backbone(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  trunk0 : __torch__.torch.nn.modules.container.___torch_mangle_5941.Sequential
  trunk1 : __torch__.torch.nn.modules.container.___torch_mangle_5991.Sequential
  trunk2 : __torch__.torch.nn.modules.container.___torch_mangle_6085.Sequential
  trunk3 : __torch__.torch.nn.modules.container.___torch_mangle_6276.Sequential
  def forward(self: __torch__.d2go.modeling.backbone.fbnet_v2.___torch_mangle_6277.FBNetV2Backbone,
    argument_1: Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor]:
    trunk3 = self.trunk3
    trunk2 = self.trunk2
    trunk1 = self.trunk1
    trunk0 = self.trunk0
    _0 = (trunk0).forward(argument_1, )
    _1 = (trunk1).forward(_0, )
    _2 = (trunk2).forward(_1, )
    _3 = (_0, _1, _2, (trunk3).forward(_2, ))
    return _3

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk0, type=Sequential:
class Sequential(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  fbnetv2_0_0 : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_5922.ConvBNRelu
  fbnetv2_0_1 : __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_5931.IRFBlock
  fbnetv2_0_2 : __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_5940.IRFBlock
  def forward(self: __torch__.torch.nn.modules.container.___torch_mangle_5941.Sequential,
    argument_1: Tensor) -> Tensor:
    fbnetv2_0_2 = self.fbnetv2_0_2
    fbnetv2_0_1 = self.fbnetv2_0_1
    fbnetv2_0_0 = self.fbnetv2_0_0
    _0 = (fbnetv2_0_1).forward((fbnetv2_0_0).forward(argument_1, ), )
    return (fbnetv2_0_2).forward(_0, )

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk0.fbnetv2_0_0, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_5919.ConvReLU2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_5920.Identity
  relu : __torch__.torch.nn.modules.linear.___torch_mangle_5921.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_5922.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    relu = self.relu
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    _2 = (relu).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk0.fbnetv2_0_0.conv, type=ConvReLU2d:
class ConvReLU2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_5919.ConvReLU2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_5919.ConvReLU2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_5919.ConvReLU2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_5919.ConvReLU2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_5919.ConvReLU2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d_relu(argument_1, _packed_params, 0.11331272125244141, 0)
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk0.fbnetv2_0_0.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_5920.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk0.fbnetv2_0_0.relu, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_5921.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk0.fbnetv2_0_1, type=IRFBlock:
class IRFBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  dw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_5924.ConvBNRelu
  pwl : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_5927.ConvBNRelu
  res_conn : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_5930.TorchAdd
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_5931.IRFBlock,
    argument_1: Tensor) -> Tensor:
    res_conn = self.res_conn
    pwl = self.pwl
    dw = self.dw
    _0 = (pwl).forward((dw).forward(argument_1, ), )
    _1 = (res_conn).forward(_0, argument_1, )
    return _1

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk0.fbnetv2_0_1.dw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5923.Conv2d
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_5924.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    conv = self.conv
    return (conv).forward(argument_1, )

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk0.fbnetv2_0_1.dw.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5923.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5923.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5923.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5923.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5923.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d(argument_1, _packed_params, 0.080030634999275208, 51)
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk0.fbnetv2_0_1.pwl, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5925.Conv2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_5926.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_5927.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk0.fbnetv2_0_1.pwl.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5925.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5925.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5925.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5925.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5925.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    _18 = ops.quantized.conv2d(argument_1, _packed_params, 0.30488735437393188, 66)
    return _18

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk0.fbnetv2_0_1.pwl.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_5926.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk0.fbnetv2_0_1.res_conn, type=TorchAdd:
class TorchAdd(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  add_func : __torch__.torch.ao.nn.quantized.modules.functional_modules.___torch_mangle_5929.QFunctional
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_5930.TorchAdd,
    argument_1: Tensor,
    argument_2: Tensor) -> Tensor:
    add_func = self.add_func
    activation_post_process = add_func.activation_post_process
    input = ops.quantized.add(argument_1, argument_2, 0.30115282535552979, 62)
    _0 = (activation_post_process).forward()
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk0.fbnetv2_0_1.res_conn.add_func, type=QFunctional:
class QFunctional(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation_post_process : __torch__.torch.nn.modules.linear.___torch_mangle_5928.Identity

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk0.fbnetv2_0_1.res_conn.add_func.activation_post_process, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_5928.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk0.fbnetv2_0_2, type=IRFBlock:
class IRFBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  dw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_5933.ConvBNRelu
  pwl : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_5936.ConvBNRelu
  res_conn : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_5939.TorchAdd
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_5940.IRFBlock,
    argument_1: Tensor) -> Tensor:
    res_conn = self.res_conn
    pwl = self.pwl
    dw = self.dw
    _0 = (pwl).forward((dw).forward(argument_1, ), )
    _1 = (res_conn).forward(_0, argument_1, )
    return _1

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk0.fbnetv2_0_2.dw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5932.Conv2d
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_5933.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    conv = self.conv
    return (conv).forward(argument_1, )

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk0.fbnetv2_0_2.dw.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5932.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5932.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5932.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5932.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5932.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d(argument_1, _packed_params, 0.10007268935441971, 62)
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk0.fbnetv2_0_2.pwl, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5934.Conv2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_5935.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_5936.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk0.fbnetv2_0_2.pwl.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5934.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5934.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5934.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5934.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5934.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    _18 = ops.quantized.conv2d(argument_1, _packed_params, 0.34575763344764709, 66)
    return _18

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk0.fbnetv2_0_2.pwl.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_5935.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk0.fbnetv2_0_2.res_conn, type=TorchAdd:
class TorchAdd(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  add_func : __torch__.torch.ao.nn.quantized.modules.functional_modules.___torch_mangle_5938.QFunctional
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_5939.TorchAdd,
    argument_1: Tensor,
    argument_2: Tensor) -> Tensor:
    add_func = self.add_func
    activation_post_process = add_func.activation_post_process
    input = ops.quantized.add(argument_1, argument_2, 0.37972483038902283, 61)
    _0 = (activation_post_process).forward()
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk0.fbnetv2_0_2.res_conn.add_func, type=QFunctional:
class QFunctional(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation_post_process : __torch__.torch.nn.modules.linear.___torch_mangle_5937.Identity

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk0.fbnetv2_0_2.res_conn.add_func.activation_post_process, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_5937.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk1, type=Sequential:
class Sequential(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  fbnetv2_1_0 : __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_5951.IRFBlock
  fbnetv2_1_1 : __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_5964.IRFBlock
  fbnetv2_1_2 : __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_5977.IRFBlock
  fbnetv2_1_3 : __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_5990.IRFBlock
  def forward(self: __torch__.torch.nn.modules.container.___torch_mangle_5991.Sequential,
    argument_1: Tensor) -> Tensor:
    fbnetv2_1_3 = self.fbnetv2_1_3
    fbnetv2_1_2 = self.fbnetv2_1_2
    fbnetv2_1_1 = self.fbnetv2_1_1
    fbnetv2_1_0 = self.fbnetv2_1_0
    _0 = (fbnetv2_1_1).forward((fbnetv2_1_0).forward(argument_1, ), )
    _1 = (fbnetv2_1_3).forward((fbnetv2_1_2).forward(_0, ), )
    return _1

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk1.fbnetv2_1_0, type=IRFBlock:
class IRFBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  pw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_5945.ConvBNRelu
  dw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_5947.ConvBNRelu
  pwl : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_5950.ConvBNRelu
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_5951.IRFBlock,
    argument_1: Tensor) -> Tensor:
    pwl = self.pwl
    dw = self.dw
    pw = self.pw
    _0 = (dw).forward((pw).forward(argument_1, ), )
    return (pwl).forward(_0, )

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk1.fbnetv2_1_0.pw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_5942.ConvReLU2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_5943.Identity
  relu : __torch__.torch.nn.modules.linear.___torch_mangle_5944.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_5945.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    relu = self.relu
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    _2 = (relu).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk1.fbnetv2_1_0.pw.conv, type=ConvReLU2d:
class ConvReLU2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_5942.ConvReLU2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_5942.ConvReLU2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_5942.ConvReLU2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_5942.ConvReLU2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_5942.ConvReLU2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d_relu(argument_1, _packed_params, 0.11048972606658936, 0)
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk1.fbnetv2_1_0.pw.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_5943.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk1.fbnetv2_1_0.pw.relu, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_5944.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk1.fbnetv2_1_0.dw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5946.Conv2d
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_5947.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    conv = self.conv
    return (conv).forward(argument_1, )

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk1.fbnetv2_1_0.dw.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5946.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5946.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5946.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5946.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5946.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d(argument_1, _packed_params, 0.13561369478702545, 68)
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk1.fbnetv2_1_0.pwl, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5948.Conv2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_5949.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_5950.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk1.fbnetv2_1_0.pwl.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5948.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5948.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5948.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5948.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5948.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d(argument_1, _packed_params, 0.29513248801231384, 61)
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk1.fbnetv2_1_0.pwl.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_5949.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk1.fbnetv2_1_1, type=IRFBlock:
class IRFBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  pw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_5955.ConvBNRelu
  dw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_5957.ConvBNRelu
  pwl : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_5960.ConvBNRelu
  res_conn : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_5963.TorchAdd
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_5964.IRFBlock,
    argument_1: Tensor) -> Tensor:
    res_conn = self.res_conn
    pwl = self.pwl
    dw = self.dw
    pw = self.pw
    _0 = (dw).forward((pw).forward(argument_1, ), )
    _1 = (res_conn).forward((pwl).forward(_0, ), argument_1, )
    return _1

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk1.fbnetv2_1_1.pw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_5952.ConvReLU2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_5953.Identity
  relu : __torch__.torch.nn.modules.linear.___torch_mangle_5954.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_5955.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    relu = self.relu
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    _2 = (relu).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk1.fbnetv2_1_1.pw.conv, type=ConvReLU2d:
class ConvReLU2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_5952.ConvReLU2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_5952.ConvReLU2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_5952.ConvReLU2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_5952.ConvReLU2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_5952.ConvReLU2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d_relu(argument_1, _packed_params, 0.049111630767583847, 0)
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk1.fbnetv2_1_1.pw.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_5953.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk1.fbnetv2_1_1.pw.relu, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_5954.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk1.fbnetv2_1_1.dw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5956.Conv2d
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_5957.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    conv = self.conv
    return (conv).forward(argument_1, )

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk1.fbnetv2_1_1.dw.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5956.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5956.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5956.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5956.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5956.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d(argument_1, _packed_params, 0.027580782771110535, 64)
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk1.fbnetv2_1_1.pwl, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5958.Conv2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_5959.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_5960.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk1.fbnetv2_1_1.pwl.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5958.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5958.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5958.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5958.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5958.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    _18 = ops.quantized.conv2d(argument_1, _packed_params, 0.14964845776557922, 61)
    return _18

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk1.fbnetv2_1_1.pwl.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_5959.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk1.fbnetv2_1_1.res_conn, type=TorchAdd:
class TorchAdd(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  add_func : __torch__.torch.ao.nn.quantized.modules.functional_modules.___torch_mangle_5962.QFunctional
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_5963.TorchAdd,
    argument_1: Tensor,
    argument_2: Tensor) -> Tensor:
    add_func = self.add_func
    activation_post_process = add_func.activation_post_process
    input = ops.quantized.add(argument_1, argument_2, 0.2888323962688446, 60)
    _0 = (activation_post_process).forward()
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk1.fbnetv2_1_1.res_conn.add_func, type=QFunctional:
class QFunctional(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation_post_process : __torch__.torch.nn.modules.linear.___torch_mangle_5961.Identity

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk1.fbnetv2_1_1.res_conn.add_func.activation_post_process, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_5961.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk1.fbnetv2_1_2, type=IRFBlock:
class IRFBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  pw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_5968.ConvBNRelu
  dw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_5970.ConvBNRelu
  pwl : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_5973.ConvBNRelu
  res_conn : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_5976.TorchAdd
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_5977.IRFBlock,
    argument_1: Tensor) -> Tensor:
    res_conn = self.res_conn
    pwl = self.pwl
    dw = self.dw
    pw = self.pw
    _0 = (dw).forward((pw).forward(argument_1, ), )
    _1 = (res_conn).forward((pwl).forward(_0, ), argument_1, )
    return _1

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk1.fbnetv2_1_2.pw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_5965.ConvReLU2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_5966.Identity
  relu : __torch__.torch.nn.modules.linear.___torch_mangle_5967.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_5968.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    relu = self.relu
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    _2 = (relu).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk1.fbnetv2_1_2.pw.conv, type=ConvReLU2d:
class ConvReLU2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_5965.ConvReLU2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_5965.ConvReLU2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_5965.ConvReLU2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_5965.ConvReLU2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_5965.ConvReLU2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d_relu(argument_1, _packed_params, 0.063758514821529388, 0)
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk1.fbnetv2_1_2.pw.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_5966.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk1.fbnetv2_1_2.pw.relu, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_5967.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk1.fbnetv2_1_2.dw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5969.Conv2d
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_5970.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    conv = self.conv
    return (conv).forward(argument_1, )

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk1.fbnetv2_1_2.dw.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5969.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5969.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5969.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5969.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5969.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d(argument_1, _packed_params, 0.0355181023478508, 68)
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk1.fbnetv2_1_2.pwl, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5971.Conv2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_5972.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_5973.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk1.fbnetv2_1_2.pwl.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5971.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5971.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5971.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5971.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5971.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    _18 = ops.quantized.conv2d(argument_1, _packed_params, 0.17543162405490875, 68)
    return _18

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk1.fbnetv2_1_2.pwl.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_5972.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk1.fbnetv2_1_2.res_conn, type=TorchAdd:
class TorchAdd(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  add_func : __torch__.torch.ao.nn.quantized.modules.functional_modules.___torch_mangle_5975.QFunctional
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_5976.TorchAdd,
    argument_1: Tensor,
    argument_2: Tensor) -> Tensor:
    add_func = self.add_func
    activation_post_process = add_func.activation_post_process
    input = ops.quantized.add(argument_1, argument_2, 0.31409719586372375, 62)
    _0 = (activation_post_process).forward()
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk1.fbnetv2_1_2.res_conn.add_func, type=QFunctional:
class QFunctional(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation_post_process : __torch__.torch.nn.modules.linear.___torch_mangle_5974.Identity

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk1.fbnetv2_1_2.res_conn.add_func.activation_post_process, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_5974.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk1.fbnetv2_1_3, type=IRFBlock:
class IRFBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  pw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_5981.ConvBNRelu
  dw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_5983.ConvBNRelu
  pwl : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_5986.ConvBNRelu
  res_conn : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_5989.TorchAdd
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_5990.IRFBlock,
    argument_1: Tensor) -> Tensor:
    res_conn = self.res_conn
    pwl = self.pwl
    dw = self.dw
    pw = self.pw
    _0 = (dw).forward((pw).forward(argument_1, ), )
    _1 = (res_conn).forward((pwl).forward(_0, ), argument_1, )
    return _1

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk1.fbnetv2_1_3.pw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_5978.ConvReLU2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_5979.Identity
  relu : __torch__.torch.nn.modules.linear.___torch_mangle_5980.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_5981.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    relu = self.relu
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    _2 = (relu).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk1.fbnetv2_1_3.pw.conv, type=ConvReLU2d:
class ConvReLU2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_5978.ConvReLU2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_5978.ConvReLU2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_5978.ConvReLU2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_5978.ConvReLU2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_5978.ConvReLU2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d_relu(argument_1, _packed_params, 0.047454677522182465, 0)
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk1.fbnetv2_1_3.pw.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_5979.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk1.fbnetv2_1_3.pw.relu, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_5980.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk1.fbnetv2_1_3.dw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5982.Conv2d
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_5983.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    conv = self.conv
    return (conv).forward(argument_1, )

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk1.fbnetv2_1_3.dw.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5982.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5982.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5982.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5982.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5982.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d(argument_1, _packed_params, 0.027043014764785767, 66)
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk1.fbnetv2_1_3.pwl, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5984.Conv2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_5985.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_5986.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk1.fbnetv2_1_3.pwl.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5984.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5984.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5984.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5984.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5984.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    _18 = ops.quantized.conv2d(argument_1, _packed_params, 0.21583858132362366, 63)
    return _18

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk1.fbnetv2_1_3.pwl.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_5985.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk1.fbnetv2_1_3.res_conn, type=TorchAdd:
class TorchAdd(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  add_func : __torch__.torch.ao.nn.quantized.modules.functional_modules.___torch_mangle_5988.QFunctional
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_5989.TorchAdd,
    argument_1: Tensor,
    argument_2: Tensor) -> Tensor:
    add_func = self.add_func
    activation_post_process = add_func.activation_post_process
    input = ops.quantized.add(argument_1, argument_2, 0.33375874161720276, 63)
    _0 = (activation_post_process).forward()
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk1.fbnetv2_1_3.res_conn.add_func, type=QFunctional:
class QFunctional(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation_post_process : __torch__.torch.nn.modules.linear.___torch_mangle_5987.Identity

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk1.fbnetv2_1_3.res_conn.add_func.activation_post_process, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_5987.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2, type=Sequential:
class Sequential(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  fbnetv2_2_0 : __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6012.IRFBlock
  fbnetv2_2_1 : __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6036.IRFBlock
  fbnetv2_2_2 : __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6060.IRFBlock
  fbnetv2_2_3 : __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6084.IRFBlock
  def forward(self: __torch__.torch.nn.modules.container.___torch_mangle_6085.Sequential,
    argument_1: Tensor) -> Tensor:
    fbnetv2_2_3 = self.fbnetv2_2_3
    fbnetv2_2_2 = self.fbnetv2_2_2
    fbnetv2_2_1 = self.fbnetv2_2_1
    fbnetv2_2_0 = self.fbnetv2_2_0
    _0 = (fbnetv2_2_1).forward((fbnetv2_2_0).forward(argument_1, ), )
    _1 = (fbnetv2_2_3).forward((fbnetv2_2_2).forward(_0, ), )
    return _1

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_0, type=IRFBlock:
class IRFBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  pw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_5995.ConvBNRelu
  dw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_5997.ConvBNRelu
  se : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6008.SEModule
  pwl : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6011.ConvBNRelu
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6012.IRFBlock,
    argument_1: Tensor) -> Tensor:
    pwl = self.pwl
    se = self.se
    dw = self.dw
    pw = self.pw
    _0 = (dw).forward((pw).forward(argument_1, ), )
    return (pwl).forward((se).forward(_0, ), )

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_0.pw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_5992.ConvReLU2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_5993.Identity
  relu : __torch__.torch.nn.modules.linear.___torch_mangle_5994.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_5995.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    relu = self.relu
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    _2 = (relu).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_0.pw.conv, type=ConvReLU2d:
class ConvReLU2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_5992.ConvReLU2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_5992.ConvReLU2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_5992.ConvReLU2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_5992.ConvReLU2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_5992.ConvReLU2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d_relu(argument_1, _packed_params, 0.10718471556901932, 0)
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_0.pw.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_5993.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_0.pw.relu, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_5994.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_0.dw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5996.Conv2d
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_5997.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    conv = self.conv
    return (conv).forward(argument_1, )

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_0.dw.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5996.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5996.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5996.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5996.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_5996.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    x = ops.quantized.conv2d(argument_1, _packed_params, 0.20592471957206726, 57)
    return x

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_0.se, type=SEModule:
class SEModule(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  avg_pool : __torch__.torch.nn.modules.pooling.___torch_mangle_5998.AdaptiveAvgPool2d
  se : __torch__.torch.nn.modules.container.___torch_mangle_6004.Sequential
  mul : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6007.TorchMultiply
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6008.SEModule,
    argument_1: Tensor) -> Tensor:
    mul = self.mul
    se = self.se
    avg_pool = self.avg_pool
    _0 = (se).forward((avg_pool).forward(argument_1, ), )
    return (mul).forward(argument_1, _0, )

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_0.se.avg_pool, type=AdaptiveAvgPool2d:
class AdaptiveAvgPool2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.pooling.___torch_mangle_5998.AdaptiveAvgPool2d,
    argument_1: Tensor) -> Tensor:
    input = torch.adaptive_avg_pool2d(argument_1, [1, 1])
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_0.se.se, type=Sequential:
class Sequential(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  __annotations__["0"] = __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6001.ConvBNRelu
  __annotations__["1"] = __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6002.Conv2d
  __annotations__["2"] = __torch__.torch.nn.modules.activation.___torch_mangle_6003.Sigmoid
  def forward(self: __torch__.torch.nn.modules.container.___torch_mangle_6004.Sequential,
    argument_1: Tensor) -> Tensor:
    _2 = getattr(self, "2")
    _1 = getattr(self, "1")
    _0 = getattr(self, "0")
    _3 = (_1).forward((_0).forward(argument_1, ), )
    return (_2).forward(_3, )

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_0.se.se.0, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_5999.ConvReLU2d
  relu : __torch__.torch.nn.modules.linear.___torch_mangle_6000.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6001.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    relu = self.relu
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (relu).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_0.se.se.0.conv, type=ConvReLU2d:
class ConvReLU2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_5999.ConvReLU2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_5999.ConvReLU2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_5999.ConvReLU2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_5999.ConvReLU2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_5999.ConvReLU2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d_relu(argument_1, _packed_params, 0.023411024361848831, 0)
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_0.se.se.0.relu, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6000.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_0.se.se.1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6002.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6002.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6002.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6002.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6002.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d(argument_1, _packed_params, 0.026023637503385544, 106)
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_0.se.se.2, type=Sigmoid:
class Sigmoid(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.activation.___torch_mangle_6003.Sigmoid,
    argument_1: Tensor) -> Tensor:
    return torch.sigmoid(argument_1)

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_0.se.mul, type=TorchMultiply:
class TorchMultiply(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  mul_func : __torch__.torch.ao.nn.quantized.modules.functional_modules.___torch_mangle_6006.QFunctional
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6007.TorchMultiply,
    argument_1: Tensor,
    argument_2: Tensor) -> Tensor:
    mul_func = self.mul_func
    activation_post_process = mul_func.activation_post_process
    input = ops.quantized.mul(argument_1, argument_2, 0.032910376787185669, 64)
    _0 = (activation_post_process).forward()
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_0.se.mul.mul_func, type=QFunctional:
class QFunctional(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation_post_process : __torch__.torch.nn.modules.linear.___torch_mangle_6005.Identity

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_0.se.mul.mul_func.activation_post_process, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6005.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_0.pwl, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6009.Conv2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6010.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6011.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_0.pwl.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6009.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6009.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6009.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6009.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6009.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d(argument_1, _packed_params, 0.20294657349586487, 60)
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_0.pwl.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6010.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_1, type=IRFBlock:
class IRFBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  pw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6016.ConvBNRelu
  dw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6018.ConvBNRelu
  se : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6029.SEModule
  pwl : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6032.ConvBNRelu
  res_conn : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6035.TorchAdd
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6036.IRFBlock,
    argument_1: Tensor) -> Tensor:
    res_conn = self.res_conn
    pwl = self.pwl
    se = self.se
    dw = self.dw
    pw = self.pw
    _0 = (dw).forward((pw).forward(argument_1, ), )
    _1 = (res_conn).forward((pwl).forward((se).forward(_0, ), ), argument_1, )
    return _1

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_1.pw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6013.ConvReLU2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6014.Identity
  relu : __torch__.torch.nn.modules.linear.___torch_mangle_6015.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6016.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    relu = self.relu
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    _2 = (relu).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_1.pw.conv, type=ConvReLU2d:
class ConvReLU2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6013.ConvReLU2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6013.ConvReLU2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6013.ConvReLU2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6013.ConvReLU2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6013.ConvReLU2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d_relu(argument_1, _packed_params, 0.044782720506191254, 0)
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_1.pw.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6014.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_1.pw.relu, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6015.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_1.dw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6017.Conv2d
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6018.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    conv = self.conv
    return (conv).forward(argument_1, )

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_1.dw.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6017.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6017.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6017.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6017.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6017.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    x = ops.quantized.conv2d(argument_1, _packed_params, 0.025387858971953392, 74)
    return x

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_1.se, type=SEModule:
class SEModule(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  avg_pool : __torch__.torch.nn.modules.pooling.___torch_mangle_6019.AdaptiveAvgPool2d
  se : __torch__.torch.nn.modules.container.___torch_mangle_6025.Sequential
  mul : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6028.TorchMultiply
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6029.SEModule,
    argument_1: Tensor) -> Tensor:
    mul = self.mul
    se = self.se
    avg_pool = self.avg_pool
    _0 = (se).forward((avg_pool).forward(argument_1, ), )
    return (mul).forward(argument_1, _0, )

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_1.se.avg_pool, type=AdaptiveAvgPool2d:
class AdaptiveAvgPool2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.pooling.___torch_mangle_6019.AdaptiveAvgPool2d,
    argument_1: Tensor) -> Tensor:
    input = torch.adaptive_avg_pool2d(argument_1, [1, 1])
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_1.se.se, type=Sequential:
class Sequential(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  __annotations__["0"] = __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6022.ConvBNRelu
  __annotations__["1"] = __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6023.Conv2d
  __annotations__["2"] = __torch__.torch.nn.modules.activation.___torch_mangle_6024.Sigmoid
  def forward(self: __torch__.torch.nn.modules.container.___torch_mangle_6025.Sequential,
    argument_1: Tensor) -> Tensor:
    _2 = getattr(self, "2")
    _1 = getattr(self, "1")
    _0 = getattr(self, "0")
    _3 = (_1).forward((_0).forward(argument_1, ), )
    return (_2).forward(_3, )

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_1.se.se.0, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6020.ConvReLU2d
  relu : __torch__.torch.nn.modules.linear.___torch_mangle_6021.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6022.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    relu = self.relu
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (relu).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_1.se.se.0.conv, type=ConvReLU2d:
class ConvReLU2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6020.ConvReLU2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6020.ConvReLU2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6020.ConvReLU2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6020.ConvReLU2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6020.ConvReLU2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d_relu(argument_1, _packed_params, 1.1920928955078125e-07, 0)
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_1.se.se.0.relu, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6021.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_1.se.se.1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6023.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6023.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6023.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6023.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6023.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d(argument_1, _packed_params, 0.00023171157226897776, 43)
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_1.se.se.2, type=Sigmoid:
class Sigmoid(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.activation.___torch_mangle_6024.Sigmoid,
    argument_1: Tensor) -> Tensor:
    return torch.sigmoid(argument_1)

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_1.se.mul, type=TorchMultiply:
class TorchMultiply(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  mul_func : __torch__.torch.ao.nn.quantized.modules.functional_modules.___torch_mangle_6027.QFunctional
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6028.TorchMultiply,
    argument_1: Tensor,
    argument_2: Tensor) -> Tensor:
    mul_func = self.mul_func
    activation_post_process = mul_func.activation_post_process
    input = ops.quantized.mul(argument_1, argument_2, 0.012559558264911175, 75)
    _0 = (activation_post_process).forward()
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_1.se.mul.mul_func, type=QFunctional:
class QFunctional(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation_post_process : __torch__.torch.nn.modules.linear.___torch_mangle_6026.Identity

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_1.se.mul.mul_func.activation_post_process, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6026.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_1.pwl, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6030.Conv2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6031.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6032.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_1.pwl.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6030.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6030.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6030.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6030.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6030.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    _18 = ops.quantized.conv2d(argument_1, _packed_params, 0.20045576989650726, 70)
    return _18

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_1.pwl.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6031.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_1.res_conn, type=TorchAdd:
class TorchAdd(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  add_func : __torch__.torch.ao.nn.quantized.modules.functional_modules.___torch_mangle_6034.QFunctional
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6035.TorchAdd,
    argument_1: Tensor,
    argument_2: Tensor) -> Tensor:
    add_func = self.add_func
    activation_post_process = add_func.activation_post_process
    input = ops.quantized.add(argument_1, argument_2, 0.25609633326530457, 62)
    _0 = (activation_post_process).forward()
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_1.res_conn.add_func, type=QFunctional:
class QFunctional(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation_post_process : __torch__.torch.nn.modules.linear.___torch_mangle_6033.Identity

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_1.res_conn.add_func.activation_post_process, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6033.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_2, type=IRFBlock:
class IRFBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  pw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6040.ConvBNRelu
  dw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6042.ConvBNRelu
  se : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6053.SEModule
  pwl : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6056.ConvBNRelu
  res_conn : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6059.TorchAdd
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6060.IRFBlock,
    argument_1: Tensor) -> Tensor:
    res_conn = self.res_conn
    pwl = self.pwl
    se = self.se
    dw = self.dw
    pw = self.pw
    _0 = (dw).forward((pw).forward(argument_1, ), )
    _1 = (res_conn).forward((pwl).forward((se).forward(_0, ), ), argument_1, )
    return _1

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_2.pw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6037.ConvReLU2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6038.Identity
  relu : __torch__.torch.nn.modules.linear.___torch_mangle_6039.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6040.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    relu = self.relu
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    _2 = (relu).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_2.pw.conv, type=ConvReLU2d:
class ConvReLU2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6037.ConvReLU2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6037.ConvReLU2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6037.ConvReLU2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6037.ConvReLU2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6037.ConvReLU2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d_relu(argument_1, _packed_params, 0.037047538906335831, 0)
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_2.pw.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6038.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_2.pw.relu, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6039.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_2.dw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6041.Conv2d
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6042.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    conv = self.conv
    return (conv).forward(argument_1, )

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_2.dw.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6041.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6041.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6041.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6041.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6041.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    x = ops.quantized.conv2d(argument_1, _packed_params, 0.013652492314577103, 55)
    return x

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_2.se, type=SEModule:
class SEModule(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  avg_pool : __torch__.torch.nn.modules.pooling.___torch_mangle_6043.AdaptiveAvgPool2d
  se : __torch__.torch.nn.modules.container.___torch_mangle_6049.Sequential
  mul : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6052.TorchMultiply
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6053.SEModule,
    argument_1: Tensor) -> Tensor:
    mul = self.mul
    se = self.se
    avg_pool = self.avg_pool
    _0 = (se).forward((avg_pool).forward(argument_1, ), )
    return (mul).forward(argument_1, _0, )

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_2.se.avg_pool, type=AdaptiveAvgPool2d:
class AdaptiveAvgPool2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.pooling.___torch_mangle_6043.AdaptiveAvgPool2d,
    argument_1: Tensor) -> Tensor:
    input = torch.adaptive_avg_pool2d(argument_1, [1, 1])
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_2.se.se, type=Sequential:
class Sequential(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  __annotations__["0"] = __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6046.ConvBNRelu
  __annotations__["1"] = __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6047.Conv2d
  __annotations__["2"] = __torch__.torch.nn.modules.activation.___torch_mangle_6048.Sigmoid
  def forward(self: __torch__.torch.nn.modules.container.___torch_mangle_6049.Sequential,
    argument_1: Tensor) -> Tensor:
    _2 = getattr(self, "2")
    _1 = getattr(self, "1")
    _0 = getattr(self, "0")
    _3 = (_1).forward((_0).forward(argument_1, ), )
    return (_2).forward(_3, )

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_2.se.se.0, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6044.ConvReLU2d
  relu : __torch__.torch.nn.modules.linear.___torch_mangle_6045.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6046.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    relu = self.relu
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (relu).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_2.se.se.0.conv, type=ConvReLU2d:
class ConvReLU2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6044.ConvReLU2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6044.ConvReLU2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6044.ConvReLU2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6044.ConvReLU2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6044.ConvReLU2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d_relu(argument_1, _packed_params, 1.1920928955078125e-07, 0)
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_2.se.se.0.relu, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6045.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_2.se.se.1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6047.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6047.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6047.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6047.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6047.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d(argument_1, _packed_params, 0.00023374080774374306, 36)
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_2.se.se.2, type=Sigmoid:
class Sigmoid(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.activation.___torch_mangle_6048.Sigmoid,
    argument_1: Tensor) -> Tensor:
    return torch.sigmoid(argument_1)

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_2.se.mul, type=TorchMultiply:
class TorchMultiply(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  mul_func : __torch__.torch.ao.nn.quantized.modules.functional_modules.___torch_mangle_6051.QFunctional
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6052.TorchMultiply,
    argument_1: Tensor,
    argument_2: Tensor) -> Tensor:
    mul_func = self.mul_func
    activation_post_process = mul_func.activation_post_process
    input = ops.quantized.mul(argument_1, argument_2, 0.0082591529935598373, 61)
    _0 = (activation_post_process).forward()
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_2.se.mul.mul_func, type=QFunctional:
class QFunctional(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation_post_process : __torch__.torch.nn.modules.linear.___torch_mangle_6050.Identity

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_2.se.mul.mul_func.activation_post_process, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6050.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_2.pwl, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6054.Conv2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6055.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6056.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_2.pwl.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6054.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6054.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6054.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6054.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6054.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    _18 = ops.quantized.conv2d(argument_1, _packed_params, 0.12843473255634308, 64)
    return _18

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_2.pwl.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6055.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_2.res_conn, type=TorchAdd:
class TorchAdd(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  add_func : __torch__.torch.ao.nn.quantized.modules.functional_modules.___torch_mangle_6058.QFunctional
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6059.TorchAdd,
    argument_1: Tensor,
    argument_2: Tensor) -> Tensor:
    add_func = self.add_func
    activation_post_process = add_func.activation_post_process
    input = ops.quantized.add(argument_1, argument_2, 0.24500380456447601, 64)
    _0 = (activation_post_process).forward()
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_2.res_conn.add_func, type=QFunctional:
class QFunctional(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation_post_process : __torch__.torch.nn.modules.linear.___torch_mangle_6057.Identity

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_2.res_conn.add_func.activation_post_process, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6057.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_3, type=IRFBlock:
class IRFBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  pw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6064.ConvBNRelu
  dw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6066.ConvBNRelu
  se : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6077.SEModule
  pwl : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6080.ConvBNRelu
  res_conn : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6083.TorchAdd
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6084.IRFBlock,
    argument_1: Tensor) -> Tensor:
    res_conn = self.res_conn
    pwl = self.pwl
    se = self.se
    dw = self.dw
    pw = self.pw
    _0 = (dw).forward((pw).forward(argument_1, ), )
    _1 = (res_conn).forward((pwl).forward((se).forward(_0, ), ), argument_1, )
    return _1

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_3.pw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6061.ConvReLU2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6062.Identity
  relu : __torch__.torch.nn.modules.linear.___torch_mangle_6063.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6064.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    relu = self.relu
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    _2 = (relu).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_3.pw.conv, type=ConvReLU2d:
class ConvReLU2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6061.ConvReLU2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6061.ConvReLU2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6061.ConvReLU2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6061.ConvReLU2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6061.ConvReLU2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d_relu(argument_1, _packed_params, 0.036591872572898865, 0)
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_3.pw.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6062.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_3.pw.relu, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6063.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_3.dw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6065.Conv2d
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6066.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    conv = self.conv
    return (conv).forward(argument_1, )

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_3.dw.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6065.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6065.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6065.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6065.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6065.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    x = ops.quantized.conv2d(argument_1, _packed_params, 0.012937571853399277, 61)
    return x

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_3.se, type=SEModule:
class SEModule(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  avg_pool : __torch__.torch.nn.modules.pooling.___torch_mangle_6067.AdaptiveAvgPool2d
  se : __torch__.torch.nn.modules.container.___torch_mangle_6073.Sequential
  mul : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6076.TorchMultiply
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6077.SEModule,
    argument_1: Tensor) -> Tensor:
    mul = self.mul
    se = self.se
    avg_pool = self.avg_pool
    _0 = (se).forward((avg_pool).forward(argument_1, ), )
    return (mul).forward(argument_1, _0, )

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_3.se.avg_pool, type=AdaptiveAvgPool2d:
class AdaptiveAvgPool2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.pooling.___torch_mangle_6067.AdaptiveAvgPool2d,
    argument_1: Tensor) -> Tensor:
    input = torch.adaptive_avg_pool2d(argument_1, [1, 1])
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_3.se.se, type=Sequential:
class Sequential(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  __annotations__["0"] = __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6070.ConvBNRelu
  __annotations__["1"] = __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6071.Conv2d
  __annotations__["2"] = __torch__.torch.nn.modules.activation.___torch_mangle_6072.Sigmoid
  def forward(self: __torch__.torch.nn.modules.container.___torch_mangle_6073.Sequential,
    argument_1: Tensor) -> Tensor:
    _2 = getattr(self, "2")
    _1 = getattr(self, "1")
    _0 = getattr(self, "0")
    _3 = (_1).forward((_0).forward(argument_1, ), )
    return (_2).forward(_3, )

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_3.se.se.0, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6068.ConvReLU2d
  relu : __torch__.torch.nn.modules.linear.___torch_mangle_6069.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6070.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    relu = self.relu
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (relu).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_3.se.se.0.conv, type=ConvReLU2d:
class ConvReLU2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6068.ConvReLU2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6068.ConvReLU2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6068.ConvReLU2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6068.ConvReLU2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6068.ConvReLU2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d_relu(argument_1, _packed_params, 1.1920928955078125e-07, 0)
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_3.se.se.0.relu, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6069.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_3.se.se.1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6071.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6071.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6071.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6071.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6071.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d(argument_1, _packed_params, 0.00020581403805408627, 56)
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_3.se.se.2, type=Sigmoid:
class Sigmoid(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.activation.___torch_mangle_6072.Sigmoid,
    argument_1: Tensor) -> Tensor:
    return torch.sigmoid(argument_1)

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_3.se.mul, type=TorchMultiply:
class TorchMultiply(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  mul_func : __torch__.torch.ao.nn.quantized.modules.functional_modules.___torch_mangle_6075.QFunctional
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6076.TorchMultiply,
    argument_1: Tensor,
    argument_2: Tensor) -> Tensor:
    mul_func = self.mul_func
    activation_post_process = mul_func.activation_post_process
    input = ops.quantized.mul(argument_1, argument_2, 0.0065461643971502781, 62)
    _0 = (activation_post_process).forward()
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_3.se.mul.mul_func, type=QFunctional:
class QFunctional(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation_post_process : __torch__.torch.nn.modules.linear.___torch_mangle_6074.Identity

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_3.se.mul.mul_func.activation_post_process, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6074.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_3.pwl, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6078.Conv2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6079.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6080.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_3.pwl.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6078.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6078.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6078.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6078.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6078.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    _18 = ops.quantized.conv2d(argument_1, _packed_params, 0.12938554584980011, 61)
    return _18

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_3.pwl.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6079.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_3.res_conn, type=TorchAdd:
class TorchAdd(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  add_func : __torch__.torch.ao.nn.quantized.modules.functional_modules.___torch_mangle_6082.QFunctional
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6083.TorchAdd,
    argument_1: Tensor,
    argument_2: Tensor) -> Tensor:
    add_func = self.add_func
    activation_post_process = add_func.activation_post_process
    input = ops.quantized.add(argument_1, argument_2, 0.28174278140068054, 62)
    _0 = (activation_post_process).forward()
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_3.res_conn.add_func, type=QFunctional:
class QFunctional(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation_post_process : __torch__.torch.nn.modules.linear.___torch_mangle_6081.Identity

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk2.fbnetv2_2_3.res_conn.add_func.activation_post_process, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6081.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3, type=Sequential:
class Sequential(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  fbnetv2_3_0 : __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6095.IRFBlock
  fbnetv2_3_1 : __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6108.IRFBlock
  fbnetv2_3_2 : __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6121.IRFBlock
  fbnetv2_3_3 : __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6134.IRFBlock
  fbnetv2_3_4 : __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6155.IRFBlock
  fbnetv2_3_5 : __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6179.IRFBlock
  fbnetv2_3_6 : __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6203.IRFBlock
  fbnetv2_3_7 : __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6227.IRFBlock
  fbnetv2_3_8 : __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6251.IRFBlock
  fbnetv2_3_9 : __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6275.IRFBlock
  def forward(self: __torch__.torch.nn.modules.container.___torch_mangle_6276.Sequential,
    argument_1: Tensor) -> Tensor:
    fbnetv2_3_9 = self.fbnetv2_3_9
    fbnetv2_3_8 = self.fbnetv2_3_8
    fbnetv2_3_7 = self.fbnetv2_3_7
    fbnetv2_3_6 = self.fbnetv2_3_6
    fbnetv2_3_5 = self.fbnetv2_3_5
    fbnetv2_3_4 = self.fbnetv2_3_4
    fbnetv2_3_3 = self.fbnetv2_3_3
    fbnetv2_3_2 = self.fbnetv2_3_2
    fbnetv2_3_1 = self.fbnetv2_3_1
    fbnetv2_3_0 = self.fbnetv2_3_0
    _0 = (fbnetv2_3_1).forward((fbnetv2_3_0).forward(argument_1, ), )
    _1 = (fbnetv2_3_3).forward((fbnetv2_3_2).forward(_0, ), )
    _2 = (fbnetv2_3_5).forward((fbnetv2_3_4).forward(_1, ), )
    _3 = (fbnetv2_3_7).forward((fbnetv2_3_6).forward(_2, ), )
    _4 = (fbnetv2_3_9).forward((fbnetv2_3_8).forward(_3, ), )
    return _4

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_0, type=IRFBlock:
class IRFBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  pw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6089.ConvBNRelu
  dw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6091.ConvBNRelu
  pwl : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6094.ConvBNRelu
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6095.IRFBlock,
    argument_1: Tensor) -> Tensor:
    pwl = self.pwl
    dw = self.dw
    pw = self.pw
    _0 = (dw).forward((pw).forward(argument_1, ), )
    return (pwl).forward(_0, )

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_0.pw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6086.ConvReLU2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6087.Identity
  relu : __torch__.torch.nn.modules.linear.___torch_mangle_6088.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6089.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    relu = self.relu
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    _2 = (relu).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_0.pw.conv, type=ConvReLU2d:
class ConvReLU2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6086.ConvReLU2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6086.ConvReLU2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6086.ConvReLU2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6086.ConvReLU2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6086.ConvReLU2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d_relu(argument_1, _packed_params, 0.054890166968107224, 0)
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_0.pw.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6087.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_0.pw.relu, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6088.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_0.dw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6090.Conv2d
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6091.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    conv = self.conv
    return (conv).forward(argument_1, )

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_0.dw.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6090.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6090.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6090.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6090.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6090.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d(argument_1, _packed_params, 0.039220698177814484, 73)
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_0.pwl, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6092.Conv2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6093.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6094.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_0.pwl.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6092.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6092.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6092.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6092.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6092.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d(argument_1, _packed_params, 0.22968432307243347, 56)
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_0.pwl.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6093.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_1, type=IRFBlock:
class IRFBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  pw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6099.ConvBNRelu
  dw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6101.ConvBNRelu
  pwl : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6104.ConvBNRelu
  res_conn : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6107.TorchAdd
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6108.IRFBlock,
    argument_1: Tensor) -> Tensor:
    res_conn = self.res_conn
    pwl = self.pwl
    dw = self.dw
    pw = self.pw
    _0 = (dw).forward((pw).forward(argument_1, ), )
    _1 = (res_conn).forward((pwl).forward(_0, ), argument_1, )
    return _1

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_1.pw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6096.ConvReLU2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6097.Identity
  relu : __torch__.torch.nn.modules.linear.___torch_mangle_6098.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6099.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    relu = self.relu
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    _2 = (relu).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_1.pw.conv, type=ConvReLU2d:
class ConvReLU2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6096.ConvReLU2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6096.ConvReLU2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6096.ConvReLU2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6096.ConvReLU2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6096.ConvReLU2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d_relu(argument_1, _packed_params, 0.040818940848112106, 0)
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_1.pw.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6097.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_1.pw.relu, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6098.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_1.dw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6100.Conv2d
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6101.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    conv = self.conv
    return (conv).forward(argument_1, )

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_1.dw.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6100.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6100.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6100.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6100.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6100.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d(argument_1, _packed_params, 0.013504836708307266, 64)
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_1.pwl, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6102.Conv2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6103.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6104.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_1.pwl.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6102.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6102.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6102.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6102.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6102.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    _18 = ops.quantized.conv2d(argument_1, _packed_params, 0.16801944375038147, 67)
    return _18

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_1.pwl.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6103.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_1.res_conn, type=TorchAdd:
class TorchAdd(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  add_func : __torch__.torch.ao.nn.quantized.modules.functional_modules.___torch_mangle_6106.QFunctional
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6107.TorchAdd,
    argument_1: Tensor,
    argument_2: Tensor) -> Tensor:
    add_func = self.add_func
    activation_post_process = add_func.activation_post_process
    input = ops.quantized.add(argument_1, argument_2, 0.23104783892631531, 58)
    _0 = (activation_post_process).forward()
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_1.res_conn.add_func, type=QFunctional:
class QFunctional(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation_post_process : __torch__.torch.nn.modules.linear.___torch_mangle_6105.Identity

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_1.res_conn.add_func.activation_post_process, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6105.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_2, type=IRFBlock:
class IRFBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  pw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6112.ConvBNRelu
  dw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6114.ConvBNRelu
  pwl : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6117.ConvBNRelu
  res_conn : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6120.TorchAdd
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6121.IRFBlock,
    argument_1: Tensor) -> Tensor:
    res_conn = self.res_conn
    pwl = self.pwl
    dw = self.dw
    pw = self.pw
    _0 = (dw).forward((pw).forward(argument_1, ), )
    _1 = (res_conn).forward((pwl).forward(_0, ), argument_1, )
    return _1

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_2.pw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6109.ConvReLU2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6110.Identity
  relu : __torch__.torch.nn.modules.linear.___torch_mangle_6111.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6112.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    relu = self.relu
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    _2 = (relu).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_2.pw.conv, type=ConvReLU2d:
class ConvReLU2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6109.ConvReLU2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6109.ConvReLU2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6109.ConvReLU2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6109.ConvReLU2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6109.ConvReLU2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d_relu(argument_1, _packed_params, 0.033009432256221771, 0)
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_2.pw.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6110.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_2.pw.relu, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6111.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_2.dw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6113.Conv2d
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6114.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    conv = self.conv
    return (conv).forward(argument_1, )

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_2.dw.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6113.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6113.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6113.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6113.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6113.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d(argument_1, _packed_params, 0.011241423897445202, 72)
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_2.pwl, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6115.Conv2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6116.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6117.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_2.pwl.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6115.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6115.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6115.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6115.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6115.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    _18 = ops.quantized.conv2d(argument_1, _packed_params, 0.1849026083946228, 79)
    return _18

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_2.pwl.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6116.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_2.res_conn, type=TorchAdd:
class TorchAdd(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  add_func : __torch__.torch.ao.nn.quantized.modules.functional_modules.___torch_mangle_6119.QFunctional
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6120.TorchAdd,
    argument_1: Tensor,
    argument_2: Tensor) -> Tensor:
    add_func = self.add_func
    activation_post_process = add_func.activation_post_process
    input = ops.quantized.add(argument_1, argument_2, 0.22170177102088928, 61)
    _0 = (activation_post_process).forward()
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_2.res_conn.add_func, type=QFunctional:
class QFunctional(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation_post_process : __torch__.torch.nn.modules.linear.___torch_mangle_6118.Identity

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_2.res_conn.add_func.activation_post_process, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6118.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_3, type=IRFBlock:
class IRFBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  pw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6125.ConvBNRelu
  dw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6127.ConvBNRelu
  pwl : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6130.ConvBNRelu
  res_conn : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6133.TorchAdd
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6134.IRFBlock,
    argument_1: Tensor) -> Tensor:
    res_conn = self.res_conn
    pwl = self.pwl
    dw = self.dw
    pw = self.pw
    _0 = (dw).forward((pw).forward(argument_1, ), )
    _1 = (res_conn).forward((pwl).forward(_0, ), argument_1, )
    return _1

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_3.pw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6122.ConvReLU2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6123.Identity
  relu : __torch__.torch.nn.modules.linear.___torch_mangle_6124.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6125.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    relu = self.relu
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    _2 = (relu).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_3.pw.conv, type=ConvReLU2d:
class ConvReLU2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6122.ConvReLU2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6122.ConvReLU2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6122.ConvReLU2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6122.ConvReLU2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6122.ConvReLU2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d_relu(argument_1, _packed_params, 0.021955253556370735, 0)
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_3.pw.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6123.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_3.pw.relu, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6124.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_3.dw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6126.Conv2d
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6127.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    conv = self.conv
    return (conv).forward(argument_1, )

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_3.dw.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6126.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6126.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6126.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6126.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6126.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d(argument_1, _packed_params, 0.0094504542648792267, 70)
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_3.pwl, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6128.Conv2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6129.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6130.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_3.pwl.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6128.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6128.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6128.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6128.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6128.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    _18 = ops.quantized.conv2d(argument_1, _packed_params, 0.12464980036020279, 65)
    return _18

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_3.pwl.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6129.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_3.res_conn, type=TorchAdd:
class TorchAdd(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  add_func : __torch__.torch.ao.nn.quantized.modules.functional_modules.___torch_mangle_6132.QFunctional
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6133.TorchAdd,
    argument_1: Tensor,
    argument_2: Tensor) -> Tensor:
    add_func = self.add_func
    activation_post_process = add_func.activation_post_process
    input = ops.quantized.add(argument_1, argument_2, 0.23290327191352844, 61)
    _0 = (activation_post_process).forward()
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_3.res_conn.add_func, type=QFunctional:
class QFunctional(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation_post_process : __torch__.torch.nn.modules.linear.___torch_mangle_6131.Identity

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_3.res_conn.add_func.activation_post_process, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6131.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_4, type=IRFBlock:
class IRFBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  pw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6138.ConvBNRelu
  dw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6140.ConvBNRelu
  se : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6151.SEModule
  pwl : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6154.ConvBNRelu
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6155.IRFBlock,
    argument_1: Tensor) -> Tensor:
    pwl = self.pwl
    se = self.se
    dw = self.dw
    pw = self.pw
    _0 = (dw).forward((pw).forward(argument_1, ), )
    return (pwl).forward((se).forward(_0, ), )

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_4.pw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6135.ConvReLU2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6136.Identity
  relu : __torch__.torch.nn.modules.linear.___torch_mangle_6137.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6138.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    relu = self.relu
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    _2 = (relu).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_4.pw.conv, type=ConvReLU2d:
class ConvReLU2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6135.ConvReLU2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6135.ConvReLU2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6135.ConvReLU2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6135.ConvReLU2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6135.ConvReLU2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d_relu(argument_1, _packed_params, 0.23348763585090637, 0)
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_4.pw.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6136.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_4.pw.relu, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6137.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_4.dw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6139.Conv2d
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6140.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    conv = self.conv
    return (conv).forward(argument_1, )

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_4.dw.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6139.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6139.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6139.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6139.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6139.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    x = ops.quantized.conv2d(argument_1, _packed_params, 0.46659347414970398, 53)
    return x

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_4.se, type=SEModule:
class SEModule(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  avg_pool : __torch__.torch.nn.modules.pooling.___torch_mangle_6141.AdaptiveAvgPool2d
  se : __torch__.torch.nn.modules.container.___torch_mangle_6147.Sequential
  mul : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6150.TorchMultiply
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6151.SEModule,
    argument_1: Tensor) -> Tensor:
    mul = self.mul
    se = self.se
    avg_pool = self.avg_pool
    _0 = (se).forward((avg_pool).forward(argument_1, ), )
    return (mul).forward(argument_1, _0, )

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_4.se.avg_pool, type=AdaptiveAvgPool2d:
class AdaptiveAvgPool2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.pooling.___torch_mangle_6141.AdaptiveAvgPool2d,
    argument_1: Tensor) -> Tensor:
    input = torch.adaptive_avg_pool2d(argument_1, [1, 1])
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_4.se.se, type=Sequential:
class Sequential(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  __annotations__["0"] = __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6144.ConvBNRelu
  __annotations__["1"] = __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6145.Conv2d
  __annotations__["2"] = __torch__.torch.nn.modules.activation.___torch_mangle_6146.Sigmoid
  def forward(self: __torch__.torch.nn.modules.container.___torch_mangle_6147.Sequential,
    argument_1: Tensor) -> Tensor:
    _2 = getattr(self, "2")
    _1 = getattr(self, "1")
    _0 = getattr(self, "0")
    _3 = (_1).forward((_0).forward(argument_1, ), )
    return (_2).forward(_3, )

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_4.se.se.0, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6142.ConvReLU2d
  relu : __torch__.torch.nn.modules.linear.___torch_mangle_6143.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6144.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    relu = self.relu
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (relu).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_4.se.se.0.conv, type=ConvReLU2d:
class ConvReLU2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6142.ConvReLU2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6142.ConvReLU2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6142.ConvReLU2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6142.ConvReLU2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6142.ConvReLU2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d_relu(argument_1, _packed_params, 0.037842597812414169, 0)
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_4.se.se.0.relu, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6143.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_4.se.se.1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6145.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6145.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6145.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6145.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6145.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d(argument_1, _packed_params, 0.0612945556640625, 96)
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_4.se.se.2, type=Sigmoid:
class Sigmoid(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.activation.___torch_mangle_6146.Sigmoid,
    argument_1: Tensor) -> Tensor:
    return torch.sigmoid(argument_1)

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_4.se.mul, type=TorchMultiply:
class TorchMultiply(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  mul_func : __torch__.torch.ao.nn.quantized.modules.functional_modules.___torch_mangle_6149.QFunctional
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6150.TorchMultiply,
    argument_1: Tensor,
    argument_2: Tensor) -> Tensor:
    mul_func = self.mul_func
    activation_post_process = mul_func.activation_post_process
    input = ops.quantized.mul(argument_1, argument_2, 0.019502054899930954, 54)
    _0 = (activation_post_process).forward()
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_4.se.mul.mul_func, type=QFunctional:
class QFunctional(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation_post_process : __torch__.torch.nn.modules.linear.___torch_mangle_6148.Identity

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_4.se.mul.mul_func.activation_post_process, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6148.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_4.pwl, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6152.Conv2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6153.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6154.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_4.pwl.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6152.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6152.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6152.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6152.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6152.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d(argument_1, _packed_params, 0.03663257509469986, 64)
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_4.pwl.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6153.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_5, type=IRFBlock:
class IRFBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  pw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6159.ConvBNRelu
  dw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6161.ConvBNRelu
  se : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6172.SEModule
  pwl : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6175.ConvBNRelu
  res_conn : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6178.TorchAdd
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6179.IRFBlock,
    argument_1: Tensor) -> Tensor:
    res_conn = self.res_conn
    pwl = self.pwl
    se = self.se
    dw = self.dw
    pw = self.pw
    _0 = (dw).forward((pw).forward(argument_1, ), )
    _1 = (res_conn).forward((pwl).forward((se).forward(_0, ), ), argument_1, )
    return _1

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_5.pw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6156.ConvReLU2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6157.Identity
  relu : __torch__.torch.nn.modules.linear.___torch_mangle_6158.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6159.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    relu = self.relu
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    _2 = (relu).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_5.pw.conv, type=ConvReLU2d:
class ConvReLU2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6156.ConvReLU2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6156.ConvReLU2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6156.ConvReLU2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6156.ConvReLU2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6156.ConvReLU2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d_relu(argument_1, _packed_params, 0.025703277438879013, 0)
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_5.pw.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6157.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_5.pw.relu, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6158.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_5.dw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6160.Conv2d
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6161.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    conv = self.conv
    return (conv).forward(argument_1, )

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_5.dw.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6160.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6160.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6160.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6160.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6160.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    x = ops.quantized.conv2d(argument_1, _packed_params, 0.012948849238455296, 67)
    return x

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_5.se, type=SEModule:
class SEModule(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  avg_pool : __torch__.torch.nn.modules.pooling.___torch_mangle_6162.AdaptiveAvgPool2d
  se : __torch__.torch.nn.modules.container.___torch_mangle_6168.Sequential
  mul : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6171.TorchMultiply
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6172.SEModule,
    argument_1: Tensor) -> Tensor:
    mul = self.mul
    se = self.se
    avg_pool = self.avg_pool
    _0 = (se).forward((avg_pool).forward(argument_1, ), )
    return (mul).forward(argument_1, _0, )

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_5.se.avg_pool, type=AdaptiveAvgPool2d:
class AdaptiveAvgPool2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.pooling.___torch_mangle_6162.AdaptiveAvgPool2d,
    argument_1: Tensor) -> Tensor:
    input = torch.adaptive_avg_pool2d(argument_1, [1, 1])
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_5.se.se, type=Sequential:
class Sequential(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  __annotations__["0"] = __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6165.ConvBNRelu
  __annotations__["1"] = __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6166.Conv2d
  __annotations__["2"] = __torch__.torch.nn.modules.activation.___torch_mangle_6167.Sigmoid
  def forward(self: __torch__.torch.nn.modules.container.___torch_mangle_6168.Sequential,
    argument_1: Tensor) -> Tensor:
    _2 = getattr(self, "2")
    _1 = getattr(self, "1")
    _0 = getattr(self, "0")
    _3 = (_1).forward((_0).forward(argument_1, ), )
    return (_2).forward(_3, )

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_5.se.se.0, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6163.ConvReLU2d
  relu : __torch__.torch.nn.modules.linear.___torch_mangle_6164.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6165.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    relu = self.relu
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (relu).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_5.se.se.0.conv, type=ConvReLU2d:
class ConvReLU2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6163.ConvReLU2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6163.ConvReLU2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6163.ConvReLU2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6163.ConvReLU2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6163.ConvReLU2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d_relu(argument_1, _packed_params, 1.1920928955078125e-07, 0)
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_5.se.se.0.relu, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6164.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_5.se.se.1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6166.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6166.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6166.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6166.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6166.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d(argument_1, _packed_params, 0.00021384170395322144, 57)
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_5.se.se.2, type=Sigmoid:
class Sigmoid(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.activation.___torch_mangle_6167.Sigmoid,
    argument_1: Tensor) -> Tensor:
    return torch.sigmoid(argument_1)

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_5.se.mul, type=TorchMultiply:
class TorchMultiply(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  mul_func : __torch__.torch.ao.nn.quantized.modules.functional_modules.___torch_mangle_6170.QFunctional
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6171.TorchMultiply,
    argument_1: Tensor,
    argument_2: Tensor) -> Tensor:
    mul_func = self.mul_func
    activation_post_process = mul_func.activation_post_process
    input = ops.quantized.mul(argument_1, argument_2, 0.0068242447450757027, 77)
    _0 = (activation_post_process).forward()
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_5.se.mul.mul_func, type=QFunctional:
class QFunctional(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation_post_process : __torch__.torch.nn.modules.linear.___torch_mangle_6169.Identity

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_5.se.mul.mul_func.activation_post_process, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6169.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_5.pwl, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6173.Conv2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6174.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6175.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_5.pwl.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6173.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6173.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6173.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6173.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6173.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    _18 = ops.quantized.conv2d(argument_1, _packed_params, 0.023744378238916397, 68)
    return _18

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_5.pwl.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6174.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_5.res_conn, type=TorchAdd:
class TorchAdd(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  add_func : __torch__.torch.ao.nn.quantized.modules.functional_modules.___torch_mangle_6177.QFunctional
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6178.TorchAdd,
    argument_1: Tensor,
    argument_2: Tensor) -> Tensor:
    add_func = self.add_func
    activation_post_process = add_func.activation_post_process
    input = ops.quantized.add(argument_1, argument_2, 0.041550789028406143, 60)
    _0 = (activation_post_process).forward()
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_5.res_conn.add_func, type=QFunctional:
class QFunctional(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation_post_process : __torch__.torch.nn.modules.linear.___torch_mangle_6176.Identity

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_5.res_conn.add_func.activation_post_process, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6176.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_6, type=IRFBlock:
class IRFBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  pw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6183.ConvBNRelu
  dw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6185.ConvBNRelu
  se : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6196.SEModule
  pwl : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6199.ConvBNRelu
  res_conn : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6202.TorchAdd
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6203.IRFBlock,
    argument_1: Tensor) -> Tensor:
    res_conn = self.res_conn
    pwl = self.pwl
    se = self.se
    dw = self.dw
    pw = self.pw
    _0 = (dw).forward((pw).forward(argument_1, ), )
    _1 = (res_conn).forward((pwl).forward((se).forward(_0, ), ), argument_1, )
    return _1

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_6.pw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6180.ConvReLU2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6181.Identity
  relu : __torch__.torch.nn.modules.linear.___torch_mangle_6182.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6183.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    relu = self.relu
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    _2 = (relu).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_6.pw.conv, type=ConvReLU2d:
class ConvReLU2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6180.ConvReLU2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6180.ConvReLU2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6180.ConvReLU2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6180.ConvReLU2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6180.ConvReLU2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d_relu(argument_1, _packed_params, 0.02115413174033165, 0)
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_6.pw.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6181.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_6.pw.relu, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6182.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_6.dw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6184.Conv2d
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6185.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    conv = self.conv
    return (conv).forward(argument_1, )

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_6.dw.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6184.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6184.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6184.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6184.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6184.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    x = ops.quantized.conv2d(argument_1, _packed_params, 0.0069300001487135887, 66)
    return x

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_6.se, type=SEModule:
class SEModule(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  avg_pool : __torch__.torch.nn.modules.pooling.___torch_mangle_6186.AdaptiveAvgPool2d
  se : __torch__.torch.nn.modules.container.___torch_mangle_6192.Sequential
  mul : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6195.TorchMultiply
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6196.SEModule,
    argument_1: Tensor) -> Tensor:
    mul = self.mul
    se = self.se
    avg_pool = self.avg_pool
    _0 = (se).forward((avg_pool).forward(argument_1, ), )
    return (mul).forward(argument_1, _0, )

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_6.se.avg_pool, type=AdaptiveAvgPool2d:
class AdaptiveAvgPool2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.pooling.___torch_mangle_6186.AdaptiveAvgPool2d,
    argument_1: Tensor) -> Tensor:
    input = torch.adaptive_avg_pool2d(argument_1, [1, 1])
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_6.se.se, type=Sequential:
class Sequential(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  __annotations__["0"] = __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6189.ConvBNRelu
  __annotations__["1"] = __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6190.Conv2d
  __annotations__["2"] = __torch__.torch.nn.modules.activation.___torch_mangle_6191.Sigmoid
  def forward(self: __torch__.torch.nn.modules.container.___torch_mangle_6192.Sequential,
    argument_1: Tensor) -> Tensor:
    _2 = getattr(self, "2")
    _1 = getattr(self, "1")
    _0 = getattr(self, "0")
    _3 = (_1).forward((_0).forward(argument_1, ), )
    return (_2).forward(_3, )

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_6.se.se.0, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6187.ConvReLU2d
  relu : __torch__.torch.nn.modules.linear.___torch_mangle_6188.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6189.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    relu = self.relu
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (relu).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_6.se.se.0.conv, type=ConvReLU2d:
class ConvReLU2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6187.ConvReLU2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6187.ConvReLU2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6187.ConvReLU2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6187.ConvReLU2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6187.ConvReLU2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d_relu(argument_1, _packed_params, 1.1920928955078125e-07, 0)
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_6.se.se.0.relu, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6188.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_6.se.se.1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6190.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6190.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6190.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6190.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6190.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d(argument_1, _packed_params, 0.0002809857833199203, 56)
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_6.se.se.2, type=Sigmoid:
class Sigmoid(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.activation.___torch_mangle_6191.Sigmoid,
    argument_1: Tensor) -> Tensor:
    return torch.sigmoid(argument_1)

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_6.se.mul, type=TorchMultiply:
class TorchMultiply(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  mul_func : __torch__.torch.ao.nn.quantized.modules.functional_modules.___torch_mangle_6194.QFunctional
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6195.TorchMultiply,
    argument_1: Tensor,
    argument_2: Tensor) -> Tensor:
    mul_func = self.mul_func
    activation_post_process = mul_func.activation_post_process
    input = ops.quantized.mul(argument_1, argument_2, 0.0036316744517534971, 63)
    _0 = (activation_post_process).forward()
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_6.se.mul.mul_func, type=QFunctional:
class QFunctional(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation_post_process : __torch__.torch.nn.modules.linear.___torch_mangle_6193.Identity

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_6.se.mul.mul_func.activation_post_process, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6193.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_6.pwl, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6197.Conv2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6198.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6199.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_6.pwl.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6197.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6197.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6197.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6197.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6197.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    _18 = ops.quantized.conv2d(argument_1, _packed_params, 0.021655313670635223, 60)
    return _18

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_6.pwl.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6198.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_6.res_conn, type=TorchAdd:
class TorchAdd(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  add_func : __torch__.torch.ao.nn.quantized.modules.functional_modules.___torch_mangle_6201.QFunctional
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6202.TorchAdd,
    argument_1: Tensor,
    argument_2: Tensor) -> Tensor:
    add_func = self.add_func
    activation_post_process = add_func.activation_post_process
    input = ops.quantized.add(argument_1, argument_2, 0.045002754777669907, 65)
    _0 = (activation_post_process).forward()
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_6.res_conn.add_func, type=QFunctional:
class QFunctional(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation_post_process : __torch__.torch.nn.modules.linear.___torch_mangle_6200.Identity

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_6.res_conn.add_func.activation_post_process, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6200.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_7, type=IRFBlock:
class IRFBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  pw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6207.ConvBNRelu
  dw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6209.ConvBNRelu
  se : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6220.SEModule
  pwl : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6223.ConvBNRelu
  res_conn : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6226.TorchAdd
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6227.IRFBlock,
    argument_1: Tensor) -> Tensor:
    res_conn = self.res_conn
    pwl = self.pwl
    se = self.se
    dw = self.dw
    pw = self.pw
    _0 = (dw).forward((pw).forward(argument_1, ), )
    _1 = (res_conn).forward((pwl).forward((se).forward(_0, ), ), argument_1, )
    return _1

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_7.pw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6204.ConvReLU2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6205.Identity
  relu : __torch__.torch.nn.modules.linear.___torch_mangle_6206.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6207.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    relu = self.relu
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    _2 = (relu).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_7.pw.conv, type=ConvReLU2d:
class ConvReLU2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6204.ConvReLU2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6204.ConvReLU2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6204.ConvReLU2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6204.ConvReLU2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6204.ConvReLU2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d_relu(argument_1, _packed_params, 0.020147399976849556, 0)
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_7.pw.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6205.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_7.pw.relu, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6206.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_7.dw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6208.Conv2d
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6209.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    conv = self.conv
    return (conv).forward(argument_1, )

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_7.dw.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6208.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6208.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6208.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6208.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6208.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    x = ops.quantized.conv2d(argument_1, _packed_params, 0.0076149627566337585, 67)
    return x

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_7.se, type=SEModule:
class SEModule(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  avg_pool : __torch__.torch.nn.modules.pooling.___torch_mangle_6210.AdaptiveAvgPool2d
  se : __torch__.torch.nn.modules.container.___torch_mangle_6216.Sequential
  mul : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6219.TorchMultiply
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6220.SEModule,
    argument_1: Tensor) -> Tensor:
    mul = self.mul
    se = self.se
    avg_pool = self.avg_pool
    _0 = (se).forward((avg_pool).forward(argument_1, ), )
    return (mul).forward(argument_1, _0, )

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_7.se.avg_pool, type=AdaptiveAvgPool2d:
class AdaptiveAvgPool2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.pooling.___torch_mangle_6210.AdaptiveAvgPool2d,
    argument_1: Tensor) -> Tensor:
    input = torch.adaptive_avg_pool2d(argument_1, [1, 1])
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_7.se.se, type=Sequential:
class Sequential(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  __annotations__["0"] = __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6213.ConvBNRelu
  __annotations__["1"] = __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6214.Conv2d
  __annotations__["2"] = __torch__.torch.nn.modules.activation.___torch_mangle_6215.Sigmoid
  def forward(self: __torch__.torch.nn.modules.container.___torch_mangle_6216.Sequential,
    argument_1: Tensor) -> Tensor:
    _2 = getattr(self, "2")
    _1 = getattr(self, "1")
    _0 = getattr(self, "0")
    _3 = (_1).forward((_0).forward(argument_1, ), )
    return (_2).forward(_3, )

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_7.se.se.0, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6211.ConvReLU2d
  relu : __torch__.torch.nn.modules.linear.___torch_mangle_6212.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6213.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    relu = self.relu
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (relu).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_7.se.se.0.conv, type=ConvReLU2d:
class ConvReLU2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6211.ConvReLU2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6211.ConvReLU2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6211.ConvReLU2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6211.ConvReLU2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6211.ConvReLU2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d_relu(argument_1, _packed_params, 1.1920928955078125e-07, 0)
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_7.se.se.0.relu, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6212.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_7.se.se.1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6214.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6214.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6214.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6214.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6214.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d(argument_1, _packed_params, 0.00040349783375859261, 30)
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_7.se.se.2, type=Sigmoid:
class Sigmoid(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.activation.___torch_mangle_6215.Sigmoid,
    argument_1: Tensor) -> Tensor:
    return torch.sigmoid(argument_1)

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_7.se.mul, type=TorchMultiply:
class TorchMultiply(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  mul_func : __torch__.torch.ao.nn.quantized.modules.functional_modules.___torch_mangle_6218.QFunctional
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6219.TorchMultiply,
    argument_1: Tensor,
    argument_2: Tensor) -> Tensor:
    mul_func = self.mul_func
    activation_post_process = mul_func.activation_post_process
    input = ops.quantized.mul(argument_1, argument_2, 0.0036761518567800522, 69)
    _0 = (activation_post_process).forward()
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_7.se.mul.mul_func, type=QFunctional:
class QFunctional(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation_post_process : __torch__.torch.nn.modules.linear.___torch_mangle_6217.Identity

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_7.se.mul.mul_func.activation_post_process, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6217.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_7.pwl, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6221.Conv2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6222.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6223.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_7.pwl.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6221.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6221.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6221.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6221.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6221.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    _18 = ops.quantized.conv2d(argument_1, _packed_params, 0.024841530248522758, 63)
    return _18

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_7.pwl.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6222.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_7.res_conn, type=TorchAdd:
class TorchAdd(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  add_func : __torch__.torch.ao.nn.quantized.modules.functional_modules.___torch_mangle_6225.QFunctional
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6226.TorchAdd,
    argument_1: Tensor,
    argument_2: Tensor) -> Tensor:
    add_func = self.add_func
    activation_post_process = add_func.activation_post_process
    input = ops.quantized.add(argument_1, argument_2, 0.052997715771198273, 58)
    _0 = (activation_post_process).forward()
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_7.res_conn.add_func, type=QFunctional:
class QFunctional(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation_post_process : __torch__.torch.nn.modules.linear.___torch_mangle_6224.Identity

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_7.res_conn.add_func.activation_post_process, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6224.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_8, type=IRFBlock:
class IRFBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  pw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6231.ConvBNRelu
  dw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6233.ConvBNRelu
  se : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6244.SEModule
  pwl : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6247.ConvBNRelu
  res_conn : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6250.TorchAdd
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6251.IRFBlock,
    argument_1: Tensor) -> Tensor:
    res_conn = self.res_conn
    pwl = self.pwl
    se = self.se
    dw = self.dw
    pw = self.pw
    _0 = (dw).forward((pw).forward(argument_1, ), )
    _1 = (res_conn).forward((pwl).forward((se).forward(_0, ), ), argument_1, )
    return _1

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_8.pw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6228.ConvReLU2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6229.Identity
  relu : __torch__.torch.nn.modules.linear.___torch_mangle_6230.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6231.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    relu = self.relu
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    _2 = (relu).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_8.pw.conv, type=ConvReLU2d:
class ConvReLU2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6228.ConvReLU2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6228.ConvReLU2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6228.ConvReLU2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6228.ConvReLU2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6228.ConvReLU2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d_relu(argument_1, _packed_params, 0.022453382611274719, 0)
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_8.pw.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6229.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_8.pw.relu, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6230.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_8.dw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6232.Conv2d
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6233.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    conv = self.conv
    return (conv).forward(argument_1, )

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_8.dw.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6232.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6232.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6232.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6232.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6232.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    x = ops.quantized.conv2d(argument_1, _packed_params, 0.0092360768467187881, 58)
    return x

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_8.se, type=SEModule:
class SEModule(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  avg_pool : __torch__.torch.nn.modules.pooling.___torch_mangle_6234.AdaptiveAvgPool2d
  se : __torch__.torch.nn.modules.container.___torch_mangle_6240.Sequential
  mul : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6243.TorchMultiply
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6244.SEModule,
    argument_1: Tensor) -> Tensor:
    mul = self.mul
    se = self.se
    avg_pool = self.avg_pool
    _0 = (se).forward((avg_pool).forward(argument_1, ), )
    return (mul).forward(argument_1, _0, )

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_8.se.avg_pool, type=AdaptiveAvgPool2d:
class AdaptiveAvgPool2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.pooling.___torch_mangle_6234.AdaptiveAvgPool2d,
    argument_1: Tensor) -> Tensor:
    input = torch.adaptive_avg_pool2d(argument_1, [1, 1])
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_8.se.se, type=Sequential:
class Sequential(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  __annotations__["0"] = __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6237.ConvBNRelu
  __annotations__["1"] = __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6238.Conv2d
  __annotations__["2"] = __torch__.torch.nn.modules.activation.___torch_mangle_6239.Sigmoid
  def forward(self: __torch__.torch.nn.modules.container.___torch_mangle_6240.Sequential,
    argument_1: Tensor) -> Tensor:
    _2 = getattr(self, "2")
    _1 = getattr(self, "1")
    _0 = getattr(self, "0")
    _3 = (_1).forward((_0).forward(argument_1, ), )
    return (_2).forward(_3, )

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_8.se.se.0, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6235.ConvReLU2d
  relu : __torch__.torch.nn.modules.linear.___torch_mangle_6236.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6237.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    relu = self.relu
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (relu).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_8.se.se.0.conv, type=ConvReLU2d:
class ConvReLU2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6235.ConvReLU2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6235.ConvReLU2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6235.ConvReLU2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6235.ConvReLU2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6235.ConvReLU2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d_relu(argument_1, _packed_params, 1.1920928955078125e-07, 0)
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_8.se.se.0.relu, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6236.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_8.se.se.1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6238.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6238.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6238.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6238.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6238.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d(argument_1, _packed_params, 0.00027793736080639064, 51)
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_8.se.se.2, type=Sigmoid:
class Sigmoid(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.activation.___torch_mangle_6239.Sigmoid,
    argument_1: Tensor) -> Tensor:
    return torch.sigmoid(argument_1)

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_8.se.mul, type=TorchMultiply:
class TorchMultiply(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  mul_func : __torch__.torch.ao.nn.quantized.modules.functional_modules.___torch_mangle_6242.QFunctional
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6243.TorchMultiply,
    argument_1: Tensor,
    argument_2: Tensor) -> Tensor:
    mul_func = self.mul_func
    activation_post_process = mul_func.activation_post_process
    input = ops.quantized.mul(argument_1, argument_2, 0.0047452496364712715, 60)
    _0 = (activation_post_process).forward()
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_8.se.mul.mul_func, type=QFunctional:
class QFunctional(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation_post_process : __torch__.torch.nn.modules.linear.___torch_mangle_6241.Identity

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_8.se.mul.mul_func.activation_post_process, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6241.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_8.pwl, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6245.Conv2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6246.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6247.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_8.pwl.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6245.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6245.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6245.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6245.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6245.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    _18 = ops.quantized.conv2d(argument_1, _packed_params, 0.031656406819820404, 60)
    return _18

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_8.pwl.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6246.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_8.res_conn, type=TorchAdd:
class TorchAdd(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  add_func : __torch__.torch.ao.nn.quantized.modules.functional_modules.___torch_mangle_6249.QFunctional
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6250.TorchAdd,
    argument_1: Tensor,
    argument_2: Tensor) -> Tensor:
    add_func = self.add_func
    activation_post_process = add_func.activation_post_process
    input = ops.quantized.add(argument_1, argument_2, 0.063239879906177521, 62)
    _0 = (activation_post_process).forward()
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_8.res_conn.add_func, type=QFunctional:
class QFunctional(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation_post_process : __torch__.torch.nn.modules.linear.___torch_mangle_6248.Identity

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_8.res_conn.add_func.activation_post_process, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6248.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_9, type=IRFBlock:
class IRFBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  pw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6255.ConvBNRelu
  dw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6257.ConvBNRelu
  se : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6268.SEModule
  pwl : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6271.ConvBNRelu
  res_conn : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6274.TorchAdd
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6275.IRFBlock,
    argument_1: Tensor) -> Tensor:
    res_conn = self.res_conn
    pwl = self.pwl
    se = self.se
    dw = self.dw
    pw = self.pw
    _0 = (dw).forward((pw).forward(argument_1, ), )
    _1 = (res_conn).forward((pwl).forward((se).forward(_0, ), ), argument_1, )
    return _1

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_9.pw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6252.ConvReLU2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6253.Identity
  relu : __torch__.torch.nn.modules.linear.___torch_mangle_6254.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6255.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    relu = self.relu
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    _2 = (relu).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_9.pw.conv, type=ConvReLU2d:
class ConvReLU2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6252.ConvReLU2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6252.ConvReLU2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6252.ConvReLU2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6252.ConvReLU2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6252.ConvReLU2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d_relu(argument_1, _packed_params, 0.029554517939686775, 0)
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_9.pw.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6253.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_9.pw.relu, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6254.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_9.dw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6256.Conv2d
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6257.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    conv = self.conv
    return (conv).forward(argument_1, )

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_9.dw.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6256.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6256.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6256.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6256.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6256.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    x = ops.quantized.conv2d(argument_1, _packed_params, 0.012390102259814739, 72)
    return x

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_9.se, type=SEModule:
class SEModule(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  avg_pool : __torch__.torch.nn.modules.pooling.___torch_mangle_6258.AdaptiveAvgPool2d
  se : __torch__.torch.nn.modules.container.___torch_mangle_6264.Sequential
  mul : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6267.TorchMultiply
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6268.SEModule,
    argument_1: Tensor) -> Tensor:
    mul = self.mul
    se = self.se
    avg_pool = self.avg_pool
    _0 = (se).forward((avg_pool).forward(argument_1, ), )
    return (mul).forward(argument_1, _0, )

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_9.se.avg_pool, type=AdaptiveAvgPool2d:
class AdaptiveAvgPool2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.pooling.___torch_mangle_6258.AdaptiveAvgPool2d,
    argument_1: Tensor) -> Tensor:
    input = torch.adaptive_avg_pool2d(argument_1, [1, 1])
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_9.se.se, type=Sequential:
class Sequential(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  __annotations__["0"] = __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6261.ConvBNRelu
  __annotations__["1"] = __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6262.Conv2d
  __annotations__["2"] = __torch__.torch.nn.modules.activation.___torch_mangle_6263.Sigmoid
  def forward(self: __torch__.torch.nn.modules.container.___torch_mangle_6264.Sequential,
    argument_1: Tensor) -> Tensor:
    _2 = getattr(self, "2")
    _1 = getattr(self, "1")
    _0 = getattr(self, "0")
    _3 = (_1).forward((_0).forward(argument_1, ), )
    return (_2).forward(_3, )

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_9.se.se.0, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6259.ConvReLU2d
  relu : __torch__.torch.nn.modules.linear.___torch_mangle_6260.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6261.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    relu = self.relu
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (relu).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_9.se.se.0.conv, type=ConvReLU2d:
class ConvReLU2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6259.ConvReLU2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6259.ConvReLU2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6259.ConvReLU2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6259.ConvReLU2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6259.ConvReLU2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d_relu(argument_1, _packed_params, 1.1920928955078125e-07, 0)
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_9.se.se.0.relu, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6260.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_9.se.se.1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6262.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6262.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6262.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6262.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6262.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d(argument_1, _packed_params, 0.00030603984487242997, 74)
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_9.se.se.2, type=Sigmoid:
class Sigmoid(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.activation.___torch_mangle_6263.Sigmoid,
    argument_1: Tensor) -> Tensor:
    return torch.sigmoid(argument_1)

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_9.se.mul, type=TorchMultiply:
class TorchMultiply(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  mul_func : __torch__.torch.ao.nn.quantized.modules.functional_modules.___torch_mangle_6266.QFunctional
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6267.TorchMultiply,
    argument_1: Tensor,
    argument_2: Tensor) -> Tensor:
    mul_func = self.mul_func
    activation_post_process = mul_func.activation_post_process
    input = ops.quantized.mul(argument_1, argument_2, 0.0064642475917935371, 69)
    _0 = (activation_post_process).forward()
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_9.se.mul.mul_func, type=QFunctional:
class QFunctional(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation_post_process : __torch__.torch.nn.modules.linear.___torch_mangle_6265.Identity

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_9.se.mul.mul_func.activation_post_process, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6265.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_9.pwl, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6269.Conv2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6270.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6271.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_9.pwl.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6269.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6269.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6269.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6269.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6269.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    _18 = ops.quantized.conv2d(argument_1, _packed_params, 0.038257457315921783, 66)
    return _18

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_9.pwl.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6270.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_9.res_conn, type=TorchAdd:
class TorchAdd(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  add_func : __torch__.torch.ao.nn.quantized.modules.functional_modules.___torch_mangle_6273.QFunctional
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6274.TorchAdd,
    argument_1: Tensor,
    argument_2: Tensor) -> Tensor:
    add_func = self.add_func
    activation_post_process = add_func.activation_post_process
    Xq = ops.quantized.add(argument_1, argument_2, 0.075057744979858398, 63)
    _0 = (activation_post_process).forward()
    return Xq

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_9.res_conn.add_func, type=QFunctional:
class QFunctional(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation_post_process : __torch__.torch.nn.modules.linear.___torch_mangle_6272.Identity

--------------------------------------------------------------------------------
Code for .model.model.backbone.body.trunk3.fbnetv2_3_9.res_conn.add_func.activation_post_process, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6272.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.quant_stubs, type=QuantStubNested:
class QuantStubNested(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  stubs : __torch__.torch.nn.modules.container.___torch_mangle_6279.ModuleList
  def forward(self: __torch__.mobile_cv.arch.utils.quantize_utils.___torch_mangle_6280.QuantStubNested,
    X: Tensor) -> Tensor:
    stubs = self.stubs
    _0 = getattr(stubs, "0")
    return (_0).forward(X, )

--------------------------------------------------------------------------------
Code for .model.model.backbone.quant_stubs.stubs, type=ModuleList:
class ModuleList(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  __annotations__["0"] = __torch__.torch.ao.nn.quantized.modules.___torch_mangle_6278.Quantize

--------------------------------------------------------------------------------
Code for .model.model.backbone.quant_stubs.stubs.0, type=Quantize:
class Quantize(Module):
  __parameters__ = []
  __buffers__ = ["scale", "zero_point", ]
  scale : Tensor
  zero_point : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.ao.nn.quantized.modules.___torch_mangle_6278.Quantize,
    X: Tensor) -> Tensor:
    input = torch.quantize_per_tensor(X, 2.1648626327514648, 57, 13)
    return input

--------------------------------------------------------------------------------
Code for .model.model.backbone.dequant_stubs, type=QuantStubNested:
class QuantStubNested(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  stubs : __torch__.torch.nn.modules.container.___torch_mangle_6285.ModuleList
  def forward(self: __torch__.mobile_cv.arch.utils.quantize_utils.___torch_mangle_6286.QuantStubNested,
    argument_1: Tensor,
    argument_2: Tensor,
    argument_3: Tensor,
    argument_4: Tensor) -> Tensor:
    stubs = self.stubs
    _3 = getattr(stubs, "3")
    stubs0 = self.stubs
    _2 = getattr(stubs0, "2")
    stubs1 = self.stubs
    _1 = getattr(stubs1, "1")
    stubs2 = self.stubs
    _0 = getattr(stubs2, "0")
    _4 = (_0).forward(argument_1, )
    _5 = (_1).forward(argument_2, )
    _6 = (_2).forward(argument_3, )
    return (_3).forward(argument_4, )

--------------------------------------------------------------------------------
Code for .model.model.backbone.dequant_stubs.stubs, type=ModuleList:
class ModuleList(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  __annotations__["0"] = __torch__.torch.ao.nn.quantized.modules.___torch_mangle_6281.DeQuantize
  __annotations__["1"] = __torch__.torch.ao.nn.quantized.modules.___torch_mangle_6282.DeQuantize
  __annotations__["2"] = __torch__.torch.ao.nn.quantized.modules.___torch_mangle_6283.DeQuantize
  __annotations__["3"] = __torch__.torch.ao.nn.quantized.modules.___torch_mangle_6284.DeQuantize

--------------------------------------------------------------------------------
Code for .model.model.backbone.dequant_stubs.stubs.0, type=DeQuantize:
class DeQuantize(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.ao.nn.quantized.modules.___torch_mangle_6281.DeQuantize,
    argument_1: Tensor) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.dequant_stubs.stubs.1, type=DeQuantize:
class DeQuantize(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.ao.nn.quantized.modules.___torch_mangle_6282.DeQuantize,
    argument_1: Tensor) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.dequant_stubs.stubs.2, type=DeQuantize:
class DeQuantize(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.ao.nn.quantized.modules.___torch_mangle_6283.DeQuantize,
    argument_1: Tensor) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.backbone.dequant_stubs.stubs.3, type=DeQuantize:
class DeQuantize(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.ao.nn.quantized.modules.___torch_mangle_6284.DeQuantize,
    argument_1: Tensor) -> Tensor:
    return torch.dequantize(argument_1)

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator, type=RPN:
class RPN(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  rpn_head : __torch__.mobile_cv.arch.utils.quantize_utils.___torch_mangle_6420.QuantWrapSubClass
  anchor_generator : __torch__.detectron2.modeling.anchor_generator.___torch_mangle_6422.DefaultAnchorGenerator
  def forward(self: __torch__.detectron2.modeling.proposal_generator.rpn.___torch_mangle_6423.RPN,
    argument_1: Tensor,
    image_size: Tensor) -> Tensor:
    _0 = __torch__.detectron2.layers.wrappers.move_device_like
    _1 = __torch__.detectron2.layers.wrappers.move_device_like
    _2 = __torch__.torchvision.ops.boxes._batched_nms_coordinate_trick
    rpn_head = self.rpn_head
    anchor_generator = self.anchor_generator
    _3 = (anchor_generator).forward(argument_1, )
    _4, _5, = (rpn_head).forward(argument_1, )
    logits_i = torch.flatten(torch.permute(_4, [0, 2, 3, 1]), 1)
    _6 = ops.prim.NumToTensor(torch.size(_5, 0))
    _7 = int(_6)
    _8 = ops.prim.NumToTensor(torch.size(_5, 2))
    _9 = int(_8)
    _10 = ops.prim.NumToTensor(torch.size(_5, 3))
    _11 = torch.view(_5, [_7, -1, 4, _9, int(_10)])
    pred_anchor_deltas_i = torch.flatten(torch.permute(_11, [0, 3, 4, 1, 2]), 1, -2)
    N = ops.prim.NumToTensor(torch.size(pred_anchor_deltas_i, 0))
    _12 = int(N)
    _13 = int(N)
    B = ops.prim.NumToTensor(torch.size(_3, 1))
    _14 = int(B)
    _15 = int(B)
    deltas = torch.reshape(pred_anchor_deltas_i, [-1, int(B)])
    _16 = torch.expand(torch.unsqueeze(_3, 0), [_13, -1, -1])
    boxes = torch.reshape(_16, [-1, _15])
    deltas0 = torch.to(deltas, 6)
    boxes0 = torch.to(boxes, 6)
    _17 = torch.slice(boxes0, 0, 0, 9223372036854775807)
    _18 = torch.select(_17, 1, 2)
    _19 = torch.slice(boxes0, 0, 0, 9223372036854775807)
    widths = torch.sub(_18, torch.select(_19, 1, 0))
    _20 = torch.slice(boxes0, 0, 0, 9223372036854775807)
    _21 = torch.select(_20, 1, 3)
    _22 = torch.slice(boxes0, 0, 0, 9223372036854775807)
    heights = torch.sub(_21, torch.select(_22, 1, 1))
    _23 = torch.slice(boxes0, 0, 0, 9223372036854775807)
    ctr_x = torch.add(torch.select(_23, 1, 0), torch.mul(widths, CONSTANTS.c0))
    _24 = torch.slice(boxes0, 0, 0, 9223372036854775807)
    ctr_y = torch.add(torch.select(_24, 1, 1), torch.mul(heights, CONSTANTS.c0))
    _25 = torch.slice(deltas0, 0, 0, 9223372036854775807)
    _26 = torch.slice(_25, 1, 0, 9223372036854775807, 4)
    dx = torch.div(_26, CONSTANTS.c1)
    _27 = torch.slice(deltas0, 0, 0, 9223372036854775807)
    _28 = torch.slice(_27, 1, 1, 9223372036854775807, 4)
    dy = torch.div(_28, CONSTANTS.c1)
    _29 = torch.slice(deltas0, 0, 0, 9223372036854775807)
    _30 = torch.slice(_29, 1, 2, 9223372036854775807, 4)
    dw = torch.div(_30, CONSTANTS.c1)
    _31 = torch.slice(deltas0, 0, 0, 9223372036854775807)
    _32 = torch.slice(_31, 1, 3, 9223372036854775807, 4)
    dh = torch.div(_32, CONSTANTS.c1)
    dw0 = torch.clamp(dw, None, 4.1351665567423561)
    dh0 = torch.clamp(dh, None, 4.1351665567423561)
    _33 = torch.slice(widths, 0, 0, 9223372036854775807)
    _34 = torch.mul(dx, torch.unsqueeze(_33, 1))
    _35 = torch.slice(ctr_x, 0, 0, 9223372036854775807)
    pred_ctr_x = torch.add(_34, torch.unsqueeze(_35, 1))
    _36 = torch.slice(heights, 0, 0, 9223372036854775807)
    _37 = torch.mul(dy, torch.unsqueeze(_36, 1))
    _38 = torch.slice(ctr_y, 0, 0, 9223372036854775807)
    pred_ctr_y = torch.add(_37, torch.unsqueeze(_38, 1))
    _39 = torch.exp(dw0)
    _40 = torch.slice(widths, 0, 0, 9223372036854775807)
    pred_w = torch.mul(_39, torch.unsqueeze(_40, 1))
    _41 = torch.exp(dh0)
    _42 = torch.slice(heights, 0, 0, 9223372036854775807)
    pred_h = torch.mul(_41, torch.unsqueeze(_42, 1))
    x1 = torch.sub(pred_ctr_x, torch.mul(pred_w, CONSTANTS.c0))
    y1 = torch.sub(pred_ctr_y, torch.mul(pred_h, CONSTANTS.c0))
    x2 = torch.add(pred_ctr_x, torch.mul(pred_w, CONSTANTS.c0))
    y2 = torch.add(pred_ctr_y, torch.mul(pred_h, CONSTANTS.c0))
    pred_boxes = torch.stack([x1, y1, x2, y2], -1)
    _43 = ops.prim.NumToTensor(torch.size(deltas0, 0))
    _44 = int(_43)
    _45 = ops.prim.NumToTensor(torch.size(deltas0, 1))
    proposals_i = torch.reshape(pred_boxes, [_44, int(_45)])
    proposals_i0 = torch.view(proposals_i, [_12, -1, _14])
    _46 = torch.arange(1, dtype=None, layout=None, device=torch.device("cpu"), pin_memory=False)
    batch_idx = _0(_46, proposals_i0, )
    Hi_Wi_A = ops.prim.NumToTensor(torch.size(logits_i, 1))
    num_proposals_i = torch.clamp(Hi_Wi_A, None, 1000)
    _47 = int(num_proposals_i)
    topk_scores, topk_idx = torch.topk(logits_i, int(num_proposals_i), 1)
    _48 = torch.slice(batch_idx, 0, 0, 9223372036854775807)
    _49 = annotate(List[Optional[Tensor]], [torch.unsqueeze(_48, 1), topk_idx])
    topk_proposals = torch.index(proposals_i0, _49)
    _50 = torch.full([_47], 0, dtype=4, layout=None, device=torch.device("cpu"), pin_memory=False)
    level_ids = _1(_50, proposals_i0, )
    tensor = torch.select(topk_proposals, 0, 0)
    tensor0 = torch.to(tensor, 6)
    scores_per_img = torch.select(topk_scores, 0, 0)
    h, w, = torch.unbind(image_size)
    _51 = annotate(number, h)
    _52 = annotate(number, w)
    _53 = annotate(number, h)
    _54 = annotate(number, w)
    _55 = torch.slice(tensor0, 0, 0, 9223372036854775807)
    x10 = torch.clamp(torch.select(_55, 1, 0), 0, _54)
    _56 = torch.slice(tensor0, 0, 0, 9223372036854775807)
    y10 = torch.clamp(torch.select(_56, 1, 1), 0, _53)
    _57 = torch.slice(tensor0, 0, 0, 9223372036854775807)
    x20 = torch.clamp(torch.select(_57, 1, 2), 0, _52)
    _58 = torch.slice(tensor0, 0, 0, 9223372036854775807)
    y20 = torch.clamp(torch.select(_58, 1, 3), 0, _51)
    box = torch.stack([x10, y10, x20, y20], -1)
    _59 = torch.slice(box, 0, 0, 9223372036854775807)
    _60 = torch.select(_59, 1, 2)
    _61 = torch.slice(box, 0, 0, 9223372036854775807)
    widths0 = torch.sub(_60, torch.select(_61, 1, 0))
    _62 = torch.slice(box, 0, 0, 9223372036854775807)
    _63 = torch.select(_62, 1, 3)
    _64 = torch.slice(box, 0, 0, 9223372036854775807)
    heights0 = torch.sub(_63, torch.select(_64, 1, 1))
    item = torch.__and__(torch.gt(widths0, 0.), torch.gt(heights0, 0.))
    _65 = annotate(List[Optional[Tensor]], [item])
    tensor1 = torch.index(box, _65)
    tensor2 = torch.to(tensor1, 6)
    _66 = annotate(List[Optional[Tensor]], [item])
    scores_per_img0 = torch.index(scores_per_img, _66)
    _67 = annotate(List[Optional[Tensor]], [item])
    _68 = torch.index(level_ids, _67)
    boxes1 = torch.to(tensor2, 6)
    keep = _2(boxes1, scores_per_img0, _68, 0.69999999999999996, )
    item0 = torch.slice(keep, 0, 0, 30)
    _69 = annotate(List[Optional[Tensor]], [item0])
    tensor3 = torch.index(boxes1, _69)
    return torch.to(tensor3, 6)

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head, type=QuantWrapSubClass:
class QuantWrapSubClass(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  rpn_feature : __torch__.d2go.modeling.backbone.fbnet_v2.___torch_mangle_6409.FBNetModule
  rpn_regressor : __torch__.d2go.modeling.backbone.modules.___torch_mangle_6412.RPNHeadConvRegressor
  quant_stubs : __torch__.mobile_cv.arch.utils.quantize_utils.___torch_mangle_6415.QuantStubNested
  dequant_stubs : __torch__.mobile_cv.arch.utils.quantize_utils.___torch_mangle_6419.QuantStubNested
  def forward(self: __torch__.mobile_cv.arch.utils.quantize_utils.___torch_mangle_6420.QuantWrapSubClass,
    argument_1: Tensor) -> Tuple[Tensor, Tensor]:
    dequant_stubs = self.dequant_stubs
    rpn_regressor = self.rpn_regressor
    rpn_feature = self.rpn_feature
    quant_stubs = self.quant_stubs
    _0 = (rpn_feature).forward((quant_stubs).forward(argument_1, ), )
    _1, _2, = (rpn_regressor).forward(_0, )
    _3, _4, = (dequant_stubs).forward(_1, _2, )
    return (_3, _4)

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature, type=FBNetModule:
class FBNetModule(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  __annotations__["0"] = __torch__.torch.nn.modules.container.___torch_mangle_6408.Sequential
  def forward(self: __torch__.d2go.modeling.backbone.fbnet_v2.___torch_mangle_6409.FBNetModule,
    argument_1: Tensor) -> Tensor:
    _0 = getattr(self, "0")
    return (_0).forward(argument_1, )

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0, type=Sequential:
class Sequential(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  fbnetv2_0_0 : __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6311.IRFBlock
  fbnetv2_0_1 : __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6335.IRFBlock
  fbnetv2_0_2 : __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6359.IRFBlock
  fbnetv2_0_3 : __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6383.IRFBlock
  fbnetv2_0_4 : __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6407.IRFBlock
  def forward(self: __torch__.torch.nn.modules.container.___torch_mangle_6408.Sequential,
    argument_1: Tensor) -> Tensor:
    fbnetv2_0_4 = self.fbnetv2_0_4
    fbnetv2_0_3 = self.fbnetv2_0_3
    fbnetv2_0_2 = self.fbnetv2_0_2
    fbnetv2_0_1 = self.fbnetv2_0_1
    fbnetv2_0_0 = self.fbnetv2_0_0
    _0 = (fbnetv2_0_1).forward((fbnetv2_0_0).forward(argument_1, ), )
    _1 = (fbnetv2_0_3).forward((fbnetv2_0_2).forward(_0, ), )
    return (fbnetv2_0_4).forward(_1, )

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_0, type=IRFBlock:
class IRFBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  pw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6291.ConvBNRelu
  dw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6293.ConvBNRelu
  se : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6304.SEModule
  pwl : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6307.ConvBNRelu
  res_conn : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6310.TorchAdd
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6311.IRFBlock,
    argument_1: Tensor) -> Tensor:
    res_conn = self.res_conn
    pwl = self.pwl
    se = self.se
    dw = self.dw
    pw = self.pw
    _0 = (dw).forward((pw).forward(argument_1, ), )
    _1 = (res_conn).forward((pwl).forward((se).forward(_0, ), ), argument_1, )
    return _1

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_0.pw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6288.ConvReLU2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6289.Identity
  relu : __torch__.torch.nn.modules.linear.___torch_mangle_6290.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6291.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    relu = self.relu
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    _2 = (relu).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_0.pw.conv, type=ConvReLU2d:
class ConvReLU2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6288.ConvReLU2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6288.ConvReLU2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6288.ConvReLU2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6288.ConvReLU2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6288.ConvReLU2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d_relu(argument_1, _packed_params, 0.035599935799837112, 0)
    return input

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_0.pw.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6289.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_0.pw.relu, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6290.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_0.dw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6292.Conv2d
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6293.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    conv = self.conv
    return (conv).forward(argument_1, )

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_0.dw.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6292.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6292.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6292.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6292.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6292.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    x = ops.quantized.conv2d(argument_1, _packed_params, 0.0090596023947000504, 56)
    return x

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_0.se, type=SEModule:
class SEModule(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  avg_pool : __torch__.torch.nn.modules.pooling.___torch_mangle_6294.AdaptiveAvgPool2d
  se : __torch__.torch.nn.modules.container.___torch_mangle_6300.Sequential
  mul : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6303.TorchMultiply
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6304.SEModule,
    argument_1: Tensor) -> Tensor:
    mul = self.mul
    se = self.se
    avg_pool = self.avg_pool
    _0 = (se).forward((avg_pool).forward(argument_1, ), )
    return (mul).forward(argument_1, _0, )

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_0.se.avg_pool, type=AdaptiveAvgPool2d:
class AdaptiveAvgPool2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.pooling.___torch_mangle_6294.AdaptiveAvgPool2d,
    argument_1: Tensor) -> Tensor:
    input = torch.adaptive_avg_pool2d(argument_1, [1, 1])
    return input

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_0.se.se, type=Sequential:
class Sequential(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  __annotations__["0"] = __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6297.ConvBNRelu
  __annotations__["1"] = __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6298.Conv2d
  __annotations__["2"] = __torch__.torch.nn.modules.activation.___torch_mangle_6299.Sigmoid
  def forward(self: __torch__.torch.nn.modules.container.___torch_mangle_6300.Sequential,
    argument_1: Tensor) -> Tensor:
    _2 = getattr(self, "2")
    _1 = getattr(self, "1")
    _0 = getattr(self, "0")
    _3 = (_1).forward((_0).forward(argument_1, ), )
    return (_2).forward(_3, )

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_0.se.se.0, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6295.ConvReLU2d
  relu : __torch__.torch.nn.modules.linear.___torch_mangle_6296.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6297.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    relu = self.relu
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (relu).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_0.se.se.0.conv, type=ConvReLU2d:
class ConvReLU2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6295.ConvReLU2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6295.ConvReLU2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6295.ConvReLU2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6295.ConvReLU2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6295.ConvReLU2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d_relu(argument_1, _packed_params, 1.1920928955078125e-07, 0)
    return input

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_0.se.se.0.relu, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6296.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_0.se.se.1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6298.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6298.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6298.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6298.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6298.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d(argument_1, _packed_params, 0.00018600169278215617, 22)
    return input

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_0.se.se.2, type=Sigmoid:
class Sigmoid(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.activation.___torch_mangle_6299.Sigmoid,
    argument_1: Tensor) -> Tensor:
    return torch.sigmoid(argument_1)

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_0.se.mul, type=TorchMultiply:
class TorchMultiply(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  mul_func : __torch__.torch.ao.nn.quantized.modules.functional_modules.___torch_mangle_6302.QFunctional
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6303.TorchMultiply,
    argument_1: Tensor,
    argument_2: Tensor) -> Tensor:
    mul_func = self.mul_func
    activation_post_process = mul_func.activation_post_process
    input = ops.quantized.mul(argument_1, argument_2, 0.0042520989663898945, 60)
    _0 = (activation_post_process).forward()
    return input

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_0.se.mul.mul_func, type=QFunctional:
class QFunctional(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation_post_process : __torch__.torch.nn.modules.linear.___torch_mangle_6301.Identity

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_0.se.mul.mul_func.activation_post_process, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6301.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_0.pwl, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6305.Conv2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6306.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6307.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_0.pwl.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6305.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6305.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6305.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6305.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6305.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    _18 = ops.quantized.conv2d(argument_1, _packed_params, 0.043158065527677536, 51)
    return _18

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_0.pwl.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6306.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_0.res_conn, type=TorchAdd:
class TorchAdd(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  add_func : __torch__.torch.ao.nn.quantized.modules.functional_modules.___torch_mangle_6309.QFunctional
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6310.TorchAdd,
    argument_1: Tensor,
    argument_2: Tensor) -> Tensor:
    add_func = self.add_func
    activation_post_process = add_func.activation_post_process
    input = ops.quantized.add(argument_1, argument_2, 0.082980155944824219, 58)
    _0 = (activation_post_process).forward()
    return input

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_0.res_conn.add_func, type=QFunctional:
class QFunctional(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation_post_process : __torch__.torch.nn.modules.linear.___torch_mangle_6308.Identity

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_0.res_conn.add_func.activation_post_process, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6308.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_1, type=IRFBlock:
class IRFBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  pw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6315.ConvBNRelu
  dw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6317.ConvBNRelu
  se : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6328.SEModule
  pwl : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6331.ConvBNRelu
  res_conn : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6334.TorchAdd
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6335.IRFBlock,
    argument_1: Tensor) -> Tensor:
    res_conn = self.res_conn
    pwl = self.pwl
    se = self.se
    dw = self.dw
    pw = self.pw
    _0 = (dw).forward((pw).forward(argument_1, ), )
    _1 = (res_conn).forward((pwl).forward((se).forward(_0, ), ), argument_1, )
    return _1

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_1.pw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6312.ConvReLU2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6313.Identity
  relu : __torch__.torch.nn.modules.linear.___torch_mangle_6314.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6315.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    relu = self.relu
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    _2 = (relu).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_1.pw.conv, type=ConvReLU2d:
class ConvReLU2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6312.ConvReLU2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6312.ConvReLU2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6312.ConvReLU2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6312.ConvReLU2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6312.ConvReLU2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d_relu(argument_1, _packed_params, 0.044427476823329926, 0)
    return input

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_1.pw.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6313.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_1.pw.relu, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6314.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_1.dw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6316.Conv2d
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6317.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    conv = self.conv
    return (conv).forward(argument_1, )

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_1.dw.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6316.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6316.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6316.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6316.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6316.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    x = ops.quantized.conv2d(argument_1, _packed_params, 0.010373517870903015, 73)
    return x

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_1.se, type=SEModule:
class SEModule(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  avg_pool : __torch__.torch.nn.modules.pooling.___torch_mangle_6318.AdaptiveAvgPool2d
  se : __torch__.torch.nn.modules.container.___torch_mangle_6324.Sequential
  mul : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6327.TorchMultiply
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6328.SEModule,
    argument_1: Tensor) -> Tensor:
    mul = self.mul
    se = self.se
    avg_pool = self.avg_pool
    _0 = (se).forward((avg_pool).forward(argument_1, ), )
    return (mul).forward(argument_1, _0, )

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_1.se.avg_pool, type=AdaptiveAvgPool2d:
class AdaptiveAvgPool2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.pooling.___torch_mangle_6318.AdaptiveAvgPool2d,
    argument_1: Tensor) -> Tensor:
    input = torch.adaptive_avg_pool2d(argument_1, [1, 1])
    return input

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_1.se.se, type=Sequential:
class Sequential(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  __annotations__["0"] = __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6321.ConvBNRelu
  __annotations__["1"] = __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6322.Conv2d
  __annotations__["2"] = __torch__.torch.nn.modules.activation.___torch_mangle_6323.Sigmoid
  def forward(self: __torch__.torch.nn.modules.container.___torch_mangle_6324.Sequential,
    argument_1: Tensor) -> Tensor:
    _2 = getattr(self, "2")
    _1 = getattr(self, "1")
    _0 = getattr(self, "0")
    _3 = (_1).forward((_0).forward(argument_1, ), )
    return (_2).forward(_3, )

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_1.se.se.0, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6319.ConvReLU2d
  relu : __torch__.torch.nn.modules.linear.___torch_mangle_6320.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6321.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    relu = self.relu
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (relu).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_1.se.se.0.conv, type=ConvReLU2d:
class ConvReLU2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6319.ConvReLU2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6319.ConvReLU2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6319.ConvReLU2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6319.ConvReLU2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6319.ConvReLU2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d_relu(argument_1, _packed_params, 1.1920928955078125e-07, 0)
    return input

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_1.se.se.0.relu, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6320.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_1.se.se.1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6322.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6322.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6322.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6322.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6322.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d(argument_1, _packed_params, 9.525578934699297e-05, 32)
    return input

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_1.se.se.2, type=Sigmoid:
class Sigmoid(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.activation.___torch_mangle_6323.Sigmoid,
    argument_1: Tensor) -> Tensor:
    return torch.sigmoid(argument_1)

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_1.se.mul, type=TorchMultiply:
class TorchMultiply(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  mul_func : __torch__.torch.ao.nn.quantized.modules.functional_modules.___torch_mangle_6326.QFunctional
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6327.TorchMultiply,
    argument_1: Tensor,
    argument_2: Tensor) -> Tensor:
    mul_func = self.mul_func
    activation_post_process = mul_func.activation_post_process
    input = ops.quantized.mul(argument_1, argument_2, 0.0052361465059220791, 72)
    _0 = (activation_post_process).forward()
    return input

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_1.se.mul.mul_func, type=QFunctional:
class QFunctional(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation_post_process : __torch__.torch.nn.modules.linear.___torch_mangle_6325.Identity

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_1.se.mul.mul_func.activation_post_process, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6325.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_1.pwl, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6329.Conv2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6330.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6331.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_1.pwl.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6329.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6329.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6329.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6329.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6329.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    _18 = ops.quantized.conv2d(argument_1, _packed_params, 0.051569793373346329, 60)
    return _18

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_1.pwl.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6330.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_1.res_conn, type=TorchAdd:
class TorchAdd(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  add_func : __torch__.torch.ao.nn.quantized.modules.functional_modules.___torch_mangle_6333.QFunctional
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6334.TorchAdd,
    argument_1: Tensor,
    argument_2: Tensor) -> Tensor:
    add_func = self.add_func
    activation_post_process = add_func.activation_post_process
    input = ops.quantized.add(argument_1, argument_2, 0.097166500985622406, 61)
    _0 = (activation_post_process).forward()
    return input

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_1.res_conn.add_func, type=QFunctional:
class QFunctional(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation_post_process : __torch__.torch.nn.modules.linear.___torch_mangle_6332.Identity

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_1.res_conn.add_func.activation_post_process, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6332.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_2, type=IRFBlock:
class IRFBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  pw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6339.ConvBNRelu
  dw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6341.ConvBNRelu
  se : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6352.SEModule
  pwl : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6355.ConvBNRelu
  res_conn : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6358.TorchAdd
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6359.IRFBlock,
    argument_1: Tensor) -> Tensor:
    res_conn = self.res_conn
    pwl = self.pwl
    se = self.se
    dw = self.dw
    pw = self.pw
    _0 = (dw).forward((pw).forward(argument_1, ), )
    _1 = (res_conn).forward((pwl).forward((se).forward(_0, ), ), argument_1, )
    return _1

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_2.pw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6336.ConvReLU2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6337.Identity
  relu : __torch__.torch.nn.modules.linear.___torch_mangle_6338.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6339.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    relu = self.relu
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    _2 = (relu).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_2.pw.conv, type=ConvReLU2d:
class ConvReLU2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6336.ConvReLU2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6336.ConvReLU2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6336.ConvReLU2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6336.ConvReLU2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6336.ConvReLU2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d_relu(argument_1, _packed_params, 0.050065327435731888, 0)
    return input

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_2.pw.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6337.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_2.pw.relu, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6338.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_2.dw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6340.Conv2d
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6341.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    conv = self.conv
    return (conv).forward(argument_1, )

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_2.dw.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6340.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6340.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6340.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6340.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6340.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    x = ops.quantized.conv2d(argument_1, _packed_params, 0.011929290369153023, 56)
    return x

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_2.se, type=SEModule:
class SEModule(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  avg_pool : __torch__.torch.nn.modules.pooling.___torch_mangle_6342.AdaptiveAvgPool2d
  se : __torch__.torch.nn.modules.container.___torch_mangle_6348.Sequential
  mul : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6351.TorchMultiply
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6352.SEModule,
    argument_1: Tensor) -> Tensor:
    mul = self.mul
    se = self.se
    avg_pool = self.avg_pool
    _0 = (se).forward((avg_pool).forward(argument_1, ), )
    return (mul).forward(argument_1, _0, )

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_2.se.avg_pool, type=AdaptiveAvgPool2d:
class AdaptiveAvgPool2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.pooling.___torch_mangle_6342.AdaptiveAvgPool2d,
    argument_1: Tensor) -> Tensor:
    input = torch.adaptive_avg_pool2d(argument_1, [1, 1])
    return input

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_2.se.se, type=Sequential:
class Sequential(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  __annotations__["0"] = __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6345.ConvBNRelu
  __annotations__["1"] = __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6346.Conv2d
  __annotations__["2"] = __torch__.torch.nn.modules.activation.___torch_mangle_6347.Sigmoid
  def forward(self: __torch__.torch.nn.modules.container.___torch_mangle_6348.Sequential,
    argument_1: Tensor) -> Tensor:
    _2 = getattr(self, "2")
    _1 = getattr(self, "1")
    _0 = getattr(self, "0")
    _3 = (_1).forward((_0).forward(argument_1, ), )
    return (_2).forward(_3, )

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_2.se.se.0, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6343.ConvReLU2d
  relu : __torch__.torch.nn.modules.linear.___torch_mangle_6344.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6345.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    relu = self.relu
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (relu).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_2.se.se.0.conv, type=ConvReLU2d:
class ConvReLU2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6343.ConvReLU2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6343.ConvReLU2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6343.ConvReLU2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6343.ConvReLU2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6343.ConvReLU2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d_relu(argument_1, _packed_params, 1.1920928955078125e-07, 0)
    return input

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_2.se.se.0.relu, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6344.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_2.se.se.1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6346.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6346.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6346.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6346.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6346.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d(argument_1, _packed_params, 0.00011300288315396756, 37)
    return input

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_2.se.se.2, type=Sigmoid:
class Sigmoid(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.activation.___torch_mangle_6347.Sigmoid,
    argument_1: Tensor) -> Tensor:
    return torch.sigmoid(argument_1)

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_2.se.mul, type=TorchMultiply:
class TorchMultiply(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  mul_func : __torch__.torch.ao.nn.quantized.modules.functional_modules.___torch_mangle_6350.QFunctional
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6351.TorchMultiply,
    argument_1: Tensor,
    argument_2: Tensor) -> Tensor:
    mul_func = self.mul_func
    activation_post_process = mul_func.activation_post_process
    input = ops.quantized.mul(argument_1, argument_2, 0.0057127703912556171, 56)
    _0 = (activation_post_process).forward()
    return input

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_2.se.mul.mul_func, type=QFunctional:
class QFunctional(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation_post_process : __torch__.torch.nn.modules.linear.___torch_mangle_6349.Identity

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_2.se.mul.mul_func.activation_post_process, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6349.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_2.pwl, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6353.Conv2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6354.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6355.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_2.pwl.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6353.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6353.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6353.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6353.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6353.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    _18 = ops.quantized.conv2d(argument_1, _packed_params, 0.069201745092868805, 61)
    return _18

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_2.pwl.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6354.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_2.res_conn, type=TorchAdd:
class TorchAdd(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  add_func : __torch__.torch.ao.nn.quantized.modules.functional_modules.___torch_mangle_6357.QFunctional
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6358.TorchAdd,
    argument_1: Tensor,
    argument_2: Tensor) -> Tensor:
    add_func = self.add_func
    activation_post_process = add_func.activation_post_process
    input = ops.quantized.add(argument_1, argument_2, 0.10592420399188995, 63)
    _0 = (activation_post_process).forward()
    return input

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_2.res_conn.add_func, type=QFunctional:
class QFunctional(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation_post_process : __torch__.torch.nn.modules.linear.___torch_mangle_6356.Identity

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_2.res_conn.add_func.activation_post_process, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6356.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_3, type=IRFBlock:
class IRFBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  pw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6363.ConvBNRelu
  dw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6365.ConvBNRelu
  se : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6376.SEModule
  pwl : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6379.ConvBNRelu
  res_conn : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6382.TorchAdd
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6383.IRFBlock,
    argument_1: Tensor) -> Tensor:
    res_conn = self.res_conn
    pwl = self.pwl
    se = self.se
    dw = self.dw
    pw = self.pw
    _0 = (dw).forward((pw).forward(argument_1, ), )
    _1 = (res_conn).forward((pwl).forward((se).forward(_0, ), ), argument_1, )
    return _1

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_3.pw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6360.ConvReLU2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6361.Identity
  relu : __torch__.torch.nn.modules.linear.___torch_mangle_6362.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6363.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    relu = self.relu
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    _2 = (relu).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_3.pw.conv, type=ConvReLU2d:
class ConvReLU2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6360.ConvReLU2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6360.ConvReLU2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6360.ConvReLU2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6360.ConvReLU2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6360.ConvReLU2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d_relu(argument_1, _packed_params, 0.050053708255290985, 0)
    return input

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_3.pw.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6361.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_3.pw.relu, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6362.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_3.dw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6364.Conv2d
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6365.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    conv = self.conv
    return (conv).forward(argument_1, )

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_3.dw.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6364.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6364.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6364.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6364.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6364.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    x = ops.quantized.conv2d(argument_1, _packed_params, 0.012063684873282909, 76)
    return x

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_3.se, type=SEModule:
class SEModule(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  avg_pool : __torch__.torch.nn.modules.pooling.___torch_mangle_6366.AdaptiveAvgPool2d
  se : __torch__.torch.nn.modules.container.___torch_mangle_6372.Sequential
  mul : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6375.TorchMultiply
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6376.SEModule,
    argument_1: Tensor) -> Tensor:
    mul = self.mul
    se = self.se
    avg_pool = self.avg_pool
    _0 = (se).forward((avg_pool).forward(argument_1, ), )
    return (mul).forward(argument_1, _0, )

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_3.se.avg_pool, type=AdaptiveAvgPool2d:
class AdaptiveAvgPool2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.pooling.___torch_mangle_6366.AdaptiveAvgPool2d,
    argument_1: Tensor) -> Tensor:
    input = torch.adaptive_avg_pool2d(argument_1, [1, 1])
    return input

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_3.se.se, type=Sequential:
class Sequential(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  __annotations__["0"] = __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6369.ConvBNRelu
  __annotations__["1"] = __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6370.Conv2d
  __annotations__["2"] = __torch__.torch.nn.modules.activation.___torch_mangle_6371.Sigmoid
  def forward(self: __torch__.torch.nn.modules.container.___torch_mangle_6372.Sequential,
    argument_1: Tensor) -> Tensor:
    _2 = getattr(self, "2")
    _1 = getattr(self, "1")
    _0 = getattr(self, "0")
    _3 = (_1).forward((_0).forward(argument_1, ), )
    return (_2).forward(_3, )

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_3.se.se.0, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6367.ConvReLU2d
  relu : __torch__.torch.nn.modules.linear.___torch_mangle_6368.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6369.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    relu = self.relu
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (relu).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_3.se.se.0.conv, type=ConvReLU2d:
class ConvReLU2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6367.ConvReLU2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6367.ConvReLU2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6367.ConvReLU2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6367.ConvReLU2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6367.ConvReLU2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d_relu(argument_1, _packed_params, 1.1920928955078125e-07, 0)
    return input

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_3.se.se.0.relu, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6368.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_3.se.se.1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6370.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6370.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6370.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6370.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6370.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d(argument_1, _packed_params, 0.00013504094386007637, 31)
    return input

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_3.se.se.2, type=Sigmoid:
class Sigmoid(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.activation.___torch_mangle_6371.Sigmoid,
    argument_1: Tensor) -> Tensor:
    return torch.sigmoid(argument_1)

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_3.se.mul, type=TorchMultiply:
class TorchMultiply(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  mul_func : __torch__.torch.ao.nn.quantized.modules.functional_modules.___torch_mangle_6374.QFunctional
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6375.TorchMultiply,
    argument_1: Tensor,
    argument_2: Tensor) -> Tensor:
    mul_func = self.mul_func
    activation_post_process = mul_func.activation_post_process
    input = ops.quantized.mul(argument_1, argument_2, 0.0063616079278290272, 72)
    _0 = (activation_post_process).forward()
    return input

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_3.se.mul.mul_func, type=QFunctional:
class QFunctional(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation_post_process : __torch__.torch.nn.modules.linear.___torch_mangle_6373.Identity

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_3.se.mul.mul_func.activation_post_process, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6373.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_3.pwl, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6377.Conv2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6378.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6379.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_3.pwl.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6377.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6377.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6377.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6377.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6377.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    _18 = ops.quantized.conv2d(argument_1, _packed_params, 0.068887114524841309, 61)
    return _18

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_3.pwl.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6378.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_3.res_conn, type=TorchAdd:
class TorchAdd(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  add_func : __torch__.torch.ao.nn.quantized.modules.functional_modules.___torch_mangle_6381.QFunctional
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6382.TorchAdd,
    argument_1: Tensor,
    argument_2: Tensor) -> Tensor:
    add_func = self.add_func
    activation_post_process = add_func.activation_post_process
    input = ops.quantized.add(argument_1, argument_2, 0.12308783084154129, 57)
    _0 = (activation_post_process).forward()
    return input

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_3.res_conn.add_func, type=QFunctional:
class QFunctional(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation_post_process : __torch__.torch.nn.modules.linear.___torch_mangle_6380.Identity

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_3.res_conn.add_func.activation_post_process, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6380.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_4, type=IRFBlock:
class IRFBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  pw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6387.ConvBNRelu
  dw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6389.ConvBNRelu
  se : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6400.SEModule
  pwl : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6403.ConvBNRelu
  res_conn : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6406.TorchAdd
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6407.IRFBlock,
    argument_1: Tensor) -> Tensor:
    res_conn = self.res_conn
    pwl = self.pwl
    se = self.se
    dw = self.dw
    pw = self.pw
    _0 = (dw).forward((pw).forward(argument_1, ), )
    _1 = (res_conn).forward((pwl).forward((se).forward(_0, ), ), argument_1, )
    return _1

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_4.pw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6384.ConvReLU2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6385.Identity
  relu : __torch__.torch.nn.modules.linear.___torch_mangle_6386.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6387.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    relu = self.relu
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    _2 = (relu).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_4.pw.conv, type=ConvReLU2d:
class ConvReLU2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6384.ConvReLU2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6384.ConvReLU2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6384.ConvReLU2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6384.ConvReLU2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6384.ConvReLU2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d_relu(argument_1, _packed_params, 0.054365098476409912, 0)
    return input

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_4.pw.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6385.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_4.pw.relu, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6386.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_4.dw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6388.Conv2d
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6389.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    conv = self.conv
    return (conv).forward(argument_1, )

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_4.dw.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6388.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6388.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6388.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6388.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6388.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    x = ops.quantized.conv2d(argument_1, _packed_params, 0.011963602155447006, 65)
    return x

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_4.se, type=SEModule:
class SEModule(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  avg_pool : __torch__.torch.nn.modules.pooling.___torch_mangle_6390.AdaptiveAvgPool2d
  se : __torch__.torch.nn.modules.container.___torch_mangle_6396.Sequential
  mul : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6399.TorchMultiply
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6400.SEModule,
    argument_1: Tensor) -> Tensor:
    mul = self.mul
    se = self.se
    avg_pool = self.avg_pool
    _0 = (se).forward((avg_pool).forward(argument_1, ), )
    return (mul).forward(argument_1, _0, )

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_4.se.avg_pool, type=AdaptiveAvgPool2d:
class AdaptiveAvgPool2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.pooling.___torch_mangle_6390.AdaptiveAvgPool2d,
    argument_1: Tensor) -> Tensor:
    input = torch.adaptive_avg_pool2d(argument_1, [1, 1])
    return input

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_4.se.se, type=Sequential:
class Sequential(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  __annotations__["0"] = __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6393.ConvBNRelu
  __annotations__["1"] = __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6394.Conv2d
  __annotations__["2"] = __torch__.torch.nn.modules.activation.___torch_mangle_6395.Sigmoid
  def forward(self: __torch__.torch.nn.modules.container.___torch_mangle_6396.Sequential,
    argument_1: Tensor) -> Tensor:
    _2 = getattr(self, "2")
    _1 = getattr(self, "1")
    _0 = getattr(self, "0")
    _3 = (_1).forward((_0).forward(argument_1, ), )
    return (_2).forward(_3, )

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_4.se.se.0, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6391.ConvReLU2d
  relu : __torch__.torch.nn.modules.linear.___torch_mangle_6392.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6393.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    relu = self.relu
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (relu).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_4.se.se.0.conv, type=ConvReLU2d:
class ConvReLU2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6391.ConvReLU2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6391.ConvReLU2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6391.ConvReLU2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6391.ConvReLU2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6391.ConvReLU2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d_relu(argument_1, _packed_params, 1.1920928955078125e-07, 0)
    return input

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_4.se.se.0.relu, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6392.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_4.se.se.1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6394.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6394.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6394.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6394.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6394.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d(argument_1, _packed_params, 0.00012768710439559072, 39)
    return input

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_4.se.se.2, type=Sigmoid:
class Sigmoid(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.activation.___torch_mangle_6395.Sigmoid,
    argument_1: Tensor) -> Tensor:
    return torch.sigmoid(argument_1)

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_4.se.mul, type=TorchMultiply:
class TorchMultiply(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  mul_func : __torch__.torch.ao.nn.quantized.modules.functional_modules.___torch_mangle_6398.QFunctional
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6399.TorchMultiply,
    argument_1: Tensor,
    argument_2: Tensor) -> Tensor:
    mul_func = self.mul_func
    activation_post_process = mul_func.activation_post_process
    input = ops.quantized.mul(argument_1, argument_2, 0.0079161925241351128, 43)
    _0 = (activation_post_process).forward()
    return input

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_4.se.mul.mul_func, type=QFunctional:
class QFunctional(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation_post_process : __torch__.torch.nn.modules.linear.___torch_mangle_6397.Identity

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_4.se.mul.mul_func.activation_post_process, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6397.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_4.pwl, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6401.Conv2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6402.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6403.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_4.pwl.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6401.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6401.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6401.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6401.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6401.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    _18 = ops.quantized.conv2d(argument_1, _packed_params, 0.099086955189704895, 64)
    return _18

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_4.pwl.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6402.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_4.res_conn, type=TorchAdd:
class TorchAdd(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  add_func : __torch__.torch.ao.nn.quantized.modules.functional_modules.___torch_mangle_6405.QFunctional
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6406.TorchAdd,
    argument_1: Tensor,
    argument_2: Tensor) -> Tensor:
    add_func = self.add_func
    activation_post_process = add_func.activation_post_process
    input = ops.quantized.add(argument_1, argument_2, 0.16053634881973267, 58)
    _0 = (activation_post_process).forward()
    return input

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_4.res_conn.add_func, type=QFunctional:
class QFunctional(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation_post_process : __torch__.torch.nn.modules.linear.___torch_mangle_6404.Identity

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_feature.0.fbnetv2_0_4.res_conn.add_func.activation_post_process, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6404.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_regressor, type=RPNHeadConvRegressor:
class RPNHeadConvRegressor(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  cls_logits : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6410.Conv2d
  bbox_pred : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6411.Conv2d
  def forward(self: __torch__.d2go.modeling.backbone.modules.___torch_mangle_6412.RPNHeadConvRegressor,
    argument_1: Tensor) -> Tuple[Tensor, Tensor]:
    bbox_pred = self.bbox_pred
    cls_logits = self.cls_logits
    _0 = ((cls_logits).forward(argument_1, ), (bbox_pred).forward(argument_1, ))
    return _0

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_regressor.cls_logits, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6410.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6410.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6410.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6410.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6410.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    Xq = ops.quantized.conv2d(argument_1, _packed_params, 0.22892257571220398, 88)
    return Xq

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.rpn_regressor.bbox_pred, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6411.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6411.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6411.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6411.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6411.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    Xq = ops.quantized.conv2d(argument_1, _packed_params, 0.040997181087732315, 92)
    return Xq

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.quant_stubs, type=QuantStubNested:
class QuantStubNested(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  stubs : __torch__.torch.nn.modules.container.___torch_mangle_6414.ModuleList
  def forward(self: __torch__.mobile_cv.arch.utils.quantize_utils.___torch_mangle_6415.QuantStubNested,
    argument_1: Tensor) -> Tensor:
    stubs = self.stubs
    _0 = getattr(stubs, "0")
    return (_0).forward(argument_1, )

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.quant_stubs.stubs, type=ModuleList:
class ModuleList(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  __annotations__["0"] = __torch__.torch.ao.nn.quantized.modules.___torch_mangle_6413.Quantize

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.quant_stubs.stubs.0, type=Quantize:
class Quantize(Module):
  __parameters__ = []
  __buffers__ = ["scale", "zero_point", ]
  scale : Tensor
  zero_point : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.ao.nn.quantized.modules.___torch_mangle_6413.Quantize,
    argument_1: Tensor) -> Tensor:
    input = torch.quantize_per_tensor(argument_1, 0.075057744979858398, 63, 13)
    return input

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.dequant_stubs, type=QuantStubNested:
class QuantStubNested(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  stubs : __torch__.torch.nn.modules.container.___torch_mangle_6418.ModuleList
  def forward(self: __torch__.mobile_cv.arch.utils.quantize_utils.___torch_mangle_6419.QuantStubNested,
    argument_1: Tensor,
    argument_2: Tensor) -> Tuple[Tensor, Tensor]:
    stubs = self.stubs
    _1 = getattr(stubs, "1")
    stubs0 = self.stubs
    _0 = getattr(stubs0, "0")
    _2 = ((_0).forward(argument_1, ), (_1).forward(argument_2, ))
    return _2

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.dequant_stubs.stubs, type=ModuleList:
class ModuleList(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  __annotations__["0"] = __torch__.torch.ao.nn.quantized.modules.___torch_mangle_6416.DeQuantize
  __annotations__["1"] = __torch__.torch.ao.nn.quantized.modules.___torch_mangle_6417.DeQuantize

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.dequant_stubs.stubs.0, type=DeQuantize:
class DeQuantize(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.ao.nn.quantized.modules.___torch_mangle_6416.DeQuantize,
    argument_1: Tensor) -> Tensor:
    return torch.dequantize(argument_1)

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.rpn_head.dequant_stubs.stubs.1, type=DeQuantize:
class DeQuantize(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.ao.nn.quantized.modules.___torch_mangle_6417.DeQuantize,
    argument_1: Tensor) -> Tensor:
    return torch.dequantize(argument_1)

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.anchor_generator, type=DefaultAnchorGenerator:
class DefaultAnchorGenerator(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  cell_anchors : __torch__.detectron2.modeling.anchor_generator.___torch_mangle_6421.BufferList
  def forward(self: __torch__.detectron2.modeling.anchor_generator.___torch_mangle_6422.DefaultAnchorGenerator,
    argument_1: Tensor) -> Tensor:
    _0 = __torch__.detectron2.layers.wrappers.move_device_like
    _1 = __torch__.detectron2.layers.wrappers.move_device_like
    cell_anchors = self.cell_anchors
    _00 = getattr(cell_anchors, "0")
    grid_height = ops.prim.NumToTensor(torch.size(argument_1, 2))
    grid_width = ops.prim.NumToTensor(torch.size(argument_1, 3))
    _2 = annotate(number, torch.mul(grid_width, CONSTANTS.c0))
    _3 = torch.arange(0., _2, 16, dtype=6, layout=None, device=torch.device("cpu"), pin_memory=False)
    _4 = _0(_3, _00, )
    _5 = torch.mul(grid_height, CONSTANTS.c0)
    _6 = torch.arange(0., annotate(number, _5), 16, dtype=6, layout=None, device=torch.device("cpu"), pin_memory=False)
    shift_y, shift_x, = torch.meshgrid([_1(_6, _00, ), _4])
    shift_x0 = torch.reshape(shift_x, [-1])
    shift_y0 = torch.reshape(shift_y, [-1])
    _7 = [shift_x0, shift_y0, shift_x0, shift_y0]
    shifts = torch.stack(_7, 1)
    _8 = torch.add(torch.view(shifts, [-1, 1, 4]), torch.view(_00, [1, -1, 4]))
    tensor = torch.reshape(_8, [-1, 4])
    return torch.to(tensor, 6)

--------------------------------------------------------------------------------
Code for .model.model.proposal_generator.anchor_generator.cell_anchors, type=BufferList:
class BufferList(Module):
  __parameters__ = []
  __buffers__ = ["0", ]
  __annotations__ = []
  __annotations__["0"] = Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]

--------------------------------------------------------------------------------
Code for .model.model.roi_heads, type=StandardROIHeads:
class StandardROIHeads(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  box_pooler : __torch__.detectron2.modeling.poolers.___torch_mangle_6426.ROIPooler
  box_head : __torch__.mobile_cv.arch.utils.quantize_utils.___torch_mangle_6574.QuantWrapSubClass
  box_predictor : __torch__.mobile_cv.arch.utils.quantize_utils.___torch_mangle_6586.QuantWrapSubClass
  mask_pooler : __torch__.detectron2.modeling.poolers.___torch_mangle_6589.ROIPooler
  mask_head : __torch__.mobile_cv.arch.utils.quantize_utils.___torch_mangle_6658.QuantWrapSubClass
  def forward(self: __torch__.detectron2.modeling.roi_heads.roi_heads.___torch_mangle_6659.StandardROIHeads,
    argument_1: Tensor,
    argument_2: Tensor,
    image_size: Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor]:
    _0 = __torch__.torchvision.ops.boxes._batched_nms_coordinate_trick
    mask_head = self.mask_head
    mask_pooler = self.mask_pooler
    box_predictor = self.box_predictor
    box_head = self.box_head
    box_pooler = self.box_pooler
    _1 = (box_pooler).forward(argument_1, argument_2, )
    _2 = (box_predictor).forward((box_head).forward(_1, ), )
    _3, _4, = _2
    _5 = ops.prim.NumToTensor(torch.size(argument_2, 0))
    _6 = int(_5)
    deltas = torch.to(_3, 6)
    boxes = torch.to(argument_2, 6)
    _7 = torch.slice(boxes, 0, 0, 9223372036854775807)
    _8 = torch.select(_7, 1, 2)
    _9 = torch.slice(boxes, 0, 0, 9223372036854775807)
    widths = torch.sub(_8, torch.select(_9, 1, 0))
    _10 = torch.slice(boxes, 0, 0, 9223372036854775807)
    _11 = torch.select(_10, 1, 3)
    _12 = torch.slice(boxes, 0, 0, 9223372036854775807)
    heights = torch.sub(_11, torch.select(_12, 1, 1))
    _13 = torch.slice(boxes, 0, 0, 9223372036854775807)
    ctr_x = torch.add(torch.select(_13, 1, 0), torch.mul(widths, CONSTANTS.c0))
    _14 = torch.slice(boxes, 0, 0, 9223372036854775807)
    ctr_y = torch.add(torch.select(_14, 1, 1), torch.mul(heights, CONSTANTS.c0))
    _15 = torch.slice(deltas, 0, 0, 9223372036854775807)
    _16 = torch.slice(_15, 1, 0, 9223372036854775807, 4)
    dx = torch.div(_16, CONSTANTS.c1)
    _17 = torch.slice(deltas, 0, 0, 9223372036854775807)
    _18 = torch.slice(_17, 1, 1, 9223372036854775807, 4)
    dy = torch.div(_18, CONSTANTS.c1)
    _19 = torch.slice(deltas, 0, 0, 9223372036854775807)
    _20 = torch.slice(_19, 1, 2, 9223372036854775807, 4)
    dw = torch.div(_20, CONSTANTS.c2)
    _21 = torch.slice(deltas, 0, 0, 9223372036854775807)
    _22 = torch.slice(_21, 1, 3, 9223372036854775807, 4)
    dh = torch.div(_22, CONSTANTS.c2)
    dw0 = torch.clamp(dw, None, 4.1351665567423561)
    dh0 = torch.clamp(dh, None, 4.1351665567423561)
    _23 = torch.slice(widths, 0, 0, 9223372036854775807)
    _24 = torch.mul(dx, torch.unsqueeze(_23, 1))
    _25 = torch.slice(ctr_x, 0, 0, 9223372036854775807)
    pred_ctr_x = torch.add(_24, torch.unsqueeze(_25, 1))
    _26 = torch.slice(heights, 0, 0, 9223372036854775807)
    _27 = torch.mul(dy, torch.unsqueeze(_26, 1))
    _28 = torch.slice(ctr_y, 0, 0, 9223372036854775807)
    pred_ctr_y = torch.add(_27, torch.unsqueeze(_28, 1))
    _29 = torch.exp(dw0)
    _30 = torch.slice(widths, 0, 0, 9223372036854775807)
    pred_w = torch.mul(_29, torch.unsqueeze(_30, 1))
    _31 = torch.exp(dh0)
    _32 = torch.slice(heights, 0, 0, 9223372036854775807)
    pred_h = torch.mul(_31, torch.unsqueeze(_32, 1))
    x1 = torch.sub(pred_ctr_x, torch.mul(pred_w, CONSTANTS.c0))
    y1 = torch.sub(pred_ctr_y, torch.mul(pred_h, CONSTANTS.c0))
    x2 = torch.add(pred_ctr_x, torch.mul(pred_w, CONSTANTS.c0))
    y2 = torch.add(pred_ctr_y, torch.mul(pred_h, CONSTANTS.c0))
    pred_boxes = torch.stack([x1, y1, x2, y2], -1)
    _33 = ops.prim.NumToTensor(torch.size(deltas, 0))
    _34 = int(_33)
    _35 = ops.prim.NumToTensor(torch.size(deltas, 1))
    _36 = torch.reshape(pred_boxes, [_34, int(_35)])
    boxes0, = torch.split_with_sizes(_36, [_6])
    _37 = ops.prim.NumToTensor(torch.size(boxes, 0))
    _38 = int(_37)
    _39 = torch.split_with_sizes(torch.softmax(_4, -1), [_38])
    scores, = _39
    _40 = torch.slice(scores, 0, 0, 9223372036854775807)
    scores0 = torch.slice(_40, 1, 0, -1)
    _41 = ops.prim.NumToTensor(torch.size(boxes0, 1))
    num_bbox_reg_classes = torch.floor_divide(_41, CONSTANTS.c3)
    _42 = int(num_bbox_reg_classes)
    tensor = torch.reshape(boxes0, [-1, 4])
    tensor0 = torch.to(tensor, 6)
    h, w, = torch.unbind(image_size)
    _43 = annotate(number, h)
    _44 = annotate(number, w)
    _45 = annotate(number, h)
    _46 = annotate(number, w)
    _47 = torch.slice(tensor0, 0, 0, 9223372036854775807)
    x10 = torch.clamp(torch.select(_47, 1, 0), 0, _46)
    _48 = torch.slice(tensor0, 0, 0, 9223372036854775807)
    y10 = torch.clamp(torch.select(_48, 1, 1), 0, _45)
    _49 = torch.slice(tensor0, 0, 0, 9223372036854775807)
    x20 = torch.clamp(torch.select(_49, 1, 2), 0, _44)
    _50 = torch.slice(tensor0, 0, 0, 9223372036854775807)
    y20 = torch.clamp(torch.select(_50, 1, 3), 0, _43)
    _51 = torch.stack([x10, y10, x20, y20], -1)
    boxes1 = torch.view(_51, [-1, _42, 4])
    filter_mask = torch.gt(scores0, 0.20000000000000001)
    filter_inds = torch.nonzero(filter_mask)
    _52 = annotate(List[Optional[Tensor]], [filter_mask])
    boxes2 = torch.index(boxes1, _52)
    _53 = annotate(List[Optional[Tensor]], [filter_mask])
    scores1 = torch.index(scores0, _53)
    _54 = torch.slice(filter_inds, 0, 0, 9223372036854775807)
    _55 = torch.select(_54, 1, 1)
    boxes3 = torch.to(boxes2, 6)
    keep = _0(boxes3, scores1, _55, 0.5, )
    keep0 = torch.slice(keep, 0, 0, 100)
    _56 = annotate(List[Optional[Tensor]], [keep0])
    tensor1 = torch.index(boxes3, _56)
    _57 = annotate(List[Optional[Tensor]], [keep0])
    _58 = torch.index(scores1, _57)
    _59 = annotate(List[Optional[Tensor]], [keep0])
    filter_inds0 = torch.index(filter_inds, _59)
    tensor2 = torch.to(tensor1, 6)
    _60 = torch.slice(filter_inds0, 0, 0, 9223372036854775807)
    class_pred = torch.select(_60, 1, 1)
    _61 = (mask_pooler).forward(argument_1, tensor2, )
    _62 = (mask_head).forward(_61, class_pred, tensor2, )
    return (tensor2, class_pred, _62, _58)

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_pooler, type=ROIPooler:
class ROIPooler(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  level_poolers : __torch__.torch.nn.modules.container.___torch_mangle_6425.ModuleList
  def forward(self: __torch__.detectron2.modeling.poolers.___torch_mangle_6426.ROIPooler,
    argument_1: Tensor,
    argument_2: Tensor) -> Tensor:
    _0 = __torch__.detectron2.modeling.poolers._convert_boxes_to_pooler_format
    level_poolers = self.level_poolers
    _00 = getattr(level_poolers, "0")
    _1 = torch.cat([argument_2])
    _2 = ops.prim.NumToTensor(torch.size(argument_2, 0))
    rois = _0(_1, torch.stack([_2]), )
    return (_00).forward(rois, argument_1, )

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_pooler.level_poolers, type=ModuleList:
class ModuleList(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  __annotations__["0"] = __torch__.detectron2.layers.roi_align.___torch_mangle_6424.ROIAlign

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_pooler.level_poolers.0, type=ROIAlign:
class ROIAlign(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.roi_align.___torch_mangle_6424.ROIAlign,
    rois: Tensor,
    argument_2: Tensor) -> Tensor:
    boxes = torch.to(rois, 6)
    X = ops.torchvision.roi_align(argument_2, boxes, 0.0625, 6, 6, 0, True)
    return X

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head, type=QuantWrapSubClass:
class QuantWrapSubClass(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  roi_box_conv : __torch__.d2go.modeling.backbone.fbnet_v2.___torch_mangle_6566.FBNetModule
  avgpool : __torch__.torch.nn.modules.pooling.___torch_mangle_6567.AdaptiveAvgPool2d
  quant_stubs : __torch__.mobile_cv.arch.utils.quantize_utils.___torch_mangle_6570.QuantStubNested
  dequant_stubs : __torch__.mobile_cv.arch.utils.quantize_utils.___torch_mangle_6573.QuantStubNested
  def forward(self: __torch__.mobile_cv.arch.utils.quantize_utils.___torch_mangle_6574.QuantWrapSubClass,
    argument_1: Tensor) -> Tensor:
    dequant_stubs = self.dequant_stubs
    avgpool = self.avgpool
    roi_box_conv = self.roi_box_conv
    quant_stubs = self.quant_stubs
    _0 = (roi_box_conv).forward((quant_stubs).forward(argument_1, ), )
    _1 = (dequant_stubs).forward((avgpool).forward(_0, ), )
    return _1

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv, type=FBNetModule:
class FBNetModule(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  __annotations__["0"] = __torch__.torch.nn.modules.container.___torch_mangle_6565.Sequential
  def forward(self: __torch__.d2go.modeling.backbone.fbnet_v2.___torch_mangle_6566.FBNetModule,
    argument_1: Tensor) -> Tensor:
    _0 = getattr(self, "0")
    return (_0).forward(argument_1, )

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0, type=Sequential:
class Sequential(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  fbnetv2_0_0 : __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6447.IRFBlock
  fbnetv2_0_1 : __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6471.IRFBlock
  fbnetv2_0_2 : __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6495.IRFBlock
  fbnetv2_0_3 : __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6519.IRFBlock
  fbnetv2_0_4 : __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6543.IRFBlock
  fbnetv2_0_5 : __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6564.IRFBlock
  def forward(self: __torch__.torch.nn.modules.container.___torch_mangle_6565.Sequential,
    argument_1: Tensor) -> Tensor:
    fbnetv2_0_5 = self.fbnetv2_0_5
    fbnetv2_0_4 = self.fbnetv2_0_4
    fbnetv2_0_3 = self.fbnetv2_0_3
    fbnetv2_0_2 = self.fbnetv2_0_2
    fbnetv2_0_1 = self.fbnetv2_0_1
    fbnetv2_0_0 = self.fbnetv2_0_0
    _0 = (fbnetv2_0_1).forward((fbnetv2_0_0).forward(argument_1, ), )
    _1 = (fbnetv2_0_3).forward((fbnetv2_0_2).forward(_0, ), )
    _2 = (fbnetv2_0_5).forward((fbnetv2_0_4).forward(_1, ), )
    return _2

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_0, type=IRFBlock:
class IRFBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  pw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6430.ConvBNRelu
  dw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6432.ConvBNRelu
  se : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6443.SEModule
  pwl : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6446.ConvBNRelu
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6447.IRFBlock,
    argument_1: Tensor) -> Tensor:
    pwl = self.pwl
    se = self.se
    dw = self.dw
    pw = self.pw
    _0 = (dw).forward((pw).forward(argument_1, ), )
    return (pwl).forward((se).forward(_0, ), )

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_0.pw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6427.ConvReLU2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6428.Identity
  relu : __torch__.torch.nn.modules.linear.___torch_mangle_6429.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6430.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    relu = self.relu
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    _2 = (relu).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_0.pw.conv, type=ConvReLU2d:
class ConvReLU2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6427.ConvReLU2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6427.ConvReLU2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6427.ConvReLU2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6427.ConvReLU2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6427.ConvReLU2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d_relu(argument_1, _packed_params, 0.062738522887229919, 0)
    return input

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_0.pw.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6428.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_0.pw.relu, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6429.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_0.dw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6431.Conv2d
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6432.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    conv = self.conv
    return (conv).forward(argument_1, )

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_0.dw.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6431.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6431.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6431.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6431.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6431.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    x = ops.quantized.conv2d(argument_1, _packed_params, 0.026383798569440842, 73)
    return x

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_0.se, type=SEModule:
class SEModule(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  avg_pool : __torch__.torch.nn.modules.pooling.___torch_mangle_6433.AdaptiveAvgPool2d
  se : __torch__.torch.nn.modules.container.___torch_mangle_6439.Sequential
  mul : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6442.TorchMultiply
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6443.SEModule,
    argument_1: Tensor) -> Tensor:
    mul = self.mul
    se = self.se
    avg_pool = self.avg_pool
    _0 = (se).forward((avg_pool).forward(argument_1, ), )
    return (mul).forward(argument_1, _0, )

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_0.se.avg_pool, type=AdaptiveAvgPool2d:
class AdaptiveAvgPool2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.pooling.___torch_mangle_6433.AdaptiveAvgPool2d,
    argument_1: Tensor) -> Tensor:
    input = torch.adaptive_avg_pool2d(argument_1, [1, 1])
    return input

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_0.se.se, type=Sequential:
class Sequential(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  __annotations__["0"] = __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6436.ConvBNRelu
  __annotations__["1"] = __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6437.Conv2d
  __annotations__["2"] = __torch__.torch.nn.modules.activation.___torch_mangle_6438.Sigmoid
  def forward(self: __torch__.torch.nn.modules.container.___torch_mangle_6439.Sequential,
    argument_1: Tensor) -> Tensor:
    _2 = getattr(self, "2")
    _1 = getattr(self, "1")
    _0 = getattr(self, "0")
    _3 = (_1).forward((_0).forward(argument_1, ), )
    return (_2).forward(_3, )

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_0.se.se.0, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6434.ConvReLU2d
  relu : __torch__.torch.nn.modules.linear.___torch_mangle_6435.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6436.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    relu = self.relu
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (relu).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_0.se.se.0.conv, type=ConvReLU2d:
class ConvReLU2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6434.ConvReLU2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6434.ConvReLU2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6434.ConvReLU2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6434.ConvReLU2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6434.ConvReLU2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d_relu(argument_1, _packed_params, 1.1920928955078125e-07, 0)
    return input

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_0.se.se.0.relu, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6435.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_0.se.se.1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6437.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6437.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6437.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6437.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6437.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d(argument_1, _packed_params, 0.00038170075276866555, 14)
    return input

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_0.se.se.2, type=Sigmoid:
class Sigmoid(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.activation.___torch_mangle_6438.Sigmoid,
    argument_1: Tensor) -> Tensor:
    return torch.sigmoid(argument_1)

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_0.se.mul, type=TorchMultiply:
class TorchMultiply(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  mul_func : __torch__.torch.ao.nn.quantized.modules.functional_modules.___torch_mangle_6441.QFunctional
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6442.TorchMultiply,
    argument_1: Tensor,
    argument_2: Tensor) -> Tensor:
    mul_func = self.mul_func
    activation_post_process = mul_func.activation_post_process
    input = ops.quantized.mul(argument_1, argument_2, 0.013206256553530693, 73)
    _0 = (activation_post_process).forward()
    return input

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_0.se.mul.mul_func, type=QFunctional:
class QFunctional(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation_post_process : __torch__.torch.nn.modules.linear.___torch_mangle_6440.Identity

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_0.se.mul.mul_func.activation_post_process, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6440.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_0.pwl, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6444.Conv2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6445.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6446.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_0.pwl.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6444.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6444.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6444.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6444.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6444.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d(argument_1, _packed_params, 0.11668611317873001, 66)
    return input

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_0.pwl.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6445.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_1, type=IRFBlock:
class IRFBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  pw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6451.ConvBNRelu
  dw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6453.ConvBNRelu
  se : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6464.SEModule
  pwl : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6467.ConvBNRelu
  res_conn : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6470.TorchAdd
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6471.IRFBlock,
    argument_1: Tensor) -> Tensor:
    res_conn = self.res_conn
    pwl = self.pwl
    se = self.se
    dw = self.dw
    pw = self.pw
    _0 = (dw).forward((pw).forward(argument_1, ), )
    _1 = (res_conn).forward((pwl).forward((se).forward(_0, ), ), argument_1, )
    return _1

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_1.pw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6448.ConvReLU2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6449.Identity
  relu : __torch__.torch.nn.modules.linear.___torch_mangle_6450.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6451.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    relu = self.relu
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    _2 = (relu).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_1.pw.conv, type=ConvReLU2d:
class ConvReLU2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6448.ConvReLU2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6448.ConvReLU2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6448.ConvReLU2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6448.ConvReLU2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6448.ConvReLU2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d_relu(argument_1, _packed_params, 0.052424553781747818, 0)
    return input

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_1.pw.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6449.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_1.pw.relu, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6450.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_1.dw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6452.Conv2d
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6453.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    conv = self.conv
    return (conv).forward(argument_1, )

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_1.dw.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6452.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6452.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6452.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6452.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6452.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    x = ops.quantized.conv2d(argument_1, _packed_params, 0.0063442853279411793, 64)
    return x

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_1.se, type=SEModule:
class SEModule(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  avg_pool : __torch__.torch.nn.modules.pooling.___torch_mangle_6454.AdaptiveAvgPool2d
  se : __torch__.torch.nn.modules.container.___torch_mangle_6460.Sequential
  mul : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6463.TorchMultiply
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6464.SEModule,
    argument_1: Tensor) -> Tensor:
    mul = self.mul
    se = self.se
    avg_pool = self.avg_pool
    _0 = (se).forward((avg_pool).forward(argument_1, ), )
    return (mul).forward(argument_1, _0, )

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_1.se.avg_pool, type=AdaptiveAvgPool2d:
class AdaptiveAvgPool2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.pooling.___torch_mangle_6454.AdaptiveAvgPool2d,
    argument_1: Tensor) -> Tensor:
    input = torch.adaptive_avg_pool2d(argument_1, [1, 1])
    return input

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_1.se.se, type=Sequential:
class Sequential(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  __annotations__["0"] = __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6457.ConvBNRelu
  __annotations__["1"] = __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6458.Conv2d
  __annotations__["2"] = __torch__.torch.nn.modules.activation.___torch_mangle_6459.Sigmoid
  def forward(self: __torch__.torch.nn.modules.container.___torch_mangle_6460.Sequential,
    argument_1: Tensor) -> Tensor:
    _2 = getattr(self, "2")
    _1 = getattr(self, "1")
    _0 = getattr(self, "0")
    _3 = (_1).forward((_0).forward(argument_1, ), )
    return (_2).forward(_3, )

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_1.se.se.0, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6455.ConvReLU2d
  relu : __torch__.torch.nn.modules.linear.___torch_mangle_6456.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6457.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    relu = self.relu
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (relu).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_1.se.se.0.conv, type=ConvReLU2d:
class ConvReLU2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6455.ConvReLU2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6455.ConvReLU2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6455.ConvReLU2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6455.ConvReLU2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6455.ConvReLU2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d_relu(argument_1, _packed_params, 1.1920928955078125e-07, 0)
    return input

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_1.se.se.0.relu, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6456.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_1.se.se.1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6458.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6458.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6458.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6458.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6458.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d(argument_1, _packed_params, 0.00010648079478414729, 71)
    return input

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_1.se.se.2, type=Sigmoid:
class Sigmoid(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.activation.___torch_mangle_6459.Sigmoid,
    argument_1: Tensor) -> Tensor:
    return torch.sigmoid(argument_1)

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_1.se.mul, type=TorchMultiply:
class TorchMultiply(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  mul_func : __torch__.torch.ao.nn.quantized.modules.functional_modules.___torch_mangle_6462.QFunctional
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6463.TorchMultiply,
    argument_1: Tensor,
    argument_2: Tensor) -> Tensor:
    mul_func = self.mul_func
    activation_post_process = mul_func.activation_post_process
    input = ops.quantized.mul(argument_1, argument_2, 0.0030735672917217016, 62)
    _0 = (activation_post_process).forward()
    return input

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_1.se.mul.mul_func, type=QFunctional:
class QFunctional(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation_post_process : __torch__.torch.nn.modules.linear.___torch_mangle_6461.Identity

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_1.se.mul.mul_func.activation_post_process, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6461.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_1.pwl, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6465.Conv2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6466.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6467.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_1.pwl.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6465.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6465.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6465.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6465.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6465.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    _18 = ops.quantized.conv2d(argument_1, _packed_params, 0.093607373535633087, 59)
    return _18

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_1.pwl.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6466.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_1.res_conn, type=TorchAdd:
class TorchAdd(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  add_func : __torch__.torch.ao.nn.quantized.modules.functional_modules.___torch_mangle_6469.QFunctional
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6470.TorchAdd,
    argument_1: Tensor,
    argument_2: Tensor) -> Tensor:
    add_func = self.add_func
    activation_post_process = add_func.activation_post_process
    input = ops.quantized.add(argument_1, argument_2, 0.11917215585708618, 66)
    _0 = (activation_post_process).forward()
    return input

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_1.res_conn.add_func, type=QFunctional:
class QFunctional(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation_post_process : __torch__.torch.nn.modules.linear.___torch_mangle_6468.Identity

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_1.res_conn.add_func.activation_post_process, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6468.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_2, type=IRFBlock:
class IRFBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  pw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6475.ConvBNRelu
  dw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6477.ConvBNRelu
  se : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6488.SEModule
  pwl : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6491.ConvBNRelu
  res_conn : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6494.TorchAdd
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6495.IRFBlock,
    argument_1: Tensor) -> Tensor:
    res_conn = self.res_conn
    pwl = self.pwl
    se = self.se
    dw = self.dw
    pw = self.pw
    _0 = (dw).forward((pw).forward(argument_1, ), )
    _1 = (res_conn).forward((pwl).forward((se).forward(_0, ), ), argument_1, )
    return _1

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_2.pw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6472.ConvReLU2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6473.Identity
  relu : __torch__.torch.nn.modules.linear.___torch_mangle_6474.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6475.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    relu = self.relu
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    _2 = (relu).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_2.pw.conv, type=ConvReLU2d:
class ConvReLU2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6472.ConvReLU2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6472.ConvReLU2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6472.ConvReLU2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6472.ConvReLU2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6472.ConvReLU2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d_relu(argument_1, _packed_params, 0.042814776301383972, 0)
    return input

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_2.pw.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6473.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_2.pw.relu, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6474.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_2.dw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6476.Conv2d
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6477.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    conv = self.conv
    return (conv).forward(argument_1, )

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_2.dw.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6476.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6476.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6476.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6476.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6476.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    x = ops.quantized.conv2d(argument_1, _packed_params, 0.0075231813825666904, 80)
    return x

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_2.se, type=SEModule:
class SEModule(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  avg_pool : __torch__.torch.nn.modules.pooling.___torch_mangle_6478.AdaptiveAvgPool2d
  se : __torch__.torch.nn.modules.container.___torch_mangle_6484.Sequential
  mul : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6487.TorchMultiply
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6488.SEModule,
    argument_1: Tensor) -> Tensor:
    mul = self.mul
    se = self.se
    avg_pool = self.avg_pool
    _0 = (se).forward((avg_pool).forward(argument_1, ), )
    return (mul).forward(argument_1, _0, )

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_2.se.avg_pool, type=AdaptiveAvgPool2d:
class AdaptiveAvgPool2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.pooling.___torch_mangle_6478.AdaptiveAvgPool2d,
    argument_1: Tensor) -> Tensor:
    input = torch.adaptive_avg_pool2d(argument_1, [1, 1])
    return input

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_2.se.se, type=Sequential:
class Sequential(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  __annotations__["0"] = __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6481.ConvBNRelu
  __annotations__["1"] = __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6482.Conv2d
  __annotations__["2"] = __torch__.torch.nn.modules.activation.___torch_mangle_6483.Sigmoid
  def forward(self: __torch__.torch.nn.modules.container.___torch_mangle_6484.Sequential,
    argument_1: Tensor) -> Tensor:
    _2 = getattr(self, "2")
    _1 = getattr(self, "1")
    _0 = getattr(self, "0")
    _3 = (_1).forward((_0).forward(argument_1, ), )
    return (_2).forward(_3, )

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_2.se.se.0, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6479.ConvReLU2d
  relu : __torch__.torch.nn.modules.linear.___torch_mangle_6480.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6481.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    relu = self.relu
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (relu).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_2.se.se.0.conv, type=ConvReLU2d:
class ConvReLU2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6479.ConvReLU2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6479.ConvReLU2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6479.ConvReLU2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6479.ConvReLU2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6479.ConvReLU2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d_relu(argument_1, _packed_params, 1.1920928955078125e-07, 0)
    return input

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_2.se.se.0.relu, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6480.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_2.se.se.1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6482.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6482.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6482.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6482.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6482.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d(argument_1, _packed_params, 0.00010738168202806264, 39)
    return input

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_2.se.se.2, type=Sigmoid:
class Sigmoid(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.activation.___torch_mangle_6483.Sigmoid,
    argument_1: Tensor) -> Tensor:
    return torch.sigmoid(argument_1)

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_2.se.mul, type=TorchMultiply:
class TorchMultiply(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  mul_func : __torch__.torch.ao.nn.quantized.modules.functional_modules.___torch_mangle_6486.QFunctional
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6487.TorchMultiply,
    argument_1: Tensor,
    argument_2: Tensor) -> Tensor:
    mul_func = self.mul_func
    activation_post_process = mul_func.activation_post_process
    input = ops.quantized.mul(argument_1, argument_2, 0.0037761512212455273, 80)
    _0 = (activation_post_process).forward()
    return input

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_2.se.mul.mul_func, type=QFunctional:
class QFunctional(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation_post_process : __torch__.torch.nn.modules.linear.___torch_mangle_6485.Identity

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_2.se.mul.mul_func.activation_post_process, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6485.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_2.pwl, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6489.Conv2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6490.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6491.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_2.pwl.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6489.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6489.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6489.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6489.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6489.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    _18 = ops.quantized.conv2d(argument_1, _packed_params, 0.094367966055870056, 61)
    return _18

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_2.pwl.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6490.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_2.res_conn, type=TorchAdd:
class TorchAdd(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  add_func : __torch__.torch.ao.nn.quantized.modules.functional_modules.___torch_mangle_6493.QFunctional
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6494.TorchAdd,
    argument_1: Tensor,
    argument_2: Tensor) -> Tensor:
    add_func = self.add_func
    activation_post_process = add_func.activation_post_process
    input = ops.quantized.add(argument_1, argument_2, 0.13020206987857819, 64)
    _0 = (activation_post_process).forward()
    return input

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_2.res_conn.add_func, type=QFunctional:
class QFunctional(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation_post_process : __torch__.torch.nn.modules.linear.___torch_mangle_6492.Identity

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_2.res_conn.add_func.activation_post_process, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6492.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_3, type=IRFBlock:
class IRFBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  pw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6499.ConvBNRelu
  dw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6501.ConvBNRelu
  se : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6512.SEModule
  pwl : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6515.ConvBNRelu
  res_conn : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6518.TorchAdd
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6519.IRFBlock,
    argument_1: Tensor) -> Tensor:
    res_conn = self.res_conn
    pwl = self.pwl
    se = self.se
    dw = self.dw
    pw = self.pw
    _0 = (dw).forward((pw).forward(argument_1, ), )
    _1 = (res_conn).forward((pwl).forward((se).forward(_0, ), ), argument_1, )
    return _1

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_3.pw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6496.ConvReLU2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6497.Identity
  relu : __torch__.torch.nn.modules.linear.___torch_mangle_6498.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6499.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    relu = self.relu
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    _2 = (relu).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_3.pw.conv, type=ConvReLU2d:
class ConvReLU2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6496.ConvReLU2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6496.ConvReLU2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6496.ConvReLU2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6496.ConvReLU2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6496.ConvReLU2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d_relu(argument_1, _packed_params, 0.032224860042333603, 0)
    return input

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_3.pw.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6497.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_3.pw.relu, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6498.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_3.dw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6500.Conv2d
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6501.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    conv = self.conv
    return (conv).forward(argument_1, )

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_3.dw.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6500.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6500.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6500.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6500.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6500.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    x = ops.quantized.conv2d(argument_1, _packed_params, 0.0066443523392081261, 79)
    return x

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_3.se, type=SEModule:
class SEModule(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  avg_pool : __torch__.torch.nn.modules.pooling.___torch_mangle_6502.AdaptiveAvgPool2d
  se : __torch__.torch.nn.modules.container.___torch_mangle_6508.Sequential
  mul : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6511.TorchMultiply
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6512.SEModule,
    argument_1: Tensor) -> Tensor:
    mul = self.mul
    se = self.se
    avg_pool = self.avg_pool
    _0 = (se).forward((avg_pool).forward(argument_1, ), )
    return (mul).forward(argument_1, _0, )

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_3.se.avg_pool, type=AdaptiveAvgPool2d:
class AdaptiveAvgPool2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.pooling.___torch_mangle_6502.AdaptiveAvgPool2d,
    argument_1: Tensor) -> Tensor:
    input = torch.adaptive_avg_pool2d(argument_1, [1, 1])
    return input

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_3.se.se, type=Sequential:
class Sequential(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  __annotations__["0"] = __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6505.ConvBNRelu
  __annotations__["1"] = __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6506.Conv2d
  __annotations__["2"] = __torch__.torch.nn.modules.activation.___torch_mangle_6507.Sigmoid
  def forward(self: __torch__.torch.nn.modules.container.___torch_mangle_6508.Sequential,
    argument_1: Tensor) -> Tensor:
    _2 = getattr(self, "2")
    _1 = getattr(self, "1")
    _0 = getattr(self, "0")
    _3 = (_1).forward((_0).forward(argument_1, ), )
    return (_2).forward(_3, )

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_3.se.se.0, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6503.ConvReLU2d
  relu : __torch__.torch.nn.modules.linear.___torch_mangle_6504.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6505.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    relu = self.relu
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (relu).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_3.se.se.0.conv, type=ConvReLU2d:
class ConvReLU2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6503.ConvReLU2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6503.ConvReLU2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6503.ConvReLU2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6503.ConvReLU2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6503.ConvReLU2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d_relu(argument_1, _packed_params, 1.1920928955078125e-07, 0)
    return input

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_3.se.se.0.relu, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6504.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_3.se.se.1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6506.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6506.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6506.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6506.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6506.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d(argument_1, _packed_params, 0.00014222526806406677, 34)
    return input

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_3.se.se.2, type=Sigmoid:
class Sigmoid(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.activation.___torch_mangle_6507.Sigmoid,
    argument_1: Tensor) -> Tensor:
    return torch.sigmoid(argument_1)

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_3.se.mul, type=TorchMultiply:
class TorchMultiply(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  mul_func : __torch__.torch.ao.nn.quantized.modules.functional_modules.___torch_mangle_6510.QFunctional
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6511.TorchMultiply,
    argument_1: Tensor,
    argument_2: Tensor) -> Tensor:
    mul_func = self.mul_func
    activation_post_process = mul_func.activation_post_process
    input = ops.quantized.mul(argument_1, argument_2, 0.0031204482074826956, 75)
    _0 = (activation_post_process).forward()
    return input

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_3.se.mul.mul_func, type=QFunctional:
class QFunctional(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation_post_process : __torch__.torch.nn.modules.linear.___torch_mangle_6509.Identity

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_3.se.mul.mul_func.activation_post_process, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6509.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_3.pwl, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6513.Conv2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6514.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6515.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_3.pwl.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6513.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6513.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6513.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6513.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6513.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    _18 = ops.quantized.conv2d(argument_1, _packed_params, 0.1437276154756546, 49)
    return _18

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_3.pwl.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6514.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_3.res_conn, type=TorchAdd:
class TorchAdd(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  add_func : __torch__.torch.ao.nn.quantized.modules.functional_modules.___torch_mangle_6517.QFunctional
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6518.TorchAdd,
    argument_1: Tensor,
    argument_2: Tensor) -> Tensor:
    add_func = self.add_func
    activation_post_process = add_func.activation_post_process
    input = ops.quantized.add(argument_1, argument_2, 0.15744976699352264, 63)
    _0 = (activation_post_process).forward()
    return input

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_3.res_conn.add_func, type=QFunctional:
class QFunctional(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation_post_process : __torch__.torch.nn.modules.linear.___torch_mangle_6516.Identity

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_3.res_conn.add_func.activation_post_process, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6516.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_4, type=IRFBlock:
class IRFBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  pw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6523.ConvBNRelu
  dw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6525.ConvBNRelu
  se : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6536.SEModule
  pwl : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6539.ConvBNRelu
  res_conn : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6542.TorchAdd
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6543.IRFBlock,
    argument_1: Tensor) -> Tensor:
    res_conn = self.res_conn
    pwl = self.pwl
    se = self.se
    dw = self.dw
    pw = self.pw
    _0 = (dw).forward((pw).forward(argument_1, ), )
    _1 = (res_conn).forward((pwl).forward((se).forward(_0, ), ), argument_1, )
    return _1

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_4.pw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6520.ConvReLU2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6521.Identity
  relu : __torch__.torch.nn.modules.linear.___torch_mangle_6522.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6523.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    relu = self.relu
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    _2 = (relu).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_4.pw.conv, type=ConvReLU2d:
class ConvReLU2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6520.ConvReLU2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6520.ConvReLU2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6520.ConvReLU2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6520.ConvReLU2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6520.ConvReLU2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d_relu(argument_1, _packed_params, 0.034690015017986298, 0)
    return input

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_4.pw.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6521.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_4.pw.relu, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6522.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_4.dw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6524.Conv2d
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6525.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    conv = self.conv
    return (conv).forward(argument_1, )

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_4.dw.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6524.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6524.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6524.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6524.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6524.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    x = ops.quantized.conv2d(argument_1, _packed_params, 0.0066893543116748333, 69)
    return x

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_4.se, type=SEModule:
class SEModule(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  avg_pool : __torch__.torch.nn.modules.pooling.___torch_mangle_6526.AdaptiveAvgPool2d
  se : __torch__.torch.nn.modules.container.___torch_mangle_6532.Sequential
  mul : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6535.TorchMultiply
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6536.SEModule,
    argument_1: Tensor) -> Tensor:
    mul = self.mul
    se = self.se
    avg_pool = self.avg_pool
    _0 = (se).forward((avg_pool).forward(argument_1, ), )
    return (mul).forward(argument_1, _0, )

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_4.se.avg_pool, type=AdaptiveAvgPool2d:
class AdaptiveAvgPool2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.pooling.___torch_mangle_6526.AdaptiveAvgPool2d,
    argument_1: Tensor) -> Tensor:
    input = torch.adaptive_avg_pool2d(argument_1, [1, 1])
    return input

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_4.se.se, type=Sequential:
class Sequential(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  __annotations__["0"] = __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6529.ConvBNRelu
  __annotations__["1"] = __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6530.Conv2d
  __annotations__["2"] = __torch__.torch.nn.modules.activation.___torch_mangle_6531.Sigmoid
  def forward(self: __torch__.torch.nn.modules.container.___torch_mangle_6532.Sequential,
    argument_1: Tensor) -> Tensor:
    _2 = getattr(self, "2")
    _1 = getattr(self, "1")
    _0 = getattr(self, "0")
    _3 = (_1).forward((_0).forward(argument_1, ), )
    return (_2).forward(_3, )

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_4.se.se.0, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6527.ConvReLU2d
  relu : __torch__.torch.nn.modules.linear.___torch_mangle_6528.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6529.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    relu = self.relu
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (relu).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_4.se.se.0.conv, type=ConvReLU2d:
class ConvReLU2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6527.ConvReLU2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6527.ConvReLU2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6527.ConvReLU2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6527.ConvReLU2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6527.ConvReLU2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d_relu(argument_1, _packed_params, 1.1920928955078125e-07, 0)
    return input

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_4.se.se.0.relu, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6528.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_4.se.se.1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6530.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6530.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6530.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6530.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6530.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d(argument_1, _packed_params, 0.0003341123228892684, 20)
    return input

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_4.se.se.2, type=Sigmoid:
class Sigmoid(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.activation.___torch_mangle_6531.Sigmoid,
    argument_1: Tensor) -> Tensor:
    return torch.sigmoid(argument_1)

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_4.se.mul, type=TorchMultiply:
class TorchMultiply(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  mul_func : __torch__.torch.ao.nn.quantized.modules.functional_modules.___torch_mangle_6534.QFunctional
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6535.TorchMultiply,
    argument_1: Tensor,
    argument_2: Tensor) -> Tensor:
    mul_func = self.mul_func
    activation_post_process = mul_func.activation_post_process
    input = ops.quantized.mul(argument_1, argument_2, 0.0033518190030008554, 69)
    _0 = (activation_post_process).forward()
    return input

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_4.se.mul.mul_func, type=QFunctional:
class QFunctional(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation_post_process : __torch__.torch.nn.modules.linear.___torch_mangle_6533.Identity

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_4.se.mul.mul_func.activation_post_process, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6533.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_4.pwl, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6537.Conv2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6538.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6539.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_4.pwl.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6537.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6537.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6537.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6537.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6537.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    _18 = ops.quantized.conv2d(argument_1, _packed_params, 0.14299988746643066, 52)
    return _18

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_4.pwl.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6538.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_4.res_conn, type=TorchAdd:
class TorchAdd(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  add_func : __torch__.torch.ao.nn.quantized.modules.functional_modules.___torch_mangle_6541.QFunctional
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6542.TorchAdd,
    argument_1: Tensor,
    argument_2: Tensor) -> Tensor:
    add_func = self.add_func
    activation_post_process = add_func.activation_post_process
    input = ops.quantized.add(argument_1, argument_2, 0.17322206497192383, 61)
    _0 = (activation_post_process).forward()
    return input

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_4.res_conn.add_func, type=QFunctional:
class QFunctional(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation_post_process : __torch__.torch.nn.modules.linear.___torch_mangle_6540.Identity

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_4.res_conn.add_func.activation_post_process, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6540.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_5, type=IRFBlock:
class IRFBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  pw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6547.ConvBNRelu
  dw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6549.ConvBNRelu
  se : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6560.SEModule
  pwl : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6563.ConvBNRelu
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6564.IRFBlock,
    argument_1: Tensor) -> Tensor:
    pwl = self.pwl
    se = self.se
    dw = self.dw
    pw = self.pw
    _0 = (dw).forward((pw).forward(argument_1, ), )
    return (pwl).forward((se).forward(_0, ), )

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_5.pw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6544.ConvReLU2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6545.Identity
  relu : __torch__.torch.nn.modules.linear.___torch_mangle_6546.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6547.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    relu = self.relu
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    _2 = (relu).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_5.pw.conv, type=ConvReLU2d:
class ConvReLU2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6544.ConvReLU2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6544.ConvReLU2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6544.ConvReLU2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6544.ConvReLU2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6544.ConvReLU2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d_relu(argument_1, _packed_params, 0.033178608864545822, 0)
    return input

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_5.pw.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6545.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_5.pw.relu, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6546.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_5.dw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6548.Conv2d
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6549.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    conv = self.conv
    return (conv).forward(argument_1, )

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_5.dw.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6548.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6548.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6548.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6548.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6548.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    x = ops.quantized.conv2d(argument_1, _packed_params, 0.0093186870217323303, 66)
    return x

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_5.se, type=SEModule:
class SEModule(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  avg_pool : __torch__.torch.nn.modules.pooling.___torch_mangle_6550.AdaptiveAvgPool2d
  se : __torch__.torch.nn.modules.container.___torch_mangle_6556.Sequential
  mul : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6559.TorchMultiply
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6560.SEModule,
    argument_1: Tensor) -> Tensor:
    mul = self.mul
    se = self.se
    avg_pool = self.avg_pool
    _0 = (se).forward((avg_pool).forward(argument_1, ), )
    return (mul).forward(argument_1, _0, )

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_5.se.avg_pool, type=AdaptiveAvgPool2d:
class AdaptiveAvgPool2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.pooling.___torch_mangle_6550.AdaptiveAvgPool2d,
    argument_1: Tensor) -> Tensor:
    input = torch.adaptive_avg_pool2d(argument_1, [1, 1])
    return input

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_5.se.se, type=Sequential:
class Sequential(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  __annotations__["0"] = __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6553.ConvBNRelu
  __annotations__["1"] = __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6554.Conv2d
  __annotations__["2"] = __torch__.torch.nn.modules.activation.___torch_mangle_6555.Sigmoid
  def forward(self: __torch__.torch.nn.modules.container.___torch_mangle_6556.Sequential,
    argument_1: Tensor) -> Tensor:
    _2 = getattr(self, "2")
    _1 = getattr(self, "1")
    _0 = getattr(self, "0")
    _3 = (_1).forward((_0).forward(argument_1, ), )
    return (_2).forward(_3, )

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_5.se.se.0, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6551.ConvReLU2d
  relu : __torch__.torch.nn.modules.linear.___torch_mangle_6552.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6553.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    relu = self.relu
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (relu).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_5.se.se.0.conv, type=ConvReLU2d:
class ConvReLU2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6551.ConvReLU2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6551.ConvReLU2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6551.ConvReLU2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6551.ConvReLU2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6551.ConvReLU2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d_relu(argument_1, _packed_params, 1.1920928955078125e-07, 0)
    return input

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_5.se.se.0.relu, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6552.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_5.se.se.1, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6554.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6554.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6554.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6554.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6554.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d(argument_1, _packed_params, 0.00016886177763808519, 17)
    return input

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_5.se.se.2, type=Sigmoid:
class Sigmoid(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.activation.___torch_mangle_6555.Sigmoid,
    argument_1: Tensor) -> Tensor:
    return torch.sigmoid(argument_1)

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_5.se.mul, type=TorchMultiply:
class TorchMultiply(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  mul_func : __torch__.torch.ao.nn.quantized.modules.functional_modules.___torch_mangle_6558.QFunctional
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6559.TorchMultiply,
    argument_1: Tensor,
    argument_2: Tensor) -> Tensor:
    mul_func = self.mul_func
    activation_post_process = mul_func.activation_post_process
    input = ops.quantized.mul(argument_1, argument_2, 0.0050564962439239025, 61)
    _0 = (activation_post_process).forward()
    return input

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_5.se.mul.mul_func, type=QFunctional:
class QFunctional(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation_post_process : __torch__.torch.nn.modules.linear.___torch_mangle_6557.Identity

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_5.se.mul.mul_func.activation_post_process, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6557.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_5.pwl, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6561.Conv2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6562.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6563.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_5.pwl.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6561.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6561.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6561.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6561.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6561.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    x = ops.quantized.conv2d(argument_1, _packed_params, 0.27006351947784424, 62)
    return x

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.roi_box_conv.0.fbnetv2_0_5.pwl.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6562.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.avgpool, type=AdaptiveAvgPool2d:
class AdaptiveAvgPool2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.pooling.___torch_mangle_6567.AdaptiveAvgPool2d,
    argument_1: Tensor) -> Tensor:
    Xq = torch.adaptive_avg_pool2d(argument_1, [1, 1])
    return Xq

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.quant_stubs, type=QuantStubNested:
class QuantStubNested(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  stubs : __torch__.torch.nn.modules.container.___torch_mangle_6569.ModuleList
  def forward(self: __torch__.mobile_cv.arch.utils.quantize_utils.___torch_mangle_6570.QuantStubNested,
    argument_1: Tensor) -> Tensor:
    stubs = self.stubs
    _0 = getattr(stubs, "0")
    return (_0).forward(argument_1, )

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.quant_stubs.stubs, type=ModuleList:
class ModuleList(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  __annotations__["0"] = __torch__.torch.ao.nn.quantized.modules.___torch_mangle_6568.Quantize

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.quant_stubs.stubs.0, type=Quantize:
class Quantize(Module):
  __parameters__ = []
  __buffers__ = ["scale", "zero_point", ]
  scale : Tensor
  zero_point : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.ao.nn.quantized.modules.___torch_mangle_6568.Quantize,
    argument_1: Tensor) -> Tensor:
    input = torch.quantize_per_tensor(argument_1, 0.073521420359611511, 63, 13)
    return input

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.dequant_stubs, type=QuantStubNested:
class QuantStubNested(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  stubs : __torch__.torch.nn.modules.container.___torch_mangle_6572.ModuleList
  def forward(self: __torch__.mobile_cv.arch.utils.quantize_utils.___torch_mangle_6573.QuantStubNested,
    argument_1: Tensor) -> Tensor:
    stubs = self.stubs
    _0 = getattr(stubs, "0")
    return (_0).forward(argument_1, )

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.dequant_stubs.stubs, type=ModuleList:
class ModuleList(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  __annotations__["0"] = __torch__.torch.ao.nn.quantized.modules.___torch_mangle_6571.DeQuantize

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_head.dequant_stubs.stubs.0, type=DeQuantize:
class DeQuantize(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.ao.nn.quantized.modules.___torch_mangle_6571.DeQuantize,
    argument_1: Tensor) -> Tensor:
    return torch.dequantize(argument_1)

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_predictor, type=QuantWrapSubClass:
class QuantWrapSubClass(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  cls_score : __torch__.torch.ao.nn.quantized.modules.linear.___torch_mangle_6576.Linear
  bbox_pred : __torch__.torch.ao.nn.quantized.modules.linear.___torch_mangle_6578.Linear
  quant_stubs : __torch__.mobile_cv.arch.utils.quantize_utils.___torch_mangle_6581.QuantStubNested
  dequant_stubs : __torch__.mobile_cv.arch.utils.quantize_utils.___torch_mangle_6585.QuantStubNested
  def forward(self: __torch__.mobile_cv.arch.utils.quantize_utils.___torch_mangle_6586.QuantWrapSubClass,
    argument_1: Tensor) -> Tuple[Tensor, Tensor]:
    dequant_stubs = self.dequant_stubs
    bbox_pred = self.bbox_pred
    cls_score = self.cls_score
    quant_stubs = self.quant_stubs
    _0 = torch.flatten((quant_stubs).forward(argument_1, ), 1)
    _1 = (dequant_stubs).forward((cls_score).forward(_0, ), (bbox_pred).forward(_0, ), )
    _2, _3, = _1
    return (_2, _3)

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_predictor.cls_score, type=Linear:
class Linear(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  _packed_params : __torch__.torch.ao.nn.quantized.modules.linear.___torch_mangle_6575.LinearPackedParams
  def forward(self: __torch__.torch.ao.nn.quantized.modules.linear.___torch_mangle_6576.Linear,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    _packed_params0 = _packed_params._packed_params
    Xq = ops.quantized.linear(argument_1, _packed_params0, 0.11318807303905487, 28)
    return Xq

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_predictor.cls_score._packed_params, type=LinearPackedParams:
class LinearPackedParams(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  dtype : int
  _packed_params : __torch__.torch.classes.quantized.LinearPackedParamsBase
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.linear.___torch_mangle_6575.LinearPackedParams) -> Tuple[Tensor, Optional[Tensor]]:
    _0 = "Unsupported dtype on dynamic quantized linear!"
    _1 = uninitialized(Tuple[Tensor, Optional[Tensor]])
    dtype = self.dtype
    if torch.eq(dtype, 12):
      _packed_params = self._packed_params
      _3, _4 = ops.quantized.linear_unpack(_packed_params)
      _2 = (_3, _4)
    else:
      dtype0 = self.dtype
      if torch.eq(dtype0, 5):
        _packed_params0 = self._packed_params
        _6, _7 = ops.quantized.linear_unpack_fp16(_packed_params0)
        _5 = (_6, _7)
      else:
        ops.prim.RaiseException(_0, "builtins.RuntimeError")
        _5 = _1
      _2 = _5
    return _2
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.linear.___torch_mangle_6575.LinearPackedParams,
    weight: Tensor,
    bias: Optional[Tensor]) -> NoneType:
    _8 = "Unsupported dtype on dynamic quantized linear!"
    dtype = self.dtype
    if torch.eq(dtype, 12):
      _9 = ops.quantized.linear_prepack(weight, bias)
      self._packed_params = _9
    else:
      dtype1 = self.dtype
      if torch.eq(dtype1, 5):
        _10 = ops.quantized.linear_prepack_fp16(weight, bias)
        self._packed_params = _10
      else:
        ops.prim.RaiseException(_8, "builtins.RuntimeError")
    return None

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_predictor.bbox_pred, type=Linear:
class Linear(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  _packed_params : __torch__.torch.ao.nn.quantized.modules.linear.___torch_mangle_6577.LinearPackedParams
  def forward(self: __torch__.torch.ao.nn.quantized.modules.linear.___torch_mangle_6578.Linear,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    _packed_params0 = _packed_params._packed_params
    Xq = ops.quantized.linear(argument_1, _packed_params0, 0.060246266424655914, 69)
    return Xq

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_predictor.bbox_pred._packed_params, type=LinearPackedParams:
class LinearPackedParams(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  dtype : int
  _packed_params : __torch__.torch.classes.quantized.LinearPackedParamsBase
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.linear.___torch_mangle_6577.LinearPackedParams) -> Tuple[Tensor, Optional[Tensor]]:
    _0 = "Unsupported dtype on dynamic quantized linear!"
    _1 = uninitialized(Tuple[Tensor, Optional[Tensor]])
    dtype = self.dtype
    if torch.eq(dtype, 12):
      _packed_params = self._packed_params
      _3, _4 = ops.quantized.linear_unpack(_packed_params)
      _2 = (_3, _4)
    else:
      dtype0 = self.dtype
      if torch.eq(dtype0, 5):
        _packed_params0 = self._packed_params
        _6, _7 = ops.quantized.linear_unpack_fp16(_packed_params0)
        _5 = (_6, _7)
      else:
        ops.prim.RaiseException(_0, "builtins.RuntimeError")
        _5 = _1
      _2 = _5
    return _2
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.linear.___torch_mangle_6577.LinearPackedParams,
    weight: Tensor,
    bias: Optional[Tensor]) -> NoneType:
    _8 = "Unsupported dtype on dynamic quantized linear!"
    dtype = self.dtype
    if torch.eq(dtype, 12):
      _9 = ops.quantized.linear_prepack(weight, bias)
      self._packed_params = _9
    else:
      dtype1 = self.dtype
      if torch.eq(dtype1, 5):
        _10 = ops.quantized.linear_prepack_fp16(weight, bias)
        self._packed_params = _10
      else:
        ops.prim.RaiseException(_8, "builtins.RuntimeError")
    return None

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_predictor.quant_stubs, type=QuantStubNested:
class QuantStubNested(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  stubs : __torch__.torch.nn.modules.container.___torch_mangle_6580.ModuleList
  def forward(self: __torch__.mobile_cv.arch.utils.quantize_utils.___torch_mangle_6581.QuantStubNested,
    argument_1: Tensor) -> Tensor:
    stubs = self.stubs
    _0 = getattr(stubs, "0")
    return (_0).forward(argument_1, )

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_predictor.quant_stubs.stubs, type=ModuleList:
class ModuleList(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  __annotations__["0"] = __torch__.torch.ao.nn.quantized.modules.___torch_mangle_6579.Quantize

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_predictor.quant_stubs.stubs.0, type=Quantize:
class Quantize(Module):
  __parameters__ = []
  __buffers__ = ["scale", "zero_point", ]
  scale : Tensor
  zero_point : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.ao.nn.quantized.modules.___torch_mangle_6579.Quantize,
    argument_1: Tensor) -> Tensor:
    x = torch.quantize_per_tensor(argument_1, 0.23095256090164185, 63, 13)
    return x

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_predictor.dequant_stubs, type=QuantStubNested:
class QuantStubNested(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  stubs : __torch__.torch.nn.modules.container.___torch_mangle_6584.ModuleList
  def forward(self: __torch__.mobile_cv.arch.utils.quantize_utils.___torch_mangle_6585.QuantStubNested,
    argument_1: Tensor,
    argument_2: Tensor) -> Tuple[Tensor, Tensor]:
    stubs = self.stubs
    _1 = getattr(stubs, "1")
    stubs0 = self.stubs
    _0 = getattr(stubs0, "0")
    _2 = (_0).forward(argument_1, )
    return ((_1).forward(argument_2, ), _2)

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_predictor.dequant_stubs.stubs, type=ModuleList:
class ModuleList(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  __annotations__["0"] = __torch__.torch.ao.nn.quantized.modules.___torch_mangle_6582.DeQuantize
  __annotations__["1"] = __torch__.torch.ao.nn.quantized.modules.___torch_mangle_6583.DeQuantize

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_predictor.dequant_stubs.stubs.0, type=DeQuantize:
class DeQuantize(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.ao.nn.quantized.modules.___torch_mangle_6582.DeQuantize,
    argument_1: Tensor) -> Tensor:
    return torch.dequantize(argument_1)

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.box_predictor.dequant_stubs.stubs.1, type=DeQuantize:
class DeQuantize(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.ao.nn.quantized.modules.___torch_mangle_6583.DeQuantize,
    argument_1: Tensor) -> Tensor:
    return torch.dequantize(argument_1)

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_pooler, type=ROIPooler:
class ROIPooler(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  level_poolers : __torch__.torch.nn.modules.container.___torch_mangle_6588.ModuleList
  def forward(self: __torch__.detectron2.modeling.poolers.___torch_mangle_6589.ROIPooler,
    argument_1: Tensor,
    tensor: Tensor) -> Tensor:
    _0 = __torch__.detectron2.modeling.poolers._convert_boxes_to_pooler_format
    level_poolers = self.level_poolers
    _00 = getattr(level_poolers, "0")
    _1 = torch.cat([tensor])
    _2 = ops.prim.NumToTensor(torch.size(tensor, 0))
    rois = _0(_1, torch.stack([_2]), )
    return (_00).forward(rois, argument_1, )

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_pooler.level_poolers, type=ModuleList:
class ModuleList(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  __annotations__["0"] = __torch__.detectron2.layers.roi_align.___torch_mangle_6587.ROIAlign

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_pooler.level_poolers.0, type=ROIAlign:
class ROIAlign(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.detectron2.layers.roi_align.___torch_mangle_6587.ROIAlign,
    rois: Tensor,
    argument_2: Tensor) -> Tensor:
    boxes = torch.to(rois, 6)
    X = ops.torchvision.roi_align(argument_2, boxes, 0.0625, 14, 14, 0, True)
    return X

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head, type=QuantWrapSubClass:
class QuantWrapSubClass(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  feature_extractor : __torch__.d2go.modeling.backbone.fbnet_v2.___torch_mangle_6649.FBNetModule
  predictor : __torch__.d2go.modeling.backbone.modules.___torch_mangle_6651.MaskRCNNConv1x1Predictor
  quant_stubs : __torch__.mobile_cv.arch.utils.quantize_utils.___torch_mangle_6654.QuantStubNested
  dequant_stubs : __torch__.mobile_cv.arch.utils.quantize_utils.___torch_mangle_6657.QuantStubNested
  def forward(self: __torch__.mobile_cv.arch.utils.quantize_utils.___torch_mangle_6658.QuantWrapSubClass,
    argument_1: Tensor,
    class_pred: Tensor,
    tensor: Tensor) -> Tensor:
    _0 = __torch__.detectron2.layers.wrappers.move_device_like
    dequant_stubs = self.dequant_stubs
    predictor = self.predictor
    feature_extractor = self.feature_extractor
    quant_stubs = self.quant_stubs
    _1 = (feature_extractor).forward((quant_stubs).forward(argument_1, ), )
    _2 = (dequant_stubs).forward((predictor).forward(_1, ), )
    num_masks = ops.prim.NumToTensor(torch.size(_2, 0))
    _3 = torch.arange(annotate(number, num_masks), dtype=None, layout=None, device=torch.device("cpu"), pin_memory=False)
    indices = _0(_3, class_pred, )
    _4 = annotate(List[Optional[Tensor]], [indices, class_pred])
    _5 = torch.slice(torch.index(_2, _4), 0, 0, 9223372036854775807)
    _6 = torch.sigmoid(torch.unsqueeze(_5, 1))
    _7 = ops.prim.NumToTensor(torch.size(tensor, 0))
    _8 = torch.split_with_sizes(_6, [int(_7)])
    _9, = _8
    return _9

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor, type=FBNetModule:
class FBNetModule(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  __annotations__["0"] = __torch__.torch.nn.modules.container.___torch_mangle_6648.Sequential
  def forward(self: __torch__.d2go.modeling.backbone.fbnet_v2.___torch_mangle_6649.FBNetModule,
    argument_1: Tensor) -> Tensor:
    _0 = getattr(self, "0")
    return (_0).forward(argument_1, )

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0, type=Sequential:
class Sequential(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  fbnetv2_0_0 : __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6599.IRFBlock
  fbnetv2_0_1 : __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6612.IRFBlock
  fbnetv2_0_2 : __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6625.IRFBlock
  fbnetv2_0_3 : __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6636.IRFBlock
  fbnetv2_0_4 : __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6647.IRFBlock
  def forward(self: __torch__.torch.nn.modules.container.___torch_mangle_6648.Sequential,
    argument_1: Tensor) -> Tensor:
    fbnetv2_0_4 = self.fbnetv2_0_4
    fbnetv2_0_3 = self.fbnetv2_0_3
    fbnetv2_0_2 = self.fbnetv2_0_2
    fbnetv2_0_1 = self.fbnetv2_0_1
    fbnetv2_0_0 = self.fbnetv2_0_0
    _0 = (fbnetv2_0_1).forward((fbnetv2_0_0).forward(argument_1, ), )
    _1 = (fbnetv2_0_3).forward((fbnetv2_0_2).forward(_0, ), )
    return (fbnetv2_0_4).forward(_1, )

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_0, type=IRFBlock:
class IRFBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  pw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6593.ConvBNRelu
  dw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6595.ConvBNRelu
  pwl : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6598.ConvBNRelu
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6599.IRFBlock,
    argument_1: Tensor) -> Tensor:
    pwl = self.pwl
    dw = self.dw
    pw = self.pw
    _0 = (dw).forward((pw).forward(argument_1, ), )
    return (pwl).forward(_0, )

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_0.pw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6590.ConvReLU2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6591.Identity
  relu : __torch__.torch.nn.modules.linear.___torch_mangle_6592.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6593.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    relu = self.relu
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    _2 = (relu).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_0.pw.conv, type=ConvReLU2d:
class ConvReLU2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6590.ConvReLU2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6590.ConvReLU2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6590.ConvReLU2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6590.ConvReLU2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6590.ConvReLU2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d_relu(argument_1, _packed_params, 0.06510905921459198, 0)
    return input

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_0.pw.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6591.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_0.pw.relu, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6592.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_0.dw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6594.Conv2d
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6595.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    conv = self.conv
    return (conv).forward(argument_1, )

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_0.dw.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6594.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6594.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6594.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6594.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6594.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d(argument_1, _packed_params, 0.012689515016973019, 56)
    return input

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_0.pwl, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6596.Conv2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6597.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6598.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_0.pwl.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6596.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6596.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6596.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6596.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6596.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d(argument_1, _packed_params, 0.10345242917537689, 65)
    return input

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_0.pwl.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6597.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_1, type=IRFBlock:
class IRFBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  pw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6603.ConvBNRelu
  dw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6605.ConvBNRelu
  pwl : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6608.ConvBNRelu
  res_conn : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6611.TorchAdd
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6612.IRFBlock,
    argument_1: Tensor) -> Tensor:
    res_conn = self.res_conn
    pwl = self.pwl
    dw = self.dw
    pw = self.pw
    _0 = (dw).forward((pw).forward(argument_1, ), )
    _1 = (res_conn).forward((pwl).forward(_0, ), argument_1, )
    return _1

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_1.pw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6600.ConvReLU2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6601.Identity
  relu : __torch__.torch.nn.modules.linear.___torch_mangle_6602.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6603.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    relu = self.relu
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    _2 = (relu).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_1.pw.conv, type=ConvReLU2d:
class ConvReLU2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6600.ConvReLU2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6600.ConvReLU2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6600.ConvReLU2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6600.ConvReLU2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6600.ConvReLU2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d_relu(argument_1, _packed_params, 0.050910450518131256, 0)
    return input

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_1.pw.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6601.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_1.pw.relu, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6602.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_1.dw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6604.Conv2d
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6605.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    conv = self.conv
    return (conv).forward(argument_1, )

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_1.dw.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6604.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6604.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6604.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6604.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6604.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d(argument_1, _packed_params, 0.008350214920938015, 66)
    return input

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_1.pwl, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6606.Conv2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6607.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6608.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_1.pwl.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6606.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6606.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6606.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6606.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6606.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    _18 = ops.quantized.conv2d(argument_1, _packed_params, 0.10633838921785355, 74)
    return _18

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_1.pwl.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6607.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_1.res_conn, type=TorchAdd:
class TorchAdd(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  add_func : __torch__.torch.ao.nn.quantized.modules.functional_modules.___torch_mangle_6610.QFunctional
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6611.TorchAdd,
    argument_1: Tensor,
    argument_2: Tensor) -> Tensor:
    add_func = self.add_func
    activation_post_process = add_func.activation_post_process
    input = ops.quantized.add(argument_1, argument_2, 0.12656402587890625, 67)
    _0 = (activation_post_process).forward()
    return input

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_1.res_conn.add_func, type=QFunctional:
class QFunctional(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation_post_process : __torch__.torch.nn.modules.linear.___torch_mangle_6609.Identity

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_1.res_conn.add_func.activation_post_process, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6609.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_2, type=IRFBlock:
class IRFBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  pw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6616.ConvBNRelu
  dw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6618.ConvBNRelu
  pwl : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6621.ConvBNRelu
  res_conn : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6624.TorchAdd
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6625.IRFBlock,
    argument_1: Tensor) -> Tensor:
    res_conn = self.res_conn
    pwl = self.pwl
    dw = self.dw
    pw = self.pw
    _0 = (dw).forward((pw).forward(argument_1, ), )
    _1 = (res_conn).forward((pwl).forward(_0, ), argument_1, )
    return _1

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_2.pw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6613.ConvReLU2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6614.Identity
  relu : __torch__.torch.nn.modules.linear.___torch_mangle_6615.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6616.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    relu = self.relu
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    _2 = (relu).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_2.pw.conv, type=ConvReLU2d:
class ConvReLU2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6613.ConvReLU2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6613.ConvReLU2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6613.ConvReLU2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6613.ConvReLU2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6613.ConvReLU2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d_relu(argument_1, _packed_params, 0.03661094605922699, 0)
    return input

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_2.pw.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6614.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_2.pw.relu, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6615.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_2.dw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6617.Conv2d
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6618.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    conv = self.conv
    return (conv).forward(argument_1, )

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_2.dw.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6617.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6617.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6617.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6617.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6617.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d(argument_1, _packed_params, 0.0066626719199120998, 70)
    return input

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_2.pwl, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6619.Conv2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6620.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6621.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_2.pwl.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6619.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6619.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6619.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6619.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6619.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    _18 = ops.quantized.conv2d(argument_1, _packed_params, 0.1241278350353241, 64)
    return _18

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_2.pwl.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6620.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_2.res_conn, type=TorchAdd:
class TorchAdd(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  add_func : __torch__.torch.ao.nn.quantized.modules.functional_modules.___torch_mangle_6623.QFunctional
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6624.TorchAdd,
    argument_1: Tensor,
    argument_2: Tensor) -> Tensor:
    add_func = self.add_func
    activation_post_process = add_func.activation_post_process
    input = ops.quantized.add(argument_1, argument_2, 0.1597774475812912, 68)
    _0 = (activation_post_process).forward()
    return input

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_2.res_conn.add_func, type=QFunctional:
class QFunctional(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  activation_post_process : __torch__.torch.nn.modules.linear.___torch_mangle_6622.Identity

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_2.res_conn.add_func.activation_post_process, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6622.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_3, type=IRFBlock:
class IRFBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  pw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6629.ConvBNRelu
  upsample : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6630.Upsample
  dw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6632.ConvBNRelu
  pwl : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6635.ConvBNRelu
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6636.IRFBlock,
    argument_1: Tensor) -> Tensor:
    pwl = self.pwl
    dw = self.dw
    upsample = self.upsample
    pw = self.pw
    _0 = (upsample).forward((pw).forward(argument_1, ), )
    return (pwl).forward((dw).forward(_0, ), )

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_3.pw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6626.ConvReLU2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6627.Identity
  relu : __torch__.torch.nn.modules.linear.___torch_mangle_6628.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6629.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    relu = self.relu
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    _2 = (relu).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_3.pw.conv, type=ConvReLU2d:
class ConvReLU2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6626.ConvReLU2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6626.ConvReLU2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6626.ConvReLU2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6626.ConvReLU2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6626.ConvReLU2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d_relu(argument_1, _packed_params, 0.054473068565130234, 0)
    return input

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_3.pw.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6627.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_3.pw.relu, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6628.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_3.upsample, type=Upsample:
class Upsample(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6630.Upsample,
    argument_1: Tensor) -> Tensor:
    input = torch.upsample_nearest2d(argument_1, None, [2., 2.])
    return input

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_3.dw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6631.Conv2d
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6632.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    conv = self.conv
    return (conv).forward(argument_1, )

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_3.dw.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6631.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6631.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6631.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6631.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6631.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d(argument_1, _packed_params, 0.013850843533873558, 60)
    return input

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_3.pwl, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6633.Conv2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6634.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6635.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_3.pwl.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6633.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6633.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6633.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6633.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6633.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d(argument_1, _packed_params, 0.2115606814622879, 64)
    return input

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_3.pwl.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6634.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_4, type=IRFBlock:
class IRFBlock(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  pw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6640.ConvBNRelu
  upsample : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6641.Upsample
  dw : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6643.ConvBNRelu
  pwl : __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6646.ConvBNRelu
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.irf_block.___torch_mangle_6647.IRFBlock,
    argument_1: Tensor) -> Tensor:
    pwl = self.pwl
    dw = self.dw
    upsample = self.upsample
    pw = self.pw
    _0 = (upsample).forward((pw).forward(argument_1, ), )
    return (pwl).forward((dw).forward(_0, ), )

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_4.pw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6637.ConvReLU2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6638.Identity
  relu : __torch__.torch.nn.modules.linear.___torch_mangle_6639.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6640.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    relu = self.relu
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    _2 = (relu).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_4.pw.conv, type=ConvReLU2d:
class ConvReLU2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6637.ConvReLU2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6637.ConvReLU2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6637.ConvReLU2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6637.ConvReLU2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.intrinsic.quantized.modules.conv_relu.___torch_mangle_6637.ConvReLU2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d_relu(argument_1, _packed_params, 0.10326559841632843, 0)
    return input

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_4.pw.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6638.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_4.pw.relu, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6639.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_4.upsample, type=Upsample:
class Upsample(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6641.Upsample,
    argument_1: Tensor) -> Tensor:
    input = torch.upsample_nearest2d(argument_1, None, [2., 2.])
    return input

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_4.dw, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6642.Conv2d
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6643.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    conv = self.conv
    return (conv).forward(argument_1, )

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_4.dw.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6642.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6642.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6642.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6642.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6642.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d(argument_1, _packed_params, 0.053558960556983948, 59)
    return input

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_4.pwl, type=ConvBNRelu:
class ConvBNRelu(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  conv : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6644.Conv2d
  bn : __torch__.torch.nn.modules.linear.___torch_mangle_6645.Identity
  def forward(self: __torch__.mobile_cv.arch.fbnet_v2.basic_blocks.___torch_mangle_6646.ConvBNRelu,
    argument_1: Tensor) -> Tensor:
    bn = self.bn
    conv = self.conv
    _0 = (conv).forward(argument_1, )
    _1 = (bn).forward()
    return _0

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_4.pwl.conv, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6644.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6644.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6644.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6644.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6644.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    input = ops.quantized.conv2d(argument_1, _packed_params, 0.84708833694458008, 60)
    return input

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.feature_extractor.0.fbnetv2_0_4.pwl.bn, type=Identity:
class Identity(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.nn.modules.linear.___torch_mangle_6645.Identity) -> NoneType:
    return None

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.predictor, type=MaskRCNNConv1x1Predictor:
class MaskRCNNConv1x1Predictor(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  mask_fcn_logits : __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6650.Conv2d
  def forward(self: __torch__.d2go.modeling.backbone.modules.___torch_mangle_6651.MaskRCNNConv1x1Predictor,
    argument_1: Tensor) -> Tensor:
    mask_fcn_logits = self.mask_fcn_logits
    _0 = (mask_fcn_logits).forward(argument_1, )
    return _0

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.predictor.mask_fcn_logits, type=Conv2d:
class Conv2d(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  in_channels : int
  out_channels : int
  kernel_size : Tuple[int, int]
  stride : Tuple[int, int]
  padding : Tuple[int, int]
  dilation : Tuple[int, int]
  transposed : bool
  output_padding : Tuple[int, int]
  groups : int
  padding_mode : str
  _packed_params : __torch__.torch.classes.quantized.Conv2dPackedParamsBase
  scale : float
  zero_point : int
  def __getstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6650.Conv2d) -> Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]:
    w, b, = (self)._weight_bias()
    in_channels = self.in_channels
    out_channels = self.out_channels
    kernel_size = self.kernel_size
    stride = self.stride
    padding = self.padding
    dilation = self.dilation
    transposed = self.transposed
    output_padding = self.output_padding
    groups = self.groups
    padding_mode = self.padding_mode
    scale = self.scale
    zero_point = self.zero_point
    training = self.training
    _0 = (in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, padding_mode, w, b, scale, zero_point, training)
    return _0
  def __setstate__(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6650.Conv2d,
    state: Tuple[int, int, Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int], bool, Tuple[int, int], int, str, Tensor, Optional[Tensor], float, int, bool]) -> NoneType:
    self.in_channels = (state)[0]
    self.out_channels = (state)[1]
    self.kernel_size = (state)[2]
    self.stride = (state)[3]
    self.padding = (state)[4]
    self.dilation = (state)[5]
    self.transposed = (state)[6]
    self.output_padding = (state)[7]
    self.groups = (state)[8]
    self.padding_mode = (state)[9]
    _1 = (self).set_weight_bias((state)[10], (state)[11], )
    self.scale = (state)[12]
    self.zero_point = (state)[13]
    self.training = (state)[14]
    return None
  def _weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6650.Conv2d) -> Tuple[Tensor, Optional[Tensor]]:
    _packed_params = self._packed_params
    return (_packed_params).unpack()
  def set_weight_bias(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6650.Conv2d,
    w: Tensor,
    b: Optional[Tensor]) -> NoneType:
    padding_mode = self.padding_mode
    if torch.eq(padding_mode, "zeros"):
      stride = self.stride
      padding = self.padding
      dilation = self.dilation
      groups = self.groups
      _2, _3, = stride
      _4 = [_2, _3]
      _5, _6, = padding
      _7 = [_5, _6]
      _8, _9, = dilation
      _10 = ops.quantized.conv2d_prepack(w, b, _4, _7, [_8, _9], groups)
      self._packed_params = _10
    else:
      stride0 = self.stride
      _11 = [0, 0]
      dilation0 = self.dilation
      groups0 = self.groups
      _12, _13, = stride0
      _14 = [_12, _13]
      _15, _16, = dilation0
      _17 = ops.quantized.conv2d_prepack(w, b, _14, _11, [_15, _16], groups0)
      self._packed_params = _17
    return None
  def forward(self: __torch__.torch.ao.nn.quantized.modules.conv.___torch_mangle_6650.Conv2d,
    argument_1: Tensor) -> Tensor:
    _packed_params = self._packed_params
    Xq = ops.quantized.conv2d(argument_1, _packed_params, 1.4873088598251343, 97)
    return Xq

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.quant_stubs, type=QuantStubNested:
class QuantStubNested(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  stubs : __torch__.torch.nn.modules.container.___torch_mangle_6653.ModuleList
  def forward(self: __torch__.mobile_cv.arch.utils.quantize_utils.___torch_mangle_6654.QuantStubNested,
    argument_1: Tensor) -> Tensor:
    stubs = self.stubs
    _0 = getattr(stubs, "0")
    return (_0).forward(argument_1, )

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.quant_stubs.stubs, type=ModuleList:
class ModuleList(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  __annotations__["0"] = __torch__.torch.ao.nn.quantized.modules.___torch_mangle_6652.Quantize

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.quant_stubs.stubs.0, type=Quantize:
class Quantize(Module):
  __parameters__ = []
  __buffers__ = ["scale", "zero_point", ]
  scale : Tensor
  zero_point : Tensor
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.ao.nn.quantized.modules.___torch_mangle_6652.Quantize,
    argument_1: Tensor) -> Tensor:
    input = torch.quantize_per_tensor(argument_1, 0.06789860874414444, 59, 13)
    return input

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.dequant_stubs, type=QuantStubNested:
class QuantStubNested(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  stubs : __torch__.torch.nn.modules.container.___torch_mangle_6656.ModuleList
  def forward(self: __torch__.mobile_cv.arch.utils.quantize_utils.___torch_mangle_6657.QuantStubNested,
    argument_1: Tensor) -> Tensor:
    stubs = self.stubs
    _0 = getattr(stubs, "0")
    return (_0).forward(argument_1, )

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.dequant_stubs.stubs, type=ModuleList:
class ModuleList(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  __annotations__["0"] = __torch__.torch.ao.nn.quantized.modules.___torch_mangle_6655.DeQuantize

--------------------------------------------------------------------------------
Code for .model.model.roi_heads.mask_head.dequant_stubs.stubs.0, type=DeQuantize:
class DeQuantize(Module):
  __parameters__ = []
  __buffers__ = []
  training : bool
  _is_full_backward_hook : Optional[bool]
  def forward(self: __torch__.torch.ao.nn.quantized.modules.___torch_mangle_6655.DeQuantize,
    argument_1: Tensor) -> Tensor:
    return torch.dequantize(argument_1)

--------------------------------------------------------------------------------